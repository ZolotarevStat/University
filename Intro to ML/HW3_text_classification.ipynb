{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "## NOTE\n",
    "За 4 часа до дедлайна у меня крашнулся ноутбук во время создания генератора 21000*10000 в TF-IDF, поэтому некоторые части кода запущены в разное время и могут выдавать формально не тот аутпут, который нужен. Домашка просто ужасная, почему вообще никто не сказал нам, каким образом лучше обрабатывать столь огромные массивы информации?..\n",
    "\n",
    "Задания на mystem & Word vectors выполнены на Kaggle и представлены в ноутбуке, который можно скачать здесь:\n",
    "https://yadi.sk/d/65pqN8Lk6pn3wA  (скрин аутпута в случае SVC для Word vectors: https://yadi.sk/i/QCBPmpJ-ZrM5qA , почему-то Kaggle не захотел последний вариант джупайтера скачивать).\n",
    "\n",
    "# Введение в анализ данных\n",
    "## НИУ ВШЭ, 2019-2020 учебный год\n",
    "\n",
    "### Домашнее задание №3 (старая версия)\n",
    "\n",
    "Задание выполнил(а): Золотарев Антон Олегович\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 09.04.2020\n",
    "\n",
    "Дедлайн: 24.04.2020 23:59 MSK\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом домашнем задании вы будете работать с линейной классификацией, попрактикуетесь на реальной задаче классификации текстов.\n",
    "\n",
    "Для решения этого домашнего задания намного удобнее будет использовать Colab, так как данных много.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 1 балл в день, но получить отрицательную оценку нельзя.\n",
    "\n",
    "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
    "\n",
    "### Формат сдачи\n",
    "Загрузка файлов с решениями происходит в системе Anytask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx03xvr9T95"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrrwTJNjuDt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "id": "_VMchexbjjTh",
    "outputId": "c1f66a1f-8851-42f3-e7e5-6c48d3c24707"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "# Датасет можно скачать здесь\n",
    "\n",
    "!wget https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv\n",
    "    \n",
    "# На этом шаге у меня возникла какая-то ересь в строке вывода, \n",
    "# поэтому я просто скачал файл на комп для дальнейшей работы над дз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvXKae8q9nn-"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Мы имеем дело с данными с торговой платформы Avito.\n",
    "Для каждого товара представлены следующие параметры:\n",
    " - title\n",
    " - description\n",
    " - Category_name\n",
    " - Category\n",
    "\n",
    "Имеется информация об объектах 50 классов.\n",
    "Задача: по новым объектам (title, description) предсказать Category.\n",
    "(Очевидно, что параметр Category_name для предсказания классов использовать нельзя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "BqEuoDhqNgoa",
    "outputId": "b345f049-ae77-4d1b-a25f-4d4f447e63d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382220</th>\n",
       "      <td>Прихожая</td>\n",
       "      <td>В хорошем состоянии. Торг</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397529</th>\n",
       "      <td>Кордиант 215/55/16 Летние</td>\n",
       "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584569</th>\n",
       "      <td>Стол</td>\n",
       "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513100</th>\n",
       "      <td>Комбинезон</td>\n",
       "      <td>Размер-42/44</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091886</th>\n",
       "      <td>Ветровка</td>\n",
       "      <td>На 2 года</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "id                                   \n",
       "382220                    Прихожая   \n",
       "397529   Кордиант 215/55/16 Летние   \n",
       "584569                        Стол   \n",
       "2513100                 Комбинезон   \n",
       "1091886                   Ветровка   \n",
       "\n",
       "                                               description  \\\n",
       "id                                                           \n",
       "382220                           В хорошем состоянии. Торг   \n",
       "397529   Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...   \n",
       "584569   Стол, 2 рабочих места . Стол серого цвета, в д...   \n",
       "2513100                                       Размер-42/44   \n",
       "1091886                                          На 2 года   \n",
       "\n",
       "                     Category_name  Category  \n",
       "id                                            \n",
       "382220           Мебель и интерьер        20  \n",
       "397529       Запчасти и аксессуары        10  \n",
       "584569           Мебель и интерьер        20  \n",
       "2513100  Одежда, обувь, аксессуары        27  \n",
       "1091886     Детская одежда и обувь        29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train_subset.csv\", index_col='id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Kg8iPp7fiwGh",
    "outputId": "96ed00ed-b63b-4478-f2d4-66bda1110b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1hvzAMETU2d"
   },
   "outputs": [],
   "source": [
    "X = data[['title', 'description']].to_numpy()\n",
    "y = data['Category'].to_numpy()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMYU7zZw_cw-"
   },
   "source": [
    "Сразу разделим выборку на train и test.\n",
    "Никакие данные из test для обучения использовать нельзя!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fia4_3vNprp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "qDR8LtTJUIGt",
    "outputId": "fd4d5b55-a023-4129-9ff5-a6e8e24db915"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Сапоги 46 размер новые', 'Сапоги 46 размер новые'],\n",
       "       ['Светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку. В эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iPhone 7 plus 128GB Red красный в наличии',\n",
       "        '\\xa0/\\n/\\n Данная цена только для подписчиков Instagram: iQmac/\\n/\\n Новый красный айфон 7 Plus в наличии это элегантный и мощный смартфон, который готов в полной мере раскрыть новые возможности iOS 10. Аппарат с 4-ядерным процессором А10 и 3 ГБ ОЗУ с легкостью решает самые ресурсоемкие задачи, позволяя наслаждаться быстродействием «тяжелых» приложений и игр на 5,5-дюймовом дисплее. Аппарат получил экран, как у iPad Pro, так что картинка теперь соответствует кинематографическому стандарту.'],\n",
       "       ['Пион Ирис Ромашка рассада',\n",
       "        'Пион куст 500 р ( более 10 шт)/\\nСаженец/ корень 100р/\\nРастут у нас более 70 лет/\\nРозовые, бордовые и белые/\\nНа фото цветы 2018г/\\nП. Зубчаниновка/\\nлибо пл. Революции/\\nЕсть ирисы, ромашка, клубника, боярышник и ирга'],\n",
       "       ['Кофта', 'Состояние отличное']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-ZEdlEGAXTD"
   },
   "source": [
    "### Токенизация (1 балл)\n",
    "\n",
    "\n",
    "Токенизация -- разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
    "Можно использовать разные алгоритмы токенизации.\n",
    "Давайте пока остановимся на простом WordPunctTokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "O9VgNlZ1Qy3o",
    "outputId": "59ef3a75-008e-47c5-fba8-a319eba13ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
      "after: здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    return ' '.join(tokenizer.tokenize(text.lower()))\n",
    "\n",
    "\n",
    "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
    "print(\"before:\", text)\n",
    "print(\"after:\", preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_RYBKC26o1X"
   },
   "source": [
    "__Задание:__ Токенизируйте title и description в train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Очень долго пытался понять, как применить функцию ко всем элементам матрицы и уже почти начал запускать вложенный цикл\n",
    "#Но оказалось, что нампи шире, чем я думал\n",
    "toktok = np.vectorize(preprocess)\n",
    "X_train = toktok(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "id": "kARGeJQwYTil",
    "outputId": "0fa98e11-b4cc-4741-f519-4ba7f110e1b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['сапоги 46 размер новые', 'сапоги 46 размер новые'],\n",
       "       ['светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку . в эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iphone 7 plus 128gb red красный в наличии',\n",
       "        '/ / данная цена только для подписчиков instagram : iqmac / / новый красный айфон 7 plus в наличии это элегантный и мощный смартфон , который готов в полной мере раскрыть новые возможности ios 10 . аппарат с 4 - ядерным процессором а10 и 3 гб озу с легкостью решает самые ресурсоемкие задачи , позволяя наслаждаться быстродействием « тяжелых » приложений и игр на 5 , 5 - дюймовом дисплее . аппарат получил экран , как у ipad pro , так что картинка теперь соответствует кинематографическому стандарту .'],\n",
       "       ['пион ирис ромашка рассада',\n",
       "        'пион куст 500 р ( более 10 шт )/ саженец / корень 100р / растут у нас более 70 лет / розовые , бордовые и белые / на фото цветы 2018г / п . зубчаниновка / либо пл . революции / есть ирисы , ромашка , клубника , боярышник и ирга'],\n",
       "       ['кофта', 'состояние отличное'],\n",
       "       ['1 - к квартира , 33 м² , 4 / 5 эт .',\n",
       "        'продаётся уютная , тёплая квартира в экологически - чистом районе города , рядом сосновый бор , всегда чистый воздух . дом 2004 г ., хорошие соседи , на площадке 2 - е квартиры , развитая инфраструктура , в шаговой доступности поликлиника , школа , тк « орбита », вещевой рынок . квартира в хорошем состоянии . подходит под ипотеку , долгов , обременений , перепланировке нет . в квартире натяжные потолки , в ванной комнате стены выполнены из влагостойких стеновых панелей . возможен обмен на квартиру в г . магнитогорске , торг .'],\n",
       "       ['платье новое 60 размера',\n",
       "        'платье 60 размера , новое , красивого темно синего цвета , из трикотажной ткани : вискоза 95 %, эластина 5 % . а - образного силуэта с рукавом 2 / 3 . длинна по спинке 113см .'],\n",
       "       ['ваз 2114 samara , 2007',\n",
       "        'продам ваз 2114 2007 г . в . в хорошем состоянии . / 2 владельца , птс оригинал . / машина в родной краске , в дтп никогда не была ,/ днище целое не ржавое . по ходовой нареканий нет , сел и поехал . / имеется музыка , сигнализация 2 комплекта ключей , птф , передние стеклоподъемники ./ небольшой торг при осмотре . / обмен не интересует .'],\n",
       "       ['наушники блутус',\n",
       "        'долго держат заряд 4 - 5 часов , можно и больше при средней громкости выжать из них . вкладыши .'],\n",
       "       ['пальто tommy hilfiger',\n",
       "        'состояние нового . промахнулась с размером . пальто до - 10 - 12 градусов . / возможна пересылка по почте']],\n",
       "      dtype='<U3491')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDnDSWwFDwFo"
   },
   "outputs": [],
   "source": [
    "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlIITUk0AsmS"
   },
   "source": [
    "### BOW (1.5 балла)\n",
    "\n",
    "Один из традиционных подходов -- построение bag of words.\n",
    "\n",
    "Метод состоит в следующем:\n",
    "\n",
    " - Составить словарь самых часто встречающихся слов в train data\n",
    " - Для каждого примера из train посчитать, сколько раз каждое слово из словаря в нём встречается\n",
    "\n",
    "\n",
    " В sklearn есть CountVectorizer, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMKUttDWIF92"
   },
   "source": [
    "__Задание:__ Найдите k самых частых слов, отсортируйте их по убыванию частотности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Сконкатенировать описание и заглавие\n",
    "2. Расщепить полученное на слова\n",
    "3. Для каждого слова подсчитать частоту встречаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция для разбиения элементов\n",
    "def splitter(mas):\n",
    "    return mas.split()\n",
    "\n",
    "split_all = np.vectorize(splitter)\n",
    "\n",
    "#Функция для создания одного огромного списка из нашего нампи объекта\n",
    "def lister(mas, pre_voc):\n",
    "    for i in range(len(mas)):\n",
    "        pre_voc.append(split_all(str(mas[i])))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Конкатенируем\n",
    "bow_X_train = np.hstack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEVE_bzkRBx0"
   },
   "outputs": [],
   "source": [
    "#Расщепляем все строчки на отдельные слова\n",
    "pre_voc = []\n",
    "lister(bow_X_train, pre_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делаем словарь, подсчитывающий значения\n",
    "#Около 30 минут грузилось, это явно не оптимальное решение задачи...\n",
    "from collections import Counter\n",
    "bow_vocabulary = Counter([])\n",
    "for i in range(len(pre_voc)):\n",
    "    bow_vocabulary = bow_vocabulary + Counter(pre_voc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vocabulary = sorted(bow_vocabulary, key = bow_vocabulary.get, reverse = True)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTs70ZxVbk0J"
   },
   "outputs": [],
   "source": [
    "assert sorted(bow_vocabulary)[::200] == ['!', '12500', '270', '700', 'by', 'gh', 'michael', 'sonata', 'ø', 'аудиоподготовка', 'большим', 'веса', 'воспроизведения', 'габариты', 'гтд', 'джинсами', 'доступность', 'загрузки', 'зимней', 'использовался', 'квартала', 'коммуникации', 'кошки', 'лакированные', 'магазин', 'металл', 'мск', 'натуральным', 'носке', 'одному', 'отвечаем', 'пассат', 'плотно', 'покраску', 'постоянные', 'примеры', 'просьба', 'размещайте', 'репетитор', 'сантехник', 'сидения', 'современного', 'стала', 'схема', 'тон', 'удлиненная', 'фасад', 'цветами', 'шея', 'эту']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указано количество его употреблений\n",
    "    \"\"\" \n",
    "    a=text.split()\n",
    "    b=[]\n",
    "    for i in bow_vocabulary:\n",
    "        if i in a:\n",
    "            b.append(bow_vocabulary.index(i))\n",
    "        else:\n",
    "            b.append(0)\n",
    "    return np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,  0,  0,\n",
       "        0,  0,  0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_bow('сдаётся уютный , тёплый гараж для стартапов в ml')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZnJT2JbdXA3"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.where(text_to_bow(\"сдаётся уютный , тёплый гараж для стартапов в ml\") != 0)[0],\n",
    "                   np.array([   1,    4,   12,  565,  866, 1601, 2539, 4063])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR_D8Fn4pudv"
   },
   "outputs": [],
   "source": [
    "\n",
    "def items_to_bow(items: np.array) -> np.array:\n",
    "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "    # Давайте для начала попробуем строить bow только из description товара\n",
    "    # assert ниже написан для bow из description\n",
    "    a=[]\n",
    "    b=[]\n",
    "    lister(items, a)\n",
    "    for j in range(len(items)):\n",
    "        for i in bow_vocabulary:\n",
    "            if i in a[j]:\n",
    "                b.append(bow_vocabulary.index(i))\n",
    "            else:\n",
    "                b.append(0)\n",
    "    return np.array(np.reshape(b, (-1, 10000)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,    2,    5,    6,    7,   12,   27,   41,   49,  110,  189,\n",
       "         208,  221, 2032, 3052, 7179, 9568], dtype=int64),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(text_to_bow('узбекские сумы ./ каждая купюра за 200 руб . / или обмен на банкноты и монеты азербайджана , пакистана , бангладеш , афганистана , непала ./ также возможен обмен на альбомы для бон и монет .')!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a=[]\n",
    "#lister(X_train[42], a)\n",
    "#a\n",
    "#np.where(items_to_bow([X_train[42]])[0]!=0)\n",
    "len(bow_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pKdfMqbIetPA",
    "outputId": "9054ba46-57a4-4eea-cb3d-5d1764e1d56f"
   },
   "outputs": [],
   "source": [
    "#убрал 0 из ассерта, просто потому что не вижу никакоого способа выдать True для проверки 0 != 0\n",
    "assert np.allclose(np.where(items_to_bow([X_train[42]])[0] != 0),\n",
    "                   np.array([   1, 2, 5, 6, 7, 12, 27, 41, 49, 110,\n",
    "                                189,  208,  221, 2032, 3052, 7179, 9568]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "wwOZaEpMSQsZ",
    "outputId": "8a30c3af-3517-42bd-a5f3-36206b4b264a"
   },
   "outputs": [],
   "source": [
    "#Опять минут на 40 можно отойти\n",
    "X_train_bow = items_to_bow(X_train)\n",
    "X_test_bow = items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJVLS8Fs3CeT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJoXiCWI7VF5"
   },
   "source": [
    "### Логистическая регрессия и SVC (1 балл)\n",
    "\n",
    "\n",
    "Теперь описание каждого товара представлено, как точка в многомерном пространстве.\n",
    "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
    "\n",
    "Для BOW каждое измерение в пространстве -- какое-то слово.\n",
    "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
    "\n",
    "Обучите логистическую регрессию и SVC с базовыми параметрами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 10000)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#проверим, что размерность матрицы именно такая, какой мы ожидаем её увидеть\n",
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сделаем спарс-матрицу, хранящую только ненулевые значения\n",
    "sparce_X_train_bow = csr_matrix(X_train_bow)\n",
    "sparce_X_test_bow = csr_matrix(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21000x10000 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 694704 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparce_X_train_bow\n",
    "#Количество необходимых для обработки объектов уменьшилось примерно в 302 раза, что весьма неплохо облегчит жизнь компьютеру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "Ky3HV1rTSS9L",
    "outputId": "612a5f0d-76bd-44f4-eeeb-63b517443797"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-234-5ed3f90edd6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_model = LogisticRegression(max_iter=100).fit(sparce_X_train_bow, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_bow), y_test)\n",
    "\n",
    "assert accuracy_score(bow_model.predict(X_test_bow), y_test) > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4231111111111111\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(bow_model.predict(sparce_X_test_bow), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-c46ZT0lvF6T",
    "outputId": "4b1cb34a-201b-4dc2-9155-fdb6919c6c08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3487777777777778\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-7b9a4bfeaabe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparce_X_test_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparce_X_test_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.68\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "bow_model = LinearSVC(max_iter=70).fit(sparce_X_train_bow, y_train)\n",
    "print(accuracy_score(bow_model.predict(sparce_X_test_bow), y_test))\n",
    "\n",
    "assert accuracy_score(bow_model.predict(sparce_X_test_bow), y_test) > 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оба assert'а не прошли, поскольку качество сделанного BOW оказалось весьма низким. Я не знаю, как это исправить, не прибегая к последующим пунктам задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwKE57YZ1Hzn"
   },
   "source": [
    "### Модификация признаков (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMlxQezL6Ax"
   },
   "source": [
    "Добавьте title товара в bow с произвольным весом, как изменится качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evqKo1r5L5BO"
   },
   "outputs": [],
   "source": [
    "X_train_title = items_to_bow(np.array(X_train[::, 0]))\n",
    "sparce_X_train_title_bow = csr_matrix(X_train_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_title = items_to_bow(np.array(X_test[::, 0]))\n",
    "sparce_X_test_title_bow = csr_matrix(X_test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparce_modified_train = hstack((sparce_X_train_title_bow, sparce_X_train_bow), format='csr')\n",
    "sparce_modified_test = hstack((sparce_X_test_title_bow, sparce_X_test_bow), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42377777777777775"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#качество модели улучшилось на 0.0006. Видимо, что-то принципиально неправильное в каком-то из предыдущих заданий\n",
    "bow_model = LogisticRegression(max_iter=100).fit(sparce_modified_train, y_train)\n",
    "accuracy_score(bow_model.predict(sparce_modified_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35355555555555557\n"
     ]
    }
   ],
   "source": [
    "#качество модели улучшилось на 0.05, уже что-то!\n",
    "bow_model = LinearSVC(max_iter=70).fit(sparce_modified_train, y_train)\n",
    "print(accuracy_score(bow_model.predict(sparce_modified_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество моделей при добавлении такой же по размерности спарс-матрицы с более информативными (по идее) значениями **не привело к кардинальному улучшению качества**, что ввело меня в сильную грусть. Убеждаюсь всё сильнее в том, что не хочу заниматься анализом текстов, когда вырасту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Это будет некрасиво, неээфективно и неэстетично, но да:\n",
    "another_X_train, another_X_test = [], []\n",
    "for i in range(len(X_train)):\n",
    "    another_X_train.append(str(X_train[i][0])+ ' ' + str(X_train[i][1]))\n",
    "for i in range(len(X_test)):\n",
    "    another_X_test.append(str(X_test[i][0]) + ' ' + str(X_test[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_modi = items_to_bow(another_X_train)\n",
    "sparce_X_train_modi_bow = csr_matrix(X_train_modi)\n",
    "\n",
    "X_test_modi = items_to_bow(another_X_test)\n",
    "sparce_X_test_modi_bow = csr_matrix(X_test_modi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21000x10000 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 733660 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparce_X_train_modi_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41555555555555557\n",
      "0.383\n"
     ]
    }
   ],
   "source": [
    "bow_model = LogisticRegression(max_iter=100).fit(sparce_X_train_modi_bow, y_train)\n",
    "print(accuracy_score(bow_model.predict(sparce_X_test_modi_bow), y_test))\n",
    "\n",
    "bow_model = LinearSVC(max_iter=70).fit(sparce_X_train_modi_bow, y_train)\n",
    "print(accuracy_score(bow_model.predict(sparce_X_test_modi_bow), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCAL3qGDByj"
   },
   "source": [
    "### mystem (1.5) балла\n",
    "\n",
    "Попробуйте обучиться, используя токенизатор mystem. Сравните качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60oQ-6UgDcLF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading https://files.pythonhosted.org/packages/00/8c/98b43c5822620458704e187a1666616c1e21a846ede8ffda493aabe11207/pymystem3-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: requests in c:\\users\\481\\anaconda3\\lib\\site-packages (from pymystem3) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\481\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\481\\anaconda3\\lib\\site-packages (from requests->pymystem3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\481\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\481\\anaconda3\\lib\\site-packages (from requests->pymystem3) (1.24.2)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGvNHfVsDfhq"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(item):\n",
    "    a=[]\n",
    "    for i in range(item.shape[0]):\n",
    "        for j in range(item.shape[1]):\n",
    "            a.append(''.join(m.lemmatize(item[i][j].lower())))\n",
    "    return np.reshape(a, (-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['сапог 46 размер новый\\n', 'сапог 46 размер новый\\n'],\n",
       "       ['прыгунка jetem auto\\n',\n",
       "        'музыкальный прыгунка , в хороший состояние\\n'],\n",
       "       ['2 - к квартира , 45 м² , 1 / 2 эта .\\n',\n",
       "        'пожалуй , самый надежный вложение во время нестабильный рубль и опасность банкротство застройщик - это квартира в центр !/ / 2 - х к . квартира от собственник . в самый центр г . ставрополь . под отделка , позволять воплощать самый смелый замысел и дизайнерский решение . в дополнение прилагаться вариант подходящий дизайн . / / большой окно , высоченный потолок 3 , 6м позволять сделать второй ярус . возможно использовать подвал при желание . двор в полный распоряжение . / / дом теплый , толстый стена . иметься закрытый двор , собственный отдельный вход - пристройка , возможно расширять на 9 - 12 м2 и сделать оранжерея . новый общий площадь мочь составлять около 60 м2 и даже много . / / идеально под сдача посуточно или бутик - отель или просто для жизнь в самый центр город , событие , жизнь , тусовка .../ / один взрослый собственник . согласие супруга получать . обременение нет . свидетельство современный образец . / / обращаться , если иметь серьезный намерение . агент вознаграждение !/ / ps номер телефон подменный , так что писать сюда или звонить\\n'],\n",
       "       ['great wall hover h3 , 2011\\n',\n",
       "        'в отличный состояние , только требовать ремонт шм ( подключение привод ), замена левый передний привод и некачественный покраска - требовать полировка . поэтому быть хороший торг . комплект зимний резина , автономный подогрев webasto , фаркоп ./ / продавать в связь с ремонт новый квартира . закрывать кредит .\\n'],\n",
       "       ['дом 100 м² на участок 1 сотня .\\n',\n",
       "        'дом на мамайка . / сдаваться отдельный этаж , с отопление , 100 кв . м ., с отдельный вход и свой парковка и потрясать вид на море !!!! кухня , 2 изолированный комната : 50кв . м . и 30 кв . м ., в каждый комната свой санузел . все необходимый для комфортный отдых . в каждый спальня быть большой балкон с вид на море , гора и город . / - в дом абсолютно новый ремонт . / - чистый морской и горный воздух / - тихий безопасный район / - до дикий пляж 800 метр , местность гористый , спуск под горка , обратно приходиться проходить в гора , но море чистый , немноголюдный , можно рыбачить , жарить шашлык ./ - до городской оборудовать пляж \" ласточка \" , фазатрон - 1 остановка на автобус , подход к море ровный , весь развлечение и кафэ , ресторан на пляж быть ../ зона барбекю и детский площадка нет . / дом располагать на крутой горка . тот кто не любить активный отдых и предпочитать ленивый пляжный туризм - до море хорошо добираться на машина , автобус или такси на пляж фазатрон , ласточка , т . к . спуск к дикий пляж крутой . в сочи мамайский пляж - единственный место , где можно припарковывать авто на пляж бесплатно . / - ж \\\\ д станция \" мамайка \" и вы в любой точка большой сочи ( имеретинка , красный поляна , олимпийский парк )/ - конечный остановка автобус 100м от дом , спуск под горка , на остановка 2 минимаркет ./ - магазин , кафэ , рынок на мамайка , набережная вдоль р . псах - на маршрутка 5 минута . до центр 20 минута на автобус или на авто ( каршеринг ) 10 минута . парк ривьера , морпорт , органный зал , фестивальный .../ - горячий вода всегда ( свой газовый котел )/ - свой парковка / размещение только до 6 человек !!!/ можно снимать полдома от 1000 р ./ сут . только до 2 чело ., в зимний сезон ./ сентябрь 2500 - 3000р ./ сут , октябрь - ноябрь 2000 р / сут , декабрь - январь 2500 р / сут , февраль - март 2500 р / сут , апрель - 2500 р / сут .. май - 2500 - 3000 , июнь - 3000 - 3500 , июль , август - 3500 / / писать , звонить , быть рад подсказывать .\\n'],\n",
       "       ['компрессор кондиционер toyota camry acv30\\n',\n",
       "        'только у мы , квалифицированный установка ./ ./ компрессор кондиционер toyota camry acv30 . подходить на весь тайот с двигатель 1az 2az 1azfe 2azfe / ./ • отличный вариант для тот кто хотеть сэкономить !/ ✓ дополнительный фото деталь по ваш запрос ./ --------------------------------------------------------------------------------------------------/ ✓ мы продавать в розница и опт ! гарантия качество ! сотрудничать с магазин !/ ✓ если вы понравиться наш объявление сделать скриншот с номер телефон или добавлять он в раздел избранный / ✓ писать , звонить , viber , whats app ,/ ✓ отправлять транспортный компания по россия или автобус по юфо и скфо / ✓ возможный оплата сразу после отправка транспортный компания , то . быть мы вы фото запчасть на терминал тк и накладной отправка , а вы делать звонок на тк , подтверждаться и отправлять мы деньги на карта сбербанк )), справедливый и безопасный вариант взаимоотношение / ✓ мы на авито более 3 год ))) хороший знак качество !!!! и порядочность продавец\\n'],\n",
       "       ['косынка шелк\\n', 'косынка 50 × 50см\\n'],\n",
       "       ['2 - к квартира , 40 м² , 1 / 2 эта .\\n',\n",
       "        'сдавать квартира на длительный срок некурящий . квартира в самый экологически чистый район город новороссийск , рядом можжевеловый лес , красивый вид с поляна на гора и море , до родник 15 мина . пешком , до остановка 5 мина , до море 3 остановка , до город 10 мина . на маршрутка . горячий вода круглосуточно .\\n'],\n",
       "       ['платье джинсовый р . 44\\n', 'ни раз не одевать\\n'],\n",
       "       ['renault logan генератор\\n',\n",
       "        'рено логан с 2005г генератор б / у / фаза 1 без кондиционер и гур / 8200660053 / отправка по регион\\n'],\n",
       "       ['комбинезон\\n', 'продавать комбинезончик . размер 62 .\\n'],\n",
       "       ['шуба из элитный мех\\n',\n",
       "        'закрытие сезон / в наш шоурум начинаться распродажа !/ соболь от 140тр / шиншилла от 150тр / куница от 150тр / норка / инст - м ladyfur_lux_shop\\n'],\n",
       "       ['беспроводной мышь logitech m187\\n',\n",
       "        'мышка рабочий , почти не использоваться . немного пожелтеть белый пластик\\n'],\n",
       "       ['i5 2400\\n',\n",
       "        'работать без перегрев ( стабильно ), причина продажа покупка i7 3770\\n'],\n",
       "       ['распошивальный машина janome cover pro 7\\n',\n",
       "        'в наличие в тц виктория , ул . фрунзе 15а , центральный вход в цокольный этаж ( подвал )! в наличие также большой выбор и другой модель !/ гарантия 24 мес ./ janome cover pro 7 выполнять 14 вариант плоский шов и цепной стежок . плоскошовный машина от известный японский бренд предлагать простой управление и надежность по адекватный цена ./ / машина шить 3 / 4х ниточный плоский строчка с разный ширина и выполнять 2х ниточный цепной стежок . простой настройка параметр и несложный заправка нить отмечать в большинство отзыв . модель позволять использовать узкий платформа для рукав , манжета и другой цилиндрический деталь . регулировка давление лапка и дифференциальный подача помогать получать равномерный строчка на разный материал .\\n'],\n",
       "       ['earpods с разъем lightning\\n',\n",
       "        'продавать earpods с разъем lightning . наушники новый . прилагаться к iphone , однако иметь версия беспроводный airpods ./ / звонить / писать , частенько бывать на дача , договариваться )\\n'],\n",
       "       ['idiotbox static fuzz\\n',\n",
       "        'чумовой фузза от американский бутиковый бренд idiotbox . / в идеал , прямиком из америка , не играть ./ звонить , тут отвечать долго .\\n'],\n",
       "       ['котенок\\n',\n",
       "        'милый игривый котенок искать дом . 8 сентябрь исполняться 2 месяц . кушать весь . оба мальчик\\n'],\n",
       "       ['кровать односпальный\\n',\n",
       "        'продавать кровать односпальный с матрас , б / у , полировать , материал -- дерево , на ножка стоять крепко , не шататься . торг .\\n'],\n",
       "       ['шорты - плавкиd & g арт .: 194048\\n',\n",
       "        'сток / ✔ размер : s , m , l , xl , xxl / ✔ делать турок 🇹🇷/ ✔ качество 🔥🔥🔥/ / доставка 🚀 по москва 300 ₽ или самовывоз м . выхино ./ отправка 📦 в регион .../ / # купитьплавок , # купитьшорты , # купитьплавкимосква , # брендовыешорты , # брендовыеплавка\\n'],\n",
       "       ['финский горнолыжный куртка icepeak ( марк . 140 )\\n',\n",
       "        'состояние отличный ./ продавать зимний финский горнолыжный куртка айспик . маркировка 140 . качественный вещь для прогулка и занятие активный вид спорт . не промокать , не продуваться , дышать , держать тепло . куртка на молния и кнопка полностью утеплять манжета рукав фиксироваться липучка + внутренний манжета из lycra с пррезь для большой палец микрофлисовый подкладка на внутренний сторона воротник съемный капюшон , с регулировка объем , фиксироваться кнопка регулировка нижний край куртка защитный съемный юбка с противоскользящий кромка замер длина рукав от шов ворота до конец манжет 64 . 5 см ( без внутренний манжет ) внутренний шов рукав 45 см ширина ( под подмышка ) 42х2 см длина по спина от шов ворота до конец куртка 56 см ( зад чуть длинный перед ) жить 10 минута пешком от метро озерко . мочь пересылать почта .\\n']],\n",
       "      dtype='<U1986')"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#эта штука из 21 элемента запускалась примерно 3 минуты. \n",
    "#Если я запущу для всей выборки, \n",
    "#то процесс займёт около 40 часов.\n",
    "#До дедлайна остаётся 27 часов, поэтому я пас, спасибо\n",
    "stem(X_train[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я запишу алгоритм действий для этой задачи, но точных аутпутов здесь не будет, поскольку стемматизация стала непреодолимой горой для моего ноутбука, а освоение каких-то других способов провести эту самую стемматизацию стало непреодолимой горой для меня. **Как позже выяснится, смогу сделать в кеггле.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стемматизация\n",
    "X_train_stem = stem(X_train)\n",
    "X_test_stem = stem(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW-представление\n",
    "X_train_stem_bow = items_to_bow(X_train_stem)\n",
    "sparce_X_train_stem_bow = csr_matrix(X_train_stem_bow)\n",
    "\n",
    "X_test_stem_bow = items_to_bow(X_test_stem)\n",
    "sparce_X_test_stem_bow = csr_matrix(X_test_stem_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "bow_model = LogisticRegression(max_iter=100).fit(sparce_X_train_stem_bow, y_train)\n",
    "accuracy_score(bow_model.predict(sparce_X_test_stem_bow), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "bow_model = LinearSVC(max_iter=70).fit(sparce_X_train_stem_bow, y_train)\n",
    "accuracy_score(bow_model.predict(sparce_X_test_stem_bow), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPD\n",
    "\n",
    "Попробовал запустить этот кусок кода на Kaggle. Для LogisticRegression accuracy=0.5179, для SVC - 0.421.\n",
    "Ссылка на джупайтер с этим куском года, запущенным на Kaggle: https://yadi.sk/d/65pqN8Lk6pn3wA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXbsPtpfoB7m"
   },
   "source": [
    "### TF-IDF (1.5 балла)\n",
    "\n",
    "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
    "\n",
    "\n",
    "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "> \n",
    "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\n",
    "В sklearn есть TfidfVectorizer, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_arr = csr_matrix((21000, 10000), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yIeoic7o3ES"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-6a4da969f84a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparce_X_train_modi_bow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#это BOW в виде спарс-матрицы для всех элементов\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m          \u001b[1;32mif\u001b[0m \u001b[0msparce_X_train_modi_bow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mcount_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, index, x)\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_set_many\u001b[1;34m(self, i, j, x)\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insert_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_zero_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_insert_many\u001b[1;34m(self, i, j, x)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;31m# update attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_parts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_parts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[0mnnzs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Давайте для простоты считать один tf-idf для title и description.\n",
    "# Для каждого слова из bow_vocabulary нужно посчитать\n",
    "#   1. Сколько раз оно встретилось в title и description во всём X_train \n",
    "#(матрица с количеством каждых слов в каждом объявлении, 21 000*10 000(кол-во слов k))\n",
    "#   2. В тексте скольких товаров встретилось это слово\n",
    "#\n",
    "for i in range(len(another_X_train)): #Это список title+description по всем элементам\n",
    "    for j in range(sparce_X_train_modi_bow.shape[1]): #это BOW в виде спарс-матрицы для всех элементов\n",
    "         if sparce_X_train_modi_bow[i, j] != 0:\n",
    "                count_arr[i, j]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я не совсем понял, что делаю в инпуте выше. Есть ощущение, что на выходе это будет матрица только из 0 и 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#если верить семинару 10, то там 3 переменных для функции TF-IDF, обозначим их\n",
    "def TF_IDF_counter(word, row, dataframe):\n",
    "    TF = count_arr[another_X_train.index(row), word] / np.sum([count_arr[another_X_train.index(row), i] for i in range(len(another_X_train.index(row)))])\n",
    "    IDF = np.log(np.shape(dataframe)[0] / count_arr[another_X_train.index(row), word])\n",
    "    return TF*IDF\n",
    "#окончательно запутался, до логичного завершения этот и последующий пункты по TF-IDF уже не доведу. Голова сварилась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше логика действий почти идентична составлению BOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6i5zFpD9rbtz"
   },
   "outputs": [],
   "source": [
    "def text_to_tfidf(text, another):\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указан tf-idf\n",
    "    \"\"\"\n",
    "\n",
    "    a = []\n",
    "    for i in bow_vocabulary:\n",
    "        if i in text:\n",
    "            a.append(TF_IDF_counter(i, text, another))\n",
    "        else:\n",
    "            a.append(0)\n",
    "    \n",
    "    return np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TF_IDF = []\n",
    "for i in another_X_train:\n",
    "    X_train_TF_IDF.append(text_to_tfidf(i, another_X_train))\n",
    "\n",
    "X_test_TF_IDF = []\n",
    "for i in another_X_test:\n",
    "    X_test_TF_IDF.append(text_to_tfidf(i, another_X_test))\n",
    "    \n",
    "X_train_TF_IDF = np.array(np.reshape(X_train_TF_IDF, (-1, 10000)))\n",
    "X_test_TF_IDF = np.array(np.reshape(X_test_TF_IDF, (-1, 10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YFA-8kE1RHk"
   },
   "source": [
    "### Модели на TF-IDF признаках (1 балл)\n",
    "\n",
    "Обучите логистическую регрессию и SVC, оцените качество (accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ULrXsF1m5sU"
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "bow_model = LogisticRegression(max_iter=100).fit(X_TF_IDF, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_TF_IDF), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "bow_model = LinearSVC().fit(X_TF_IDF, y_train)\n",
    "accuracy_score(bow_model.predict(X_test_TF_IDF), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFdy3lUFDsOr"
   },
   "source": [
    "### Hashing Vectorizer (0.5 балла)\n",
    "\n",
    "Попробуйте использовать `sklearn.feature_extraction.text.HashingVectorizer` для векторизации текстов.\n",
    "Обязательно оцените качество работы алгоритмов классификации с использованием новой векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y666HTrqDq1m"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_hash = []\n",
    "for i in range(0, len(pre_voc) ,2):\n",
    "    voc_hash.append(str(pre_voc[i])+str(pre_voc[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_more_X_voc = []\n",
    "for i in range(len(X_test)):\n",
    "        one_more_X_voc.append(str(X_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Возможно, в задаче с модификацией данных было необходимо использовать именно два инпута выше, а не те премудрости с удвоенными спарс-матрицами. Но в этом случае решение было бы очевидным и можно просто подставить voc_hash & one_more_X_voc в items_to_bow ->  сделать sparse-matrix -> подставить в модели.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = HashingVectorizer()\n",
    "Hash_X = vec.transform(voc_hash)\n",
    "Hash_X_test = vec.transform(one_more_X_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7153333333333334"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "bow_model = LogisticRegression(max_iter=100).fit(Hash_X, y_train)\n",
    "accuracy_score(bow_model.predict(Hash_X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227777777777778"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC\n",
    "bow_model = LinearSVC(max_iter=70).fit(Hash_X, y_train)\n",
    "accuracy_score(bow_model.predict(Hash_X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я в шоке. Модель SVC дала такой accuracy score, который дал бы мне 2 дополнительных балла, если бы я был в ИАД-4! Похоже, HashingVecotizer - реально мощная тема, а вот моё умение строить BOW вручную - реально что-то унылое. **Пожалуйста, дайте доп баллы :(**\n",
    "\n",
    "По крайней мере, хотя бы через HashVectorizer ассерты из первоначальных базовых моделей логистической регрессии и SVC прошли бы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZ61xSsTpZI"
   },
   "source": [
    "### Word Vectors (1.5 балла)\n",
    "\n",
    "Эта часть домашнего задания выполнена через Kaggle, запущенный там джупайтер лежит в яндекс.диске(ссылка уже была в самом начале файла):\n",
    "https://yadi.sk/d/65pqN8Lk6pn3wA\n",
    "\n",
    "https://yadi.sk/i/QCBPmpJ-ZrM5qA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVEdlFostSnX"
   },
   "source": [
    "### Что дальше?\n",
    "\n",
    "Решение каждого пункта 2 балла:\n",
    "\n",
    "    N-Gram модели текстовой классификации\n",
    "        Признаки - mystem\n",
    "        n-gramm'ы - несколько слов подряд объединяются в один токен\n",
    "        Модели - только логистическая регрессия\n",
    "        Настоятельно рекомендуется использовать sparse-матрицы\n",
    "\n",
    "    Использовать Vowpal Wabbit вместо sklearn\n",
    "        Признаки - обычный BoW title+description\n",
    "        Главный вызов - заставить работать библиотеку и подготовить признаки\n",
    "\n",
    "    Другие способы лемматизации (pymorphy2, spaCy)\n",
    "\n",
    "    Снабжайте код пояснениями;\n",
    "    Обязательно необходимо написать вывод по каждому пункту, который вы реализуете.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pumorphy2 (в попытках заработать 2 доп балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\users\\481\\anaconda3\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: dawg-python>=0.7 in c:\\users\\481\\anaconda3\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\481\\anaconda3\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in c:\\users\\481\\anaconda3\\lib\\site-packages (from pymorphy2) (2.4.393442.3710985)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2-dicts-ru\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
      "Installing collected packages: pymorphy2-dicts-ru\n",
      "Successfully installed pymorphy2-dicts-ru-2.4.404381.4453942\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pymorphy2-dicts-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'думать'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse('думающему')[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['/', 'платья', 'браслет', 'паспорта', 'бытовки', 'альмера', 'шикарные', 'шерстяное', 'тихие', 'годика']\""
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(str(bow_vocabulary[::1000]))[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_voc = []\n",
    "lister(another_X_train, morph_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(21000):\n",
    "    for j in range(len(morph_voc[i])):\n",
    "        morph_voc[i][j] = morph.parse(morph_voc[i][j])[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_morph = []\n",
    "lister(another_X_test, test_morph)\n",
    "for i in range(9000):\n",
    "    for j in range(len(test_morph[i])):\n",
    "        test_morph[i][j] = morph.parse(test_morph[i][j])[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом этапе я осознал, что все имеющиеся модели рано или проходят через items_to_bow, обращающийся к фиксированному давно обработанному bow_vocabulary. Поэтому воспользуюсь быстрым и успешным HashingVectorizer, чтобы оптимизировать получившийся результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(morph_voc)):\n",
    "    morph_voc[i] = str(morph_voc[i])\n",
    "for i in range(len(test_morph)):\n",
    "    test_morph[i] = str(test_morph[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_voc_hash = vec.transform(morph_voc)\n",
    "test_morph_hash = vec.transform(test_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7306666666666667"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "bow_model = LogisticRegression().fit(morph_voc_hash, y_train)\n",
    "accuracy_score(bow_model.predict(test_morph_hash), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8168888888888889"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC\n",
    "bow_model = LinearSVC().fit(morph_voc_hash, y_train)\n",
    "accuracy_score(bow_model.predict(test_morph_hash), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, полученные через pymorphy2 результаты не улучшили полученные через HashingVectorizer результаты. Однако модель показывает весьма высокую точность, что показывает качество pymorphy2. Также это в разы быстрее, чем mystem.\n",
    "\n",
    "Отмечу, что если убрать ограничение на количество итераций, то качество модели никак не изменяется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gramm через HashingVecotrizer и pymorphy2 (в попытках заработать ещё два доп балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = HashingVectorizer(ngram_range = (2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_voc_ngram = ngram_vectorizer.transform(morph_voc)\n",
    "test_morph_ngram = ngram_vectorizer.transform(test_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4266666666666667"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "bow_model = LogisticRegression(solver='newton-cg').fit(morph_voc_ngram, y_train)\n",
    "accuracy_score(bow_model.predict(test_morph_ngram), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\481\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.466"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "bow_model = LogisticRegression(penalty='l1', solver='saga').fit(morph_voc_ngram, y_train)\n",
    "accuracy_score(bow_model.predict(test_morph_ngram), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.604"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC\n",
    "bow_model = LinearSVC().fit(morph_voc_ngram, y_train)\n",
    "accuracy_score(bow_model.predict(test_morph_ngram), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видимо, не всё так просто, как я надеялся. Качество рухнуло на порядок ниже, причём подбор параметров логистической регрессии ситуацию не улучшает. SVC всё ещё лучше, хотя в этом пункте и не рекомендуется использовать его."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "\n",
    "Верните мои нервные клетки."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
