# Домашнее задание 1

Напишите web crawler, обходящий некоторый сайт и скачивающий его страницы.

## input
* url (исходная ссылка)
* depth (глубина обхода)

## output
* urls.txt (файл со ссылками на все вложенные url на заданной глубине обхода)
* data/ (папка с html-файлами вложенных страниц из исходного url)

## Комментарии
* два выходных дня потратил на безуспешные попытки корректно распарсить habr.com/en хотя бы до глубины 2, но постоянно выскакивали неприятные ошибки, не дающие успешно завершить работу программы.
* В итоге в последние часы перед дедлайном решил спарсить html-файлики с простенького сайта, на котором смотрю баскетбол. Проверил, что всё работает корректно на любой глубине
* Показалось, что было бы полезным также задать требование сохранять список с уровнем глубины для конкретной найденной страницы, но поскольку в задаче такого не требовалось на выходе, не хочу попадаться на ловушке перфекционизма
* Постарался учесть всевозможные ошибки, но наверняка учёл не всё

## Как запускать
* скачать папку
* запустить main.py, не изменяя параметр url в вызове функции после if __name__=='__main__':
