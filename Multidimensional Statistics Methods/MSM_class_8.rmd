---
title: "Семинар 8"
author: "Многомерные статистические методы -- 2020-21 гг. -- 3 курс"
date: "02.11.2020"
output:
  pdf_document:
    df_print: paged
  html_document:
    df_print: paged
lang: ru-russian
---


В этом семинаре рассматривается метод снижения размерности данных, а именно метод главных компонент в R:

* выбор оптимального количества комонент итогового преобразованного массива

* оценка матрицы нагрузок на главные компоненты

* визуализация результатов и промежуточных выводов


### Загрузка пакетов и данных

Чтобы определить рабочую директорию текущей сессии и установить ее, где нужно, используем функции:

Сегодня нам понадобятся пакеты:
```{r message=FALSE, warning=FALSE}
#install.packages(c("FactoMineR", "factoextra"))
#install.packages("devtools")
#install.packages("robustHD")
#install.packages("rio")
#install.packages("ellipsis")
#install.packages("corrplot")
#install.packages("psych")
#install.packages("htmlTable")
#install.packages("ggpubr")

library(FactoMineR)
library(factoextra)
library(devtools)
library(rio)
library(corrplot)
library(psych)
library(htmlTable)
library(ggpubr)
```

Упражняемся на данных с прошлых семинаров о ценах на квартиры в Москве, размещенных ВШЭ в открытом доступе на платформе [Kaggle](https://www.kaggle.com/hugoncosta/price-of-flats-in-moscow):

```{r}
data <- import('flats_moscow.xlsx')

data$price <- data$price * 77 / 10 # теперь зависимая переменная измерена в 10 000 руб.

data$walk <- factor(data$walk) # бинарные переменные теперь считываются как факторы
data$brick <- factor(data$brick)
data$floor <- factor(data$floor)

data <- data[, c('price', 'totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor')]

str(data)
```

Часто перед компонентным или факторным анализом, кластеризацией применяют стандартизацию к исследуемому набору данных. Мы хотим соотнести переменные, когда их средние значения и стандартные отклонения сильно разнятся.

```{r}
data[,1:6] <- scale(data[,1:6])
str(data)
```

Для построения главных компонент будем использовать только непрерывные независимые переменные:

```{r}
indep_data <- data[,c('totsp', 'livesp', 'kitsp', 'dist', 'metrdist')]
str(indep_data)
```

Далее приступаем непостредственно к анализу методом главных компонент:
```{r}
res.pca  <- PCA(indep_data, graph=FALSE)
print(res.pca)
```

### Определение оптимального количества главных компонент:

Находим собственные значения (2 способа):

```{r}
res.pca$eig # способ 1
eig.val <-  get_eigenvalue(res.pca) # способ 2
eig.val
 
```

## Методы определения оптимального числа компонент:

Метод 1: Выбираем количество компонент, когда eigenvalue > 1 (метод Кайзера). В нашем случае получаем две новые компоненты.

Метод 2: решаем, какое количество компонент взять по значениям объяснённой дисперсии. Принято брать такое количество, когда первый раз кумулятивная сумма объяснённой дисперсии превышает 70%. Выбираем снова две новые компоненты, как оптимальное количество.

Метод 3: определение по графику каменистой осыпи (выбирают такое количество компонент, при котором происходит резкое падение накопленного значения объяснённой дисперсии):

```{r}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 60))
```

Реализация МГК (здесь будем использовать не ковариацию, а корреляцию между переменными, поэтому количество главных компонент изменилось):

```{r}
eigen(cor(indep_data))$values #собственные значения
eigen(cor(indep_data))$vectors #собственные векторы
```

```{r}
plot(eigen(cor(indep_data))$values, xlab = 'Количество компонент', ylab = 'Собственное значение (Eigenvalue Size)', 
     main = 'График каменистой осыпи', type = 'b', xaxt = 'n')
axis(1, at = seq(1, 6, by = 1))
abline(h = 1, col = "red", lty = 5)
legend("topright", legend = c("Eigenvalue = 1"),
       col=c("red"), lty = 5, cex = 0.6)
```

```{r}
pca <- principal(cor(indep_data), nfactors = 2, rotate = "none", covar = FALSE)
pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 2, nrow = 5)
htmlTable(round(pca_loadings,2), caption = "Таблица нагрузок на компоненты", header = c("PC1", "PC2"),
        rnames = c('totsp', 'livesp', 'kitsp', 'dist', 'metrdist'))
```

Вклады переменных в процентах в каждую компоненту:

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1, top =  10)
fviz_contrib(res.pca, choice = "var", axes = 2, top =  10)
```

Визуализация результатов выделения главных компонент (график корреляций между исходными переменными и новыми компонентами). Важно обратить внимание на то, что переменные, имеющие больший вклад, хорошо выделяются на данном графике:

```{r}
fviz_pca_var(res.pca, col.var = "contrib", gradient.cols = c("blue", "yellow", "red"),
legend.title = "Cont.Var" 
, alpha.var = "contrib" # добавляет прозрачность по вкладу переменной в компоненту
)
```

Корреляционные связи между переменными и главными компонентами (отсортированы по значимости коэффициентов корреляции):

```{r}
res.desc <- dimdesc(res.pca, axes = c(1,2), proba = 0.05)
res.desc$Dim.1
res.desc$Dim.2
```

Визуализация результатов исследования с разбиением отдельно по факторным переменным:

```{r}
fviz_pca_ind(res.pca, geom.ind = "point", col.ind = data$walk, legend.title = "Groups by walk",)
fviz_pca_ind(res.pca, geom.ind = "point", col.ind = data$brick, legend.title = "Groups by brick")
fviz_pca_ind(res.pca, geom.ind = "point", col.ind = data$floor, legend.title = "Groups by floor")
```

Другой способ визуализации результатов с разбиением по фактору floor:

```{r}
ind.p <- fviz_pca_ind(res.pca, geom = "point", col.ind = data$floor)
ggpar(ind.p, title = "Principal Component Analysis", subtitle = "Flats data set", caption = "Source: factoextra", xlab = "PC1", ylab = "PC2", legend.title = "Floor", legend.position = "top", ggtheme = theme_gray())
```


Построение регрессии на две главные компоненты:
```{r}
components <- as.data.frame(res.pca$ind$coord[,1:2])
lm_pca <- lm(data$price ~ components$Dim.1 + components$Dim.2)
summary(lm_pca)
lm <- lm(data$price ~ .,indep_data)
summary(lm)
```

# Самостоятельная работа:

## 1. Построить регрессию на 3 главные компоненты, сравнить с предыдущими результатами, сделать выводы.

По сути, вклад переменных в третью компоненту уже был найден во время семинара, поэтому остаётся лишь проинтерпретировать полученные ранее результаты и добавить в модель регрессии третью компоненту.

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1, top =  10)
fviz_contrib(res.pca, choice = "var", axes = 2, top =  10)
fviz_contrib(res.pca, choice = "var", axes = 3, top =  10)
```
Видно, что зависимость третьей компоненты от объясняющих переменных распределена почти так же, как и для второй компоненты с некоторыми изменениями: она почти полностью нивелирует влияние расстояние до метро на стоимость квартиры (0.756 для второй компоненты и -0.65 для третьей).

```{r}
res.desc <- dimdesc(res.pca, axes = c(1,2,3), proba = 0.05)
res.desc$Dim.1
res.desc$Dim.2
res.desc$Dim.3
```
Видно, что во всех компонентах, кроме второй (где p-value для жилой площади квартиры весьма велико), все непрерывные переменные значимо отличны от 0 и вносят значительный вклад в характер компоненты.
```{r}
components <- as.data.frame(res.pca$ind$coord[,1:3])
lm_pca <- lm(data$price ~ components$Dim.1 + components$Dim.2 +components$Dim.3)
summary(lm_pca)
lm <- lm(data$price ~ .,indep_data)
summary(lm)
```
Сравнивая полученные метрики качества для регрессии на двух компонентах и на трёх, становится очевидно, что вклад третьей компоненты в улучшение понимания факторов, влияющих на цену квартиры, весьма несущественен. Стандартная ошибка остатков регрессии уменьшается лишь на 0.0033, тогда как скорректированный $R^2$ увеличивается лишь на 0,004. Поэтому наличие в оптимальной регрессионной модели третьей компоненты будет здесь избыточно. Это может быть даже вредно, поскольку с третьей компонентной несколько сложнее визуально представить данные и интуитивно понять их суть, а также, выражаясь в терминах машинного обучения, это может привести к переобучению модели, когда значения коэффициентов придают излишнюю важность тем взаимосвязям, которые оказались в наборе данных случайно.

## 2. Добавить в регрессию на главные компоненты имеющиеся бинарные переменные, сравнить с регрессией на исходных данных, сделать выводы. 

Чтобы добавить бинарные переменные, их также сначала нужно отмасштабировать, поскольку иначе построение на них главных компонент не представится возможным:

```{r}
data <- import('flats_moscow.xlsx')
data$price <- data$price * 77 / 10 # теперь зависимая переменная измерена в 10 000 руб.
data <- data[, c('price', 'totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor')]

data[,1:6] <- scale(data[,1:6])
data[,7:9] <- scale(data[,7:9])
str(data)
```

Теперь проведём необходимые операции для составления компонент:
```{r}
exp_data <- data[,c('totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick','floor')]
res_all.pca  <- PCA(exp_data, graph=FALSE)
print(res_all.pca)
```

Находим собственные значения:

```{r}
eig.val <-  get_eigenvalue(res_all.pca)
eig.val
 
```
### Определение оптимального количества главных компонент:
```{r}
fviz_eig(res_all.pca, addlabels = TRUE, ylim = c(0, 60))
```
Возьмём 4 первых компоненты как те, которые объясняют хотя бы 70% дисперсии значений
```{r}
fviz_contrib(res_all.pca, choice = "var", axes = 1, top =  10)
fviz_contrib(res_all.pca, choice = "var", axes = 2, top =  10)
fviz_contrib(res_all.pca, choice = "var", axes = 3, top =  10)
fviz_contrib(res_all.pca, choice = "var", axes = 4, top =  10)
```
Видно, что зависимость четвёртой компоненты от объясняющих переменных распределена почти так же, как и для третьей компоненты, но теперь фактор возможности дойти до метро пешком играет чуть большую роль для компоненты №4.

```{r}
res.desc <- dimdesc(res_all.pca, axes = c(1,2,3,4), proba = 0.05)
res.desc$Dim.1
res.desc$Dim.2
res.desc$Dim.3
res.desc$Dim.4
```
Попробуем интерпретировать компоненты:
1-ая компонента сочетает в себе разновидности площади квартиры и определяет негативную зависимость с расстоянием до центра города. Действительно, интуитивно кажется достаточно хорошим коррелятом цены на квартиру.
2-ая компонента положительно зависит от расстояния до центра города, негативно от факторов возможности дойти до метро пешком и "кирпичности" материала фундамента дома, скорее всего в регрессии у этой компоненты будет отрицательный коэффициент
3-ья компонента впервые уделяет серьёзное внимание расстоянию до метро и этажу расположения квартиры, видимо, это будут достаточно незначительные, но всё же влияющие на цену квартиры факторы
4-ая компонента нивелирует влияние 3-ье компоненты на этаж квартиры и усиливает влияние расстояния до метро.
```{r}
components <- as.data.frame(res_all.pca$ind$coord[,1:4])
lm_pca_all <- lm(data$price ~ components$Dim.1 + components$Dim.2 +components$Dim.3+components$Dim.4)
summary(lm_pca_all) #обученная на всех признаках модель
summary(lm_pca) #обученная только на непрерывных переменных модель
lm <- lm(data$price ~ .,indep_data)
summary(lm) #обычная линейная регрессия без главных компонент
```
По итогам создания регрессии получен достаточно прорывной результат: модель, построенная на главных компонентах, основанных на всех признаках из набора данных, имеет большую предсказательную силу, чем линейная регрессия, построенная через МНК! При этом веса компонент убывают при возрастании номера компонент, что вполне логично. И стандартная ошибка, и скорректированное значение $R^2$ для новой модели оказались на уровень лучше.

## 3. Выбрать наилучшую модель

Наилучшей моделью из представленных выше является та, которая учитывает при анализе все возможные признаки, включая бинарные. Но может быть стоит добавить все 8 главных компонент?
```{r}
#res_all.pca$ind$coord
#res.desc <- dimdesc(res_all.pca, axes = c(1,2,3,4,5,6,7,8), proba = 0.05)
#
#res_all.pca$ind$coord[,1:5]
```
Не совсем понимаю, почему при вызове "res_all.pca$ind$coord" начинает отображаться лишь 5 компонент. Как будто какие-то отклики семинарской части задачи вступают в игру, которые я забыл подправить где-то в коде...

```{r}
components <- as.data.frame(res_all.pca$ind$coord)
lm_pca_all_all_all <- lm(data$price ~ components$Dim.1 + components$Dim.2 +components$Dim.3+ components$Dim.4+ components$Dim.5)
summary(lm_pca_all_all_all)
```

При добавлении 5-ой компоненты скорректированный $R^2$ увеличивается лишь на 0.0008, что не является значительным увличением. Вряд ли при увеличении количества компонент значение увеличится на уровень. Похоже, что предсказательная сила на уровне 0,66 является верхней границей для данного набора данных в силу отсутствия информации о каких-то важных влияющих на цену квартиры факторах (например, отделка квартиры, репутация компании-застройщика или удобность планировки).