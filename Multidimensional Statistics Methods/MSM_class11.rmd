---
title: "Семинар 11"
author: "Многомерные статистические методы -- 2020-21 гг. -- 3 курс"
date: "16.11.2020"
output:
  html_document:
    df_print: paged
  pdf_document:
    df_print: paged
lang: ru-russian
---

В этом семинаре рассматриваются возможности реализации методов многомерной классификации в среде R:

* иерархические методы кластерного анализа

* итеративный алгоритм кластерного анализа k-средних

* визуализация и интерпретация результатов

* определение оптимального числа кластеров

Загрузка пакетов:
```{r}
#install.packages('rio')
#install.packages('factoextra')
library(rio) #для импорта
library(factoextra) #для факторов
```

Импорт данных:
```{r}
data <- import('russian_regions.xlsx')
str(data)
```

```{r}
rownames(data) <- data$`Cубъект РФ`
data <- data[, 2:7]

vars <- colnames(data) # сохраним длинные названия переменных
colnames(data) <- c('income', 'hosp_load', 'morbidity',
                    'emissions', 'tourism', 'rnd_staff') # дадим короткие названия переменным
str(data)
```

```{r}
vars
```

Предварительный визуальный анализ:
```{r}
boxplot(scale(data))
```

```{r}
# построить ящичковые диаграммы для стандартизированных данных

for( j in 1:4) {
  hist(data[, j], breaks = sqrt(nrow(data)),
       xlab = vars[j], ylab = 'Частота', main = 'Гистограмма частот')
}

hist(data[, 5], breaks = sqrt(nrow(data)),
     ylab = 'Частота', main = 'Гистограмма частот',
     xlab = 'ЧИСЛЕННОСТЬ РОССИЙСКИХ ТУРИСТОВ, ОБСЛУЖЕННЫХ \n ТУРИСТСКИМИ ФИРМАМИ (% от численности населения)')

hist(data[, 6], breaks = sqrt(nrow(data)),
     ylab = 'Частота', main = 'Гистограмма частот',
     xlab = 'ЧИСЛЕННОСТЬ ПЕРСОНАЛА, ЗАНЯТОГО НАУЧНЫМИ \n ИССЛЕДОВАНИЯМИ И РАЗРАБОТКАМИ (чел. на 1000 населения)')
```

Можем применить логарифмирование для преобразования данных для кластеризации. Это приближает эмпирическое распределение к нормальному, стабилизирует дисперсию, что должно улучшить результаты кластеризации. Перед применением кластерного анализа рекомендуется стандартизировать признаки (это обязательная процедура для применения евклидовой метрики).
```{r}
complete_data <- data[complete.cases(data), ] # данные без пропусков
cluster_data <- scale(log(complete_data))# логарифмирование и стандартизация
boxplot(cluster_data)

for(j in 1:6) {
  hist(cluster_data[, j], breaks = sqrt(nrow(cluster_data)),
       xlab = colnames(data)[j], ylab = 'Частота', main = 'Гистограмма частот (лог. шкала)')
}
```

Матрица расстояний в евклидовом пространстве:
```{r}
eucl_dist <- dist(cluster_data, method = 'euclidian') # матрица расстояний
fviz_dist(eucl_dist)
```

## Иерархическая кластеризация

Остановимся на выборе евклидовой метрики. Рассмотрим различные принципы определения расстояния между кластерами.

1) Метод Варда (как правило, дает наиболее удачное разбиение, основан на минимизации суммы внутрикластерных дисперсий):
```{r}
hclust_w <- hcut(cluster_data, k = 4, hc_metric = 'euclidian', hc_method = 'ward.D2')
```

2) Метод ближнего соседа:
```{r}
hclust_nn <- hcut(cluster_data, k = 4, hc_metric = 'euclidian', hc_method = 'single')
```

3) Метод дальнего соседа:
```{r}
hclust_fn <- hcut(cluster_data, k = 4, hc_metric = 'euclidian', hc_method = 'complete')
```

4) Метод средней связи:
```{r}
hclust_av <- hcut(cluster_data, k = 4, hc_metric = 'euclidian', hc_method = 'average')
```

5) Метод центра тяжести:
```{r}
hclust_c <- hcut(cluster_data, k = 4, hc_metric = 'euclidian', hc_method = 'centroid')
```

Изобразим результаты иерархической кластеризации через с помощью дендрограммы:

1) Метод Варда:
```{r}
fviz_dend(hclust_w,
          cex = 0.5, # размер подписей
          color_labels_by_k = TRUE, # выделить объекты цветом по принадлежности к кластерам
          main = 'Дендрограмма (принцип Варда)', ylab = 'Расстояние')
```

2) Метод ближнего соседа
```{r}
fviz_dend(hclust_nn, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип ближнего соседа)', ylab = 'Расстояние')
```

3) Метод средней связи:
```{r}
fviz_dend(hclust_av, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип средней связи)', ylab = 'Расстояние')
```

### Алгоритм k-средних

По результатам иерархической кластеризации можем выдвинуть предположение о возможной необходимости исключения из дальнейшего анализа: г. Москва, г. Севастополь, Чеченская Республика

```{r}
# исключим выделявшиеся регионы при иерархической кластеризации
rownames(complete_data)
```

```{r}
new_cluster_data <- cluster_data[-c(18, 36, 41), ]
kmeans4 <- kmeans(new_cluster_data, centers = 4)
kmeans4
```

Визуализируем результаты кластеризации через график средних. По графику средних даем интерпретацию полученным кластерам.
```{r}
plot(1:6, kmeans4$centers[1,], type = 'l', col = 'red', lwd = 2, ylim = c(-3, 1.5),
     ylab = 'Среднее значение признака', xlab = 'Классифицирующий признак', xaxt = 'n')
lines(1:6, kmeans4$centers[2,], type = 'l', col = 'green', lwd = 2)
lines(1:6, kmeans4$centers[3,], type = 'l', col = 'blue', lwd = 2)
lines(1:6, kmeans4$centers[4,], type = 'l', col = 'magenta', lwd = 2)
title('График средних (признаки стандартизованы)')
axis(1, at = 1:6, labels = colnames(cluster_data), las = 2)
legend(1, -1.3, c('Кластер 1', 'Кластер 2', 'Кластер 3', 'Кластер 4'),
       lwd = c(2, 2, 2, 2), col = c('red', 'green', 'blue', 'magenta'))
```

Можем давать интерпретацию кластеров (например в 1 кластере в регионах в среднем больше доходы чем в других регионах, меньшая нагрузка на здравоохранение и тд)

```{r}
fviz_cluster(object = kmeans4, data = new_cluster_data,
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры регионов в пространстве первых двух главных компонент')
```

```{r}
fviz_cluster(object = kmeans4, data = new_cluster_data, choose.vars = c('income', 'emissions'),
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры регионов в пространстве среднедушевых 
             денежных доходов и выбросов загрязняющих веществ')
```

В кластерном анализе гиперпараметром является число кластеров, то есть мы должны знать число кластеров априори. Иногда выбор числа кластеров может быть основан на экспертном суждении, но можно использовать для этого и чисто статистический подход. Например, можно сравнить разбиения с различным числом кластеров по функционалу качества типа WSS (сумма внутрикластерных дисперсий):

\[
WSS = \sum_{l=1}^k \sum_{X_i \in S_l} d^2 (X_i, \bar{X}_l)
\]

Правило выбора числа кластеров, основанное на WSS, называется иногда методом локтя (elbow method). Принято считать оптимальным число кластеров, после которого убывание WSS начинает замедляться, т.н. точка изгиба, отсюда название.
```{r}
fviz_nbclust(new_cluster_data, kmeans, method = 'wss') +
  labs(x = 'число кластеров', y = 'сумма внутрикластерных дисперсий',
       title = 'Зависимость WSS от числа кластеров')
```

Можно использовать метод силуэтов. Вводятся следующие величины:
\[
s_i = \begin{cases}
\frac{b_i - a_i}{\max(a_i, b_i)} & |S_l| > 1 \\
0 & |S_l| = 1
\end{cases}
\]
где
\[
a_i = \frac{1}{|S_l| - 1} \sum_{X_j \in S_l, i \neq j} d(X_i, X_j)
\]
\[
b_i = \min_{p \neq l} \frac{1}{|S_p|} \sum_{X_j \in S_p} d(X_i, X_j)
\]
Собственно силуэтом называется разница в числителе, а шириной ее соотношение с максимумом. При кластеризации хорошего качества среднее расстояние от объекта до соседей по кластеру не должно превышать минимального среднего расстояния по прочим кластерам. Оптимальным следует считать значение параметра, соответствующее наибольшему среднему значению ширины силуэта.

```{r}
fviz_nbclust(new_cluster_data, kmeans, method = 'silhouette') +
  labs(x = 'число кластеров', y = 'средняя ширина силуэта по всем точкам',
       title = 'Зависимость средней ширины силуэта от числа кластеров')
```

Можно воспользоваться статистикой разрыва (Gap-статистика). Нулевая гипотеза состоит в том, что выборка взята из однородной (объекты не кластеризуемы) генеральной совокупности с некоторым параметризуемым распределением. Идея: генерируются $B$ выборок, проводится кластерный анализ, вычисляется $WSS^*_B$. Статистика разрыва определяется следующим образом:
\[
Gap(k) = \frac{1}{B} \sum_{b = 1}^B \log WSS_b^*(k) - \log WSS(k)
\]
Идея: при истинности $H_0$ разница будет статистически несущественна.

Правило: выбрать оптимальным наименьшее $k$ такое, что $Gap(k) \geq Gap(k+1) - s_{k+1}$, где
\[
s_k = \frac{1}{k} \sqrt{(k+1) \sum_b (\log WSS^*_b(k) - \overline{WSS^*})^2}
\]

Подробнее здесь http://web.stanford.edu/~hastie/Papers/gap.pdf (Tibshirani et al, 2001).

```{r}
fviz_nbclust(new_cluster_data, kmeans, method = 'gap_stat') +
  labs(x = 'число кластеров', y = 'статистика разрыва',
       title = 'Зависимость статистики разрыва от числа кластеров')
```

Самостоятельная работа:

1) Применить метод главных компонент к данным на Листе 2 в файле "russian_regions"

```{r}
#install.packages("FactoMineR") #загрузим нужный для МГК пакет
library(FactoMineR)
```

```{r}
data2 <- import('russian_regions.xlsx', sheet = 'data2') # загрузим новые данные с листа 2 и назовем data2
rownames(data2) <- data2$`Cубъект РФ`
data2 <- data2[, 2:15]

vars2 <- colnames(data2) # сохраним длинные названия переменных
colnames(data2) <- c('income', 'GRP_per_capita', 'popul_per_hosp_bed', 'popul_per_med', 'morbidity', 'emissions', 'wastewater_discharge', 'employment_in_R&D', 'R&D expenses', 'R&D_org', 'tourists_abroad', 'tourists_in_Russia', 'theatres_visitors', 'museum_visitors') # дадим короткие названия переменным
str(data2)
```
Предварительный визуальный анализ:
```{r}
complete_data2 <- data2[complete.cases(data2), ] # данные без пропусков
cluster_data2 <- scale(log(complete_data2))# логарифмирование и стандартизация
boxplot(cluster_data2, col = 'pink')

```

Далее приступаем непостредственно к анализу методом главных компонент:
```{r}
res.pca  <- PCA(cluster_data2, graph = TRUE)
print(res.pca)
```


2) Провести кластеризацию регионов иерархическими методами в пространстве признаков сниженной размерности

3) Провести кластеризацию регионов методом k-средних в пространстве признков сниженной размерности

4) Сделать выводы