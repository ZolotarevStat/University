---
title: "Компьютерная работа №2"
author: "Команда студентов БСТ182 (Джумаева, Золотарев, Кабанова, Морина, Тислюк, Хисяметдинова)"
date: "06 12 2020"
output:
  word_document: 
    fig_width: 10
  html_document:
    df_print: paged
  pdf_document: default
---


```{r}
#используем только если надо установить пакеты в cамом начале
#install.packages('DescTools')
#install.packages('EnvStats')
#install.packages('outliers')
#install.packages("ggpubr")
#install.packages("Hmisc")
#install.packages('corrplot')
#install.packages('ppcor')
#install.packages('rio')
#install.packages('robustHD')
#install.packages('ggplot2')
#install.packages('lmtest')
#install.packages('sjPlot')
#install.packages('normtest')
#install.packages('GGally')
#install.packages('leaps')
#install.packages('R6')
```

```{r}
library(DescTools)
library(EnvStats)
library(outliers)
library(ggplot2)
library(ggpubr)
library(Hmisc)
library(corrplot)
library(ppcor)
library(rio)
library(robustHD)
library(lmtest)
library(normtest)
library(GGally)
library(leaps)
library(tidyr)
library(sjPlot)
library(MASS)
library(FactoMineR)
library(factoextra)
library(devtools)
library(psych)
library(htmlTable)
```


# Многомерные статистические методы

## Компьютерная работа №2. 

## Воспроизведение главных результатов первой части работы

Импортируем данные: имеем 142 наблюдения и 11 переменных.
```{r}
data <- read.csv('GDP Analysis.csv', sep = ';')
data$GDP.per.capita.... <- log(data$GDP.per.capita....)
```

Перед нами стоит задача по имеющейся выборке предсказать показатель ВВП на лушу населения, обозначенный переменной `GDP.per.capita....`, в зависимости от таких объективных факторов:

Обозначение переменной | Переменная
------------- | -------------
Export | Экспорт товаров и услуг (в % от ВВП)
Consumption | Расходы на конечное потребление (в % от ВВП)
Savings | Валовые сбережния домохозяйств (в % от ВВП)
National.Expenditure | Государственные расходы
Import | Импорт товаров и услуг (в % от ВВП)
Labor.among.youth | Коэффициент участия в рабочей силе в возрасте 15-24 лет (%)
Emploument.in.services | Доля занятых в сфере услуг (%)
Employment.in.industry | Доля занятых в промышленном секторе (%)
Mortality.rate.under.5 | Смертность до 5 лет
Population..largest.city. | Население крупнейшего города (в % от городского населения страны)
University.enrollment | Коэффициент вовлеченности в профессиональное образование
Unemployment | Общий уровень безработицы (%)
Urban.population | Городское население (% от общего населения)


Проанализируем те признаки, которые будем использовать далее в качестве объясняющих переменных:
```{r}
data <- data[, c('Export', 'Consumption', 'Savings', 'National.Expanditure', 'Labor.among.youth', 'Employment.in.services', 'Employment.in.industry', 'Mortality.rate.under.5', 'Population..largest.city.', 'University.enrollment', 'Unemployment', 'Urban.population', 'GDP.per.capita....')]
GDP_per_capita <- data$GDP.per.capita....
Desc(GDP_per_capita)
```

```{r}
out_of_1.5IQR <- boxplot.stats(GDP_per_capita)$out 
out_of_1.5IQR_ind <- which(GDP_per_capita %in% out_of_1.5IQR)
out_of_1.5IQR_ind
```
Здесь нам уже не надо проводить одинаковые манипуляции с двумя наборами данных: с выбросами и без выбросов, поэтому будем везде пользоваться данными без выбросов:
```{r}
drop <- c("National.Expanditure", "Savings")
data <- data[ , !(names(data) %in% drop)]
```

```{r}
rdata <- scale(data)
```

```{r}
boxplot(rdata, varwidth = TRUE, col = "bisque", main = "Ящичковая диаграмма для нормированных объясняющих и целевой переменных")
```

Для дальнейшего сравнения с регрессиями, построенными через метод главных компонент или на основе полученных кластеров, оставим три модели, полученных в первой части: линейную со всеми десятью зависимыми объясняющими переменными, экспоненциальную с десятью объясняющими переменными и непрологарифмированными переменными "потребление" и "Занятость в сфере услуг", а также экспоненциальную с девятью переменными, отличающуюся от предыдущей лишь отсутствием переменной "Потребление". Выбор трёх этих мультифакторных регрессий обусловлен либо их наилучшим качеством по сравнению со своими аналогами, либо их интерпретируемостью и логичностью.

```{r}
cor_p <- pcor(data) #частные коэффициенты корреляции
res2 <- cor.mtest(data, conf.level = 0.95)
corrplot(cor_p$estimate, p.mat = res2$p, sig.level = 0.05)
``` 

```{r}
lm <- lm(GDP.per.capita.... ~ .,data)
summary(lm)
```

Экспоненциальная регрессионная модель без логарифмирования потребления и занятости в сфере услуг:
```{r}
nlm_clearData <- data
nlm_clearData$Population..largest.city. <- log(nlm_clearData$Population..largest.city.)
nlm_clearData$Unemployment <- log(nlm_clearData$Unemployment)
nlm_clearData$Export <- log(nlm_clearData$Export)
nlm_clearData$Labor.among.youth <- log(nlm_clearData$Labor.among.youth)
nlm_clearData$Employment.in.industry <- log(nlm_clearData$Employment.in.industry)
nlm_clearData$Mortality.rate.under.5 <- log(nlm_clearData$Mortality.rate.under.5)
nlm_clearData$University.enrollment <- log(nlm_clearData$University.enrollment)
nlm_clearData$Urban.population <- log(nlm_clearData$Urban.population)
nlm1 <- lm(GDP.per.capita.... ~ ., nlm_clearData)
summary(nlm1)
```

```{r}
drop <- c("Consumption")
nlm_Data_exclude_Consumption <- data[ , !(names(data) %in% drop)]
nlm_Data_exclude_Consumption$Population..largest.city. <- log(nlm_Data_exclude_Consumption$Population..largest.city.)
nlm_Data_exclude_Consumption$Unemployment <- log(nlm_Data_exclude_Consumption$Unemployment)
nlm_Data_exclude_Consumption$Export <- log(nlm_Data_exclude_Consumption$Export)
nlm_Data_exclude_Consumption$Labor.among.youth <- log(nlm_Data_exclude_Consumption$Labor.among.youth)
nlm_Data_exclude_Consumption$Employment.in.services <- nlm_Data_exclude_Consumption$Employment.in.services
nlm_Data_exclude_Consumption$Employment.in.industry <- log(nlm_Data_exclude_Consumption$Employment.in.industry)
nlm_Data_exclude_Consumption$Mortality.rate.under.5 <- log(nlm_Data_exclude_Consumption$Mortality.rate.under.5)
nlm_Data_exclude_Consumption$University.enrollment <- log(nlm_Data_exclude_Consumption$University.enrollment)
nlm_Data_exclude_Consumption$Urban.population <- log(nlm_Data_exclude_Consumption$Urban.population)
nlm2 <- lm(GDP.per.capita.... ~ ., nlm_Data_exclude_Consumption)
summary(nlm2)
```

```{r}
AIC(lm)
AIC(nlm1)
AIC(nlm2)
```
Теперь мы вспомнили ключевые результаты компьютерной работы №1 и можем перейти к работе над второй частью:

## Выделение главных компонент

1. Вывод о числе главных компонент, которые необходимо оставить для дальнейшего анализа
с использованием:
(a) критерия Кайзера;
(b) критерия каменистой осыпи.

Воспользуемся кодом с семинара 8, чтобы воспроизвести необходимые вычисления. На вход дадим отнормированные данные, не включающие в себя целевую переменную:
```{r}
rdata_components <- rdata[ , -11]
res.pca  <- PCA(rdata_components, graph=FALSE)
print(res.pca)
```
Находим собственные значения:

```{r}
res.pca$eig
```
Очевидно, что в соответствии критерием Кайзера нам надо выбрать 3 главных компоненты, поскольку у первых трёх собственное значение больше 1. Однако следует обратить внимание на то, что у последней из главных компонент собственное значение лишь на несколько сотых меньше единицы. Поэтому необходимо принять решение о включении этой компоненты на основании дополнительного метода, воспользуемся критерием каменистой осыпи (Кэттеля) и взглянем на график отсортированных по убыванию собственных значений:

```{r}
plot(eigen(cor(rdata_components))$values, xlab = 'Количество компонент', ylab = 'Собственное значение (Eigenvalue Size)', 
     main = 'График каменистой осыпи', type = 'b', xaxt = 'n')
axis(1, at = seq(1, 6, by = 1))
abline(h = 1, col = "red", lty = 5)
legend("topright", legend = c("Eigenvalue = 1"),
       col=c("red"), lty = 5, cex = 0.6)
```

Видим, что по критерию Кэттеля нам следует оставить всего 2 главных компоненты, поскольку дальше разница между смежными собственными значениями крайне невелика. Поскольку в данном случае предполагаемая разница между анализом на двух и трёх главных компонентах будет крайне существенна - воспользуемся третьим методом для принятия окончательного решения, посмотрим на график накопленной объяснённой дисперсии. Из кода на 195 строчке работы мы видим, что накопленная дисперсия  превышает критическое значение 70% при четрёх главных компонентах, потому следует принять следующее решение: ищем среднее арифметическое от оптимального количествах главных компонент по версии разных методов и получаем (2+3+4)/2=3. Именно столько мы и будем использовать для нашего анализа, тогда как остальные компоненты следует принять за "факториальную осыпь".

2. Описание суммарного вклада первых главных компонент;

Ещё раз воспроизведём функцию, которой мы уже пользовались:
```{r}
res.pca$eig
```
Видим, что первые три главных компоненты, которые мы и будем использовать для дальнешего анализа, описывают 69% дисперсии между объектами, что намекает нам на то, что качество построенных на их основе регрессий будет на порядок хуже, но при этом подарит нам очень важное свойство ортогональности признакового пространства, благодаря которому мы можем быть уверенными в отсутствии взаимозависимостей между изначально выбранными для анализа переменными.

3. Построение графика накопленного вклада главных компонент в суммарную дисперсию ис-
ходного признакового пространства;

Для начала покажем относительные значения объяснённых дисперсий:
```{r}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 60))
```

```{r}
plot(res.pca$eig[,3], ylab = 'Накопленная объяснённая дисперсия', type = 'b', xaxt = 'n')
```

Как мы уже убедились ранее из анализа таблицы для собственных значений и доли объяснённой дисперсии, при увеличении количества рассматриваемых компонент мы можем объяснить всё большие и большие различия между данными, однако скорость изменения увеличения количества с каждой компонентой становится всё меньше и меньше (затухает), поэтому пытаемся выбрать какое-то оптимальное количество компонент, чтобы обрести некоторый баланс между низкой размерностью и высоким качеством моделирования разницы между странами мира.

4. Интерпретация главных компонент на основе анализа матрицы факторных нагрузок.
```{r}
pca <- principal(cor(rdata_components), nfactors = 4, rotate = "none", covar = FALSE)
pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 4, nrow = 10)
htmlTable(round(pca_loadings,2), header = c("PC1", "PC2", "PC3", "PC4"),
        rnames = c('Export', 'Consumption', 'Labor.among.youth', 'Employment.in.services', 'Employment.in.industry', 'Mortality.rate.under.5', 'Population..largest.city.', 'University.enrollment', 'Unemployment', 'Urban.population'))
```

Вклады переменных в процентах в каждую компоненту:

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1, top =  10)
fviz_contrib(res.pca, choice = "var", axes = 2, top =  10)
fviz_contrib(res.pca, choice = "var", axes = 3, top =  10)
fviz_contrib(res.pca, choice = "var", axes = 4, top =  10)

```

Визуализируем результаты выделения главных компонент (график корреляций между исходными переменными и новыми компонентами). Важно обратить внимание на то, что переменные, имеющие больший вклад, хорошо выделяются на данном графике:

```{r}
fviz_pca_var(res.pca, col.var = "contrib", gradient.cols = c("blue", "yellow", "red", "green"),
legend.title = "Cont.Var" 
, alpha.var = "contrib" # добавляет прозрачность по вкладу переменной в компоненту
)
```

Корреляционные связи между переменными и главными компонентами (отсортированы по значимости коэффициентов корреляции):

```{r}
res.desc <- dimdesc(res.pca, axes = c(1,2,3,4), proba = 0.05)
res.desc$Dim.1
res.desc$Dim.2
res.desc$Dim.3
res.desc$Dim.4

```

Мы видим, что первая главная компонента главным образом зависит от занятости в сфере услуг (положительно), смертности детей младше 5 лет (отрицательно), доли городского населения и доли поступающих в высшие учебные заведения среди соответствующей возрастной группы. Показательно, что именно эти параметры лучше всего обуславливают разницу между странами, очевидно, эта компонента войдёт в уравнение регрессии с положительным коэффициентом.

Вторая компонента меньше скореллирована с исходными признаками, но главным образом обуславливается отрицательной зависимостью с безработицей и положительной - с долей населения в крупнейшем городе и уровнем экспорта (вероятно, будет иметь положительный коэффициент в регрессии).

Третья компонента главным образом зависит от доли населения, сосредоточенной в наибольшем по размеру городе, лишь незначительно завися от остальных переменных (по остальным зависимостях можно предположить, что она также войдёт в итоговое уравнение с положительным коэффициентом). Важно отметить, что все три главных компоненты не описывают значительной зависимости с потреблением, хотя и все каким-то образом зависят от него. Будет иметь отрицательный коэффициент

Последняя рассматриваемая нами компонента положительно зависит от потребления и негативно - от занятости в сфере промышленности.

Для следующей главы также найдём главные компоненты, построенные на логарифмированных данных:

```{r}
drop <- c("GDP.per.capita....")
nlm_component_Data <- nlm_Data_exclude_Consumption[ , !(names(nlm_Data_exclude_Consumption) %in% drop)]
nlm_component_rData <- scale(nlm_component_Data)
res.pca_nlm  <- PCA(nlm_component_rData, graph=FALSE)
print(res.pca_nlm)
```
Находим собственные значения:

```{r}
res.pca_nlm$eig
```
Возьмём три компоненты, исходя из тех же принципов, что были описаны при выборе количества главных компонент для линейных данных.

## Построение уравнения регрессии с использованием выделенных главных компонент

1. Построение линейного уравнения регрессии на ГК;

Стандартизуем значения логарифма ВВП на душу населения:
```{r}
rGDP <- scale(GDP_per_capita)
```

```{r}
components <- as.data.frame(res.pca$ind$coord[,1:4])
lm_pca <- lm(GDP_per_capita ~ components$Dim.1 + components$Dim.2 +components$Dim.3 + components$Dim.4)
summary(lm_pca)
```
Видим, что третья главная компонента на линейных переменных всё же негативно зависит от изначального набора, качество модели по adjusted $R^2$ незначительно ухудшилось, но стандартная ошибка остатков значительно уменьшилась, что является положительным звоночком!
```{r}
components <- as.data.frame(res.pca_nlm$ind$coord[,1:3])
nlm_pca <- lm(rGDP ~ components$Dim.1 + components$Dim.2 + components$Dim.3)
summary(nlm_pca)
```
Нелинейная модель на главных компонентах оказалась качественно хуже своего "собрата" по всем рассматриваемым метрикам качества (Residual St.Error & Adjusted $R^2$).

2. Сопоставление свойств ранее полученных уравнений регрессии (линейное и нелинейное урав-
нения регрессии) с уравнением регрессии на ГК;
3. Выбор и обоснование лучшего уравнения.

```{r}
AIC(lm)
AIC(nlm1)
AIC(nlm2)
AIC(lm_pca)
AIC(nlm_pca)
```

```{r}
BIC(lm)
BIC(nlm1)
BIC(nlm2)
BIC(lm_pca)
BIC(nlm_pca)
```
Мы видим, что в соответствии с информационным критерием Акайке наилучшей моделью будет линейная модель на главных компонентах, полученная по результатам последних вычислений, по результатам байесовского критерия Шварца та же самая модель будет наиболее эффективной, но с ещё большим отрывом от моделей из первой части работы (ровно как и по сумме квадратов отклонений). По критериям скорректированного $R^2$ нелинейная модель с сохранением переменной потребления остаётся лучшей, имея наилучшую предсказательную силу.

Таким образом, сравнив качество модели через продвинутые информационные критерии, модель, полученная по результатам метода главных компонент, будет на порядок лучше, поскольку она выделяется очень важным свойством отсутствия коллинеарности и имеет высокую предсказательную силу, используя крайне небольшое количество объясняющих переменных (формально, на самом деле внутри трёх главных компонент хранится аж 10 переменных, без которых её качество не было бы столь хорошим). При этом модель обладает достаточно высоким уровнем логарифмической функции правдоподобия.

На столь положительной ноте (наши труды не оказались бесполезными и дали улучшение с точки зрения возможного прогнозирования значений) мы можем перейти к следующей главе нашей работы, кластерному анализу.

## Кластерный анализ

1. Построение и анализ дендрограмм;
Рассмотреть 5-6 вариантов разбиения объектов на кластеры: метод ближнего соседа, метод
дальнего соседа, центра тяжести, средней связи, метод Уорда и др. Сделать вывод о наиболее предпочтительном числе кластеров (часто исследователь может выдвинуть рабочую гипотезу об альтернативном количестве кластеров, например, 3 или 4).

Будем использовать евклидову метрику для подсчёта расстояний между наблюдениями (нет никаких оснований полагать, что расстояния Махланобиса или Манхэттена дадут более качественную кластеризацию). Представим матрицу расстояний:
```{r}
eucl_dist <- dist(rdata_components, method = 'euclidian')
fviz_dist(eucl_dist)
```

Сначала попытаемся определить значение единственного гиперпараметра в кластерном анализе - количество кластеров. Применим изученные методы: метод локтя, силуэтов и статистику разрыва:

Сравним разбиения с различным числом кластеров по функционалу качества типа WSS (сумма внутрикластерных дисперсий):

\[
WSS = \sum_{l=1}^k \sum_{X_i \in S_l} d^2 (X_i, \bar{X}_l)
\]

Правило выбора числа кластеров, основанное на WSS, называется иногда методом локтя (elbow method). Принято считать оптимальным число кластеров, после которого убывание WSS начинает замедляться, т.н. точка изгиба, отсюда название.
```{r}
fviz_nbclust(rdata_components, kmeans, method = 'wss') +
  labs(x = 'число кластеров', y = 'сумма внутрикластерных дисперсий',
       title = 'Зависимость WSS от числа кластеров')
```
График показывает, что оптимальным будет примерно 4-5 кластеров, поскольку именно там скорость изменения суммы внутрикластерных дисперсий значительно уменьшается.

Для лучшего понимания оптимального значения кластеров попробуем также использовать метод силуэтов. Вводятся следующие величины:
\[
s_i = \begin{cases}
\frac{b_i - a_i}{\max(a_i, b_i)} & |S_l| > 1 \\
0 & |S_l| = 1
\end{cases}
\]
где
\[
a_i = \frac{1}{|S_l| - 1} \sum_{X_j \in S_l, i \neq j} d(X_i, X_j)
\]
\[
b_i = \min_{p \neq l} \frac{1}{|S_p|} \sum_{X_j \in S_p} d(X_i, X_j)
\]
Собственно силуэтом называется разница в числителе, а шириной ее соотношение с максимумом. При кластеризации хорошего качества среднее расстояние от объекта до соседей по кластеру не должно превышать минимального среднего расстояния по прочим кластерам. Оптимальным следует считать значение параметра, соответствующее наибольшему среднему значению ширины силуэта.

```{r}
fviz_nbclust(rdata_components, kmeans, method = 'silhouette') +
  labs(x = 'число кластеров', y = 'средняя ширина силуэта по всем точкам',
       title = 'Зависимость средней ширины силуэта от числа кластеров')
```
Этот метод подсказывает, что лучше использовать лишь 2 кластера.

Также попробуем воспользоваться Gap-статистикой. Нулевая гипотеза состоит в том, что выборка взята из однородной (объекты не кластеризуемы) генеральной совокупности с некоторым параметризуемым (то есть по типу тех, которые изучали на теории вероятностей, а не тех, которые изучаются на курсах ФКН "Байесовские методы") распределением. Идея: генерируются $B$ выборок, проводится кластерный анализ, вычисляется $WSS^*_B$. Статистика разрыва определяется следующим образом:
\[
Gap(k) = \frac{1}{B} \sum_{b = 1}^B \log WSS_b^*(k) - \log WSS(k)
\]
Идея: при истинности $H_0$ разница будет статистически несущественна.

Правило: выбрать оптимальным наименьшее $k$ такое, что $Gap(k) \geq Gap(k+1) - s_{k+1}$, где
\[
s_k = \frac{1}{k} \sqrt{(k+1) \sum_b (\log WSS^*_b(k) - \overline{WSS^*})^2}
\]


```{r}
fviz_nbclust(rdata_components, kmeans, method = 'gap_stat') +
  labs(x = 'число кластеров', y = 'статистика разрыва',
       title = 'Зависимость статистики разрыва от числа кластеров')
```

В соответствии с методом локтя и статистики разрыва, 4 кластера будут являться оптимальным количеством. 

Дендрограмма иллюстрирует поэтапный процесс кластеризации, отображая по горизонтальной оси объекты, в нашем случае, страны, а по вертикальной – расстояния.
Проведём необходимые вычисления для построения соответствующих дендрограмм и построим их, произведя кластеризацию по методу Варда, ближнего соседа, дальнего соседа, методу средней связи и центра тяжести:

1) Метод Варда (как правило, дает наиболее удачное разбиение, основан на минимизации суммы внутрикластерных дисперсий):
```{r}
hclust_w <- hcut(rdata_components, k = 4, hc_metric = 'euclidian', hc_method = 'ward.D2')
fviz_dend(hclust_w,
          cex = 0.5, # размер подписей
          color_labels_by_k = TRUE, # выделить объекты цветом по принадлежности к кластерам
          main = 'Дендрограмма (принцип Варда)', ylab = 'Расстояние')
```

Как видно из дендрограммы, наиболее однородным является третий кластер, так как объединения стран в данном кластере происходило на наименьших расстояниях. Третий кластер содержит только небольшие государства, каждое из которых за 2018 год имеет положительные темпы роста ВВП. В первый кластер входят в основном африканские страны, исключениями являются Афганистан и Непал. Второй кластер объединяет страны, расположенные в Восточной Европе, и в Юго – Восточной Азии. В последний класс вошли страны, в которых невысокий уровень безработицы и, как правило, высокие расходы на потребление. 
 
2) Метод ближнего соседа:
```{r}
hclust_nn <- hcut(rdata_components, k = 5, hc_metric = 'euclidian', hc_method = 'single')
fviz_dend(hclust_nn, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип ближнего соседа)', ylab = 'Расстояние')
```

Достаточно интересные результаты получаются при использовании метода ближнего соседа. Самым большим кластером является четвертый, в то время как первые три в сумме включают всего 5 стран. В качестве первого класса выступает Ливан, где по сравнению со странами высокая доля городского населения при относительно небольших значениях ВВП на душу населения. Второй класс образуют Мальта и Гонконг, третий – государства Конго и Джибути.

3) Метод дальнего соседа:
```{r}
hclust_fn <- hcut(rdata_components, k = 4, hc_metric = 'euclidian', hc_method = 'complete')
fviz_dend(hclust_fn, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип дальнего соседа)', ylab = 'Расстояние')
```

При использовании принципа дальнего соседа степень сходства объектов оценивается степенью сходства между наиболее отдаленными объектами кластера. Как видно, второй кластер расширяется, в то время как третий остается наиболее однородным по выбранным признакам.

4) Метод средней связи:
```{r}
hclust_av <- hcut(rdata_components, k = 4, hc_metric = 'euclidian', hc_method = 'average')
fviz_dend(hclust_av, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип средней связи)', ylab = 'Расстояние')
```

Метод средней связи выделяет кластеры довольно схожие по составу и размеру с кластерами, полученными при использовании принципа ближнего соседа. Первый класс полностью совпадает, второй кластер является объединением второго и третьего классов, представленных в пункте (2). Все 4 страны из третьего кластера характеризуются высокой безработицей и низкими положительными темпами роста ВВП. Стоит заметить, что судя по дендрограмме, наиболее схожие по рассматриваемым признакам страны находятся в последнем кластере. 

5) Метод центра тяжести:
```{r}
hclust_c <- hcut(rdata_components, k = 4, hc_metric = 'euclidian', hc_method = 'centroid')
fviz_dend(hclust_c, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип центра тяжести)', ylab = 'Расстояние')
```

Здесь кластеры содержат примерно одинаковое количество объектов. Первый кластер объединяет как европейские и азиатские страны, так и африканские, тоже самое можно сказать и об остальных кластерах. Страны во втором классе отличаются невысокими среднедушевыми доходами, исключением является разве что Соединенное Королевство. Основным отличием стран в третьем и четвертом кластерах является низкая занятость в сфере услуг.  

Теперь попробуем кластеризовать данные через метод к-средних.

## Использование метода к-средних для классификации объектов

1. Построение и анализ графика средних значений показателей в кластерах;
2. Проверка гипотезы о равенстве средних значений в кластерах (в случае необходимости);
3. Интерпретация полученных результатов (необходимо дать названия кластерам и обосновать
их выбор);
4. Описание кластеров с помощью графических средств, помогающих обосновать название кла-
стеров;
5. Выводы.


```{r}
kmeans4 <- kmeans(rdata_components, centers = 4, nstart=30) 
kmeans4
```
В результате получаем 4 кластера разной размерности, наименьший включает в себя 11 стран, в то время как крупнейший 61 страну. 

Визуализируем результаты кластеризации через график средних. По графику средних даем интерпретацию полученным кластерам.
```{r}
plot(1:10, kmeans4$centers[1,], type = 'l', col = 'red', lwd = 2, ylim = c(-5, 5),
     ylab = 'Среднее значение признака', xaxt = 'n')
lines(1:10, kmeans4$centers[2,], type = 'l', col = 'green', lwd = 2)
lines(1:10, kmeans4$centers[3,], type = 'l', col = 'blue', lwd = 2)
lines(1:10, kmeans4$centers[4,], type = 'l', col = 'magenta', lwd = 2)
title('График средних (признаки стандартизованы)')
axis(1, at=1:10, labels = colnames(rdata_components), las = 2)
legend(1, -1.8, c('Кластер 1', 'Кластер 2', 'Кластер 3', 'Кластер 4'),
       lwd = c(2, 2, 2, 2), col = c('red', 'green', 'blue', 'magenta'))
```
Вспомним техническую часть работы с семинаров. Присоединим вектор значений принадлежности к кластеру к основным данным:
```{r}
cl <- kmeans4$cluster
reg_cluster_data <- cbind(rdata, cl)
```

Получим значения объясняющих переменных с разбивкой по кластерам:
```{r}
data_cluster1 <- reg_cluster_data[reg_cluster_data[,12] == 1, ]
data_cluster2 <- reg_cluster_data[reg_cluster_data[,12] == 2, ]
data_cluster3 <- reg_cluster_data[reg_cluster_data[,12] == 3, ]
data_cluster4 <- reg_cluster_data[reg_cluster_data[,12] == 4, ]
```

```{r}
colnames(data_cluster1)
```

```{r}
for (n in 1:10) {
  print(paste("Тест о равенстве дисперсий признака", n))
  pval <- var.test(data_cluster1[,n], data_cluster2[,n])$p.value
  print(paste("p-value для равенства дисперсий признака в кластерах 1 и 2", round(pval,3)))
  pval <- var.test(data_cluster1[,n], data_cluster3[,n])$p.value
  print(paste("p-value для равенства дисперсий признака в кластерах 1 и 3", round(pval,3)))
  pval <- var.test(data_cluster1[,n], data_cluster4[,n])$p.value
  print(paste("p-value для равенства дисперсий признака в кластерах 1 и 4", round(pval,3)))
  pval <- var.test(data_cluster2[,n], data_cluster3[,n])$p.value
  print(paste("p-value для равенства дисперсий признака в кластерах 2 и 3", round(pval,3)))
  pval <- var.test(data_cluster2[,n], data_cluster4[,n])$p.value
  print(paste("p-value для равенства дисперсий признака в кластерах 2 и 4", round(pval,3)))
  pval <- var.test(data_cluster3[,n], data_cluster4[,n])$p.value
  print(paste("p-value для равенства дисперсий признака в кластерах 3 и 4", round(pval,3)))

}
```

```{r}
for (n in 1:10) {
  print(paste("Тест о равенстве средних признака", n))
  pval <- t.test(data_cluster1[,n], data_cluster2[,n])$p.value
  print(paste("p-value для равенства средних признака в кластерах 1 и 2", round(pval,3)))
  pval <- t.test(data_cluster1[,n], data_cluster3[,n])$p.value
  print(paste("p-value для равенства средних признака в кластерах 1 и 3", round(pval,3)))
  pval <- t.test(data_cluster1[,n], data_cluster4[,n])$p.value
  print(paste("p-value для равенства средних признака в кластерах 1 и 4", round(pval,3)))
  pval <- t.test(data_cluster2[,n], data_cluster3[,n])$p.value
  print(paste("p-value для равенства средних признака в кластерах 2 и 3", round(pval,3)))
  pval <- t.test(data_cluster2[,n], data_cluster4[,n])$p.value
  print(paste("p-value для равенства средних признака в кластерах 2 и 4", round(pval,3)))
  pval <- t.test(data_cluster3[,n], data_cluster4[,n])$p.value
  print(paste("p-value для равенства средних признака в кластерах 3 и 4", round(pval,3)))

}
```

Таким образом, самый малочисленный кластер  из 11 стран характеризуется наибольшей долей экспорта в отличие от других групп, что неудивительно, учитывая размер регионов, входящих в кластер. Также значительно возвышается над другими кластерами значение численности населения крупнейшего города, так, например, население Гонконга превышает 7,5 миллионов (из которых ввиду особенностей площади страны в крупнейшем городе живёт более 99% жителей) и высока детская смертность. Минимален в данном кластере показатель потребления. Кроме того, примечательно распределение рабочей силы, так, в странах второго кластера самая высокая доля занятых в сфере услуг, в то время как доля занятых в индустриальной сфере находится на втором месте с конца, это может быть обосновано максимальной долей городского населения, так как в городе преимущественно преобладает занятость в сфере услуг.

Первый кластер, включающий в себя 28 стран из рассматриваемой выборки, среди которых Турция, Испания, Словения, Аргентина, Сербия, Греция и др., характеризуется сравнительно меньшей долей экспорта, но почти максимальным объемом потребления, что может свидетельствовать о сравнительно приемлемом уровне жизни в данных странах. Минимально в странах первого кластера значение занятости среди молодежи, в то же время высок показатель численности студентов высших учебных заведений.Также на низком уровне находится детская смертность, что может говорить о достаточно хорошей степени развитости медицины.  Кластер 1 характеризуется средним значением безработицы, сильно превыщающим средние значения этого показателя других кластеров. Возможно это связано с трудной экономической ситуацией внутри стран, например, Греция в 2010 году потерпевшая долговой кризис до сих пор имеет высокий уровень безработицы, чье значение превышает 19%. Среднее значение потребления ближе к 1 и практически достигает верхнего значения среди имеющихся кластеров. Среднее значение экспорта находится в районе нуля и соответствует остальным значениям. Занятость среди молодежи имеет самое низкое среднее значение среди всех четырех кластеров. Среднее значение занятости в сфере услуг находится на уровне нуля и пересекается со значением кластера 3. Среднее значение занятости в производстве практически совпадает с значением для кластера 3 и является одним из наибольших. Среднее значение смертности детей до 5 лет имеет среднее для других кластеров значение около 0. Значение проживающих в наикрупнейшем городе совпадает с кластерами 3 и 4. Вовлеченность в получение высшего образования имеет среднее значение, практически совпадающее со значениями для кластеров 2 и 3, а среднее значение городского населения совпадает со значением для кластера 3.

В третий и крупнейший по численности кластер входят Великобритания, Новая Зеландия, Норвегия, Япония, Индия, Россия и др., не считая аномально выбивающегося значения экспорта второго кластера, третий характеризуется максимальным показателем экспорта и низким уровнем потребления. Примечателен достаточно высокий уровень занятости среди молодежи при максимальной численности студентов высших учебных заведений. Также в отличие от других кластеров высока как доля занятых в сфере услуг, так и в индустриальной сфере и минимальная безработица.

Четвертый кластер, включающий в себя 38 государств (среди них Бангладеш, Мадагаскар, Бурунди, Афганистан и др.), отличается минимальной долей экспорта и занятости. Велико значение занятости среди молодежи и минимально число студентов вузов. При этом страны четвертого кластера характеризуются слабо развитой системой здравоохранения, поскольку экстремально велик уровень детской смертности до 5 лет.

Можем заключить, что здесь нет смысла проверять гипотезу о равенстве средних ввиду наличия значимых отличий в полученном разбиении.

```{r}
fviz_cluster(object = kmeans4, data = rdata_components,
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры регионов в пространстве первых двух главных компонент')
```

Из графика видно разбиение регионов по кластерам в пространстве двух первых главных компонент. Учитывая наше знание о знаке перед коэффициентами этих двух компонент, мы можем утверждать, что линии уровня благосостояния государств в данном пространстве будут идти вправо вверх (хотя угол наклона идёт ближе к виртуальной оси ОХ и соответствует примерно 20 градусам), поскольку обе компоненты значимы и положительно влияют на ВВП на душу населения. 

Поэтому можем заключить, что первый кластер соответствует странам, наименее преуспевающим в своём экономическом развитии; 
второй - одним из наиболее успешных стран мира среди тех, ВВП на душу населения которых не было принято считать выбросами; 
третий - среднестатистический кластер, внутри которого содержатся развивающиеся страны мира;
четвёртый - наиболее успешные страны мира, показатель ВВП на душу населения которых существенно отличается от остальных кластеров.
Очевидно, здесь на каком-то этапе произошла ошибка, поскольку сложно поверить, что в странах с высоким значением младенческой смертности находятся наиболее успешные страны мира, очевидно, здесь прозошла какая-то путаница между кластерами, которую мы, глядя на исходные данные и написанный код, так и не смогли обнаружить, к сожалению.

## Построение регрессионных моделей в кластерах (типологическая регрессия)

1. Построение уравнений регрессии в кластерах;
2. Сопоставление и интерпретация коэффициентов регрессии в кластерах с использованием
коэффициентов эластичности;
3. Сопоставление качества построенных моделей в кластерах и для всей совокупности объектов
в целом.
4. Выводы.



Теперь попробуем определить оптимальные регрессионные уравнения для каждого из кластеров, не применяя при этом такой козырь как регрессию на главных компонентах, поскольку мы желаем получить наиболее легко интерпретируемые данные и не хотим загромождать ассистента проверкой двух разных по сути, но похожих по реализации механизмов. Однако следует отметить, что, вероятно, применение регрессий для кластеров на главных компонентах могло бы помочь избавиться от некоторых проблем, которые мы встретим в дальнейшем по ходу данной главы. 

## Регрессия для первого кластера
```{r}
lm_cl1 <- lm(GDP.per.capita.... ~ .,as.data.frame(data_cluster1[, -c(12)]))
summary(lm_cl1)
```
Скорректированное значение $R^2$ достаточно велико и составляет порядка 65%, что говорит о приемлемом качестве модели. Для первого кластера регрессионная модель имеет 5 отрицательных компонент, среди них потребление, занятость среди молодежи, доля занятых в промышленности, население крупнейшего города, а также с наибольшим по модулю коэффициентом - смертность детей до 5 лет. Положительно на целевую переменную в странах первого кластера в большей степени влияет доля занятых в сфере услуг. Показательно, что разницу между наблюдениями внутри кластера можно главным образом объяснить выделяющимися значениями младенческой смертности и занятости в сфере промышленности, хотя по итогам кластерного анализа среднее этих переменных никак не отличалось от среднего по всей выборке. Это важный результат, благодаря которому мы можем сделать вывод о наличии возможности эффективно определять значение целевой функции, пристально концентрируясь на выделяющихся в однородную группу относительно остальной выборки наблюдениях. Примем на вооружение!

Найдем значимые переменные:
```{r}
coefci(lm_cl1)
```
Таким образом, можно сделать вывод о том, что 7 признаков не являются значимыми, поскольку в их доверительный интервал входит 0. Попробуем построить регрессию только на значимых переменных:

```{r}
new_lm_cl1 <- lm(GDP.per.capita.... ~ Consumption + Employment.in.services + Mortality.rate.under.5, data = as.data.frame(data_cluster1[, -c(12)]))
summary(new_lm_cl1)
```

```{r}
AIC(lm_cl1)
AIC(new_lm_cl1)
```

Построив новую модель для первого кластера удалось увеличить скорректированный $R^2$, то есть, увеличить долю объясненной дисперсии ВВП на душу населения. Таким образом, внутри кластера стран третьего мира (в соответствии с интерпретацией кластеров в пространстве двух первых компонент) мы можем определить благосостояние проживающих там жителей на основании переменных потребления (чем меньше потребляют и, соответственно, больше сберегают, тем лучше качество жизни там), занятости в сфере услуг (страны движутся в верном направлении, развивая сферу услуг)  и смертности детей младше 5 лет (развитие медицины позволяет значимо отличиться и двигаться в сторону выхода из данного кластера).

```{r}
coefci(new_lm_cl1)
```
Ожидаемо, все переменные значимы. 

Визуализируем модель первого кластера:
```{r}
pred <- predict(new_lm_cl1) 
plot(seq(1, nrow(data_cluster1[,-c(1, 3, 5, 7, 8, 9, 10, 12)]), 1), data_cluster1[, 11], pch = 16, xlab = "Номер наблюдения", ylab = "Логарифм ВВП на душу населения") 
lines(seq(1, nrow(data_cluster1[,-c(1, 3, 5, 7, 8, 9, 10, 12)]), 1), pred, col = "pink", lwd = 2)
```

Видим, что в целом модель в верном направлении прогнозирует реальные значения ВВП на душу населения, а ошибка модели заключается лишь в невозможности точь-в-точь попасть в некоторые выбросы.


Видим, что в целом, данная модель имеет приемлемую предсказательную мощность.

```{r}
AIC(lm_cl1)
AIC(new_lm_cl1)
BIC(lm_cl1)
BIC(new_lm_cl1)
```
Вторая модель почти в 2 раза лучше в соответствии с известными информационными критериями. Ее уравнение:

$y=-0.35-0.24x_{1}+0.23x_{2}-0.44x_{3}$

$x_{1}-Consumption, x_{2}-employment.in.services, x_{3}-Mortality.rate.under.5$


Коэффициент эластичности показывает насколько в среднем изменится целевой показатель при изменении объясняющей переменной на 1%. Собственно, здесь всё полностью соотносится с интерпретацией коэффиицентов полученной модели, странам третьего мира критически важно увеличить занятость в сфере услуг, затем усовершенствовать состояние медицины и сделать всё возможное для того, чтобы их граждане имели возможность сберегать. Но очевидно, что если просто создавать рабочие места по профессиям типа продавцов, парикмахеров и т.д. делая большие ставки по сберегательным депозитам в странах по типу Зимбабве - вряд ли это хоть как-то положительно повлияет на уровень жизни в стране, поскольку тут скорее обратная завимиость: если в стране высокий уровень жизни, то у её граждан есть возможность сходить в магазин, постричься вне дома и оставить какие-то сбережения для будущих лет.

## Регрессия для второго кластера

```{r}
lm_cl2 <- lm(GDP.per.capita.... ~ .,as.data.frame(data_cluster2[, -c(12)]))
summary(lm_cl2)
```
Как видим, базовая функция выдает не совсем корректные результаты, это связано с тем, что в данном случае количество предикторов превышает количество наблюдений, поэтому данный кластер требует другой метод. 
Второй кластер - наименьший по численности из 4 (включает в себя всего 5 стран), попробуем воспользоваться алгоритмом пошаговой регрессии. Целью этого алгоритма является добавление и удаление потенциальных кандидатов в моделях и сохранение тех, кто оказывает существенное влияние на зависимую переменную. Этот алгоритм имеет смысл, когда набор данных содержит больше предикторов, чем наблюдений, но данный метод не сработал, поскольку у нашей изначальной модели, содержащей все переменные AIC отрицательно бесконечна.

Построим регрессию вручную, основываясь на результатах кластерного анализа. Включим в модель только ту переменную, которые в наименьшей степени характеризуют второй кластер (пользуясь выводом, полученым по результатам регрессии для первого кластера), это доля поступающих в университеты. Берём только одну объясняющую переменную, поскольку большее количество нарушало бы все предпосылки для построения линейной регрессии.

```{r}
new_lm_cl2 <- lm(GDP.per.capita.... ~ University.enrollment, data = as.data.frame(data_cluster2[, -c(12)])) 
summary(new_lm_cl2)
```

Таким образом имеем высокое значение скорректированного коэффициента детерминации, что говорит о высоком качестве модели. 
$y=0.42849+0.97779x, x-University.enrollment$

Визуализируем модель второго кластера:
```{r}
pred2 <- predict(new_lm_cl2) 
plot(seq(1, nrow(data_cluster2[,-c(2, 5, 6, 7, 8, 9, 10, 11, 12)]), 1), data_cluster2[, 11], pch = 16, xlab = "Номер наблюдения", ylab = "Логарифм ВВП на душу населения") 
lines(seq(1, nrow(data_cluster2[,-c(2, 5, 6, 7, 8, 9, 10, 11, 12)]), 1), pred2, col = "pink", lwd = 2)
```

Таким образом, мы можем сделать вывод, что увеличение доли поступающих в университете в кластере одних из наиболее развитых стран мира существенно влияет на уровень ВВП на душу населения, что говорит о необходимости углубленно заниматься развитием института образования в развитых странах.

## Регрессия для третьего кластера
```{r}
lm_cl3 <- lm(GDP.per.capita.... ~ .,as.data.frame(data_cluster3[, -c(12)]))
summary(lm_cl3)
```
Скорректированное значение $R^2$ велико и составляет порядка 86%, что говорит о хорошем качестве модели, что ожидаемо, поскольку третий кластер самый многочисленный и предпосылки линейной регрессии не нарушены. Для третьего кластера регрессионная модель имеет 5 отрицательных компонент, среди них потребление, население крупнейшего города, безработица, доля городского населения (что отличается от всех моделей построеных ранее), а также с наибольшим по модулю коэффициентом - смертность детей до 5 лет. Положительно на целевую переменную в странах этого кластера в большей степени влияет доля занятых в сфере услуг.

Найдем значимые переменные:
```{r}
coefci(lm_cl3)
```

Снова доверительные интервалы 7 переменных включают в себя 0, что говорит о незначимости данных коэффициентов регрессии. Попробуем аналогично кластеру 1 построить регрессионную модель на значимых переменных. 


Модель для третьего кластера, включающая только 3 переменные дает значительно худшие результаты по сравнению с полной моделью. Так доля объясненной дисперсии после наших преобразований уменьшилась почти на 20%. Однозначно модель $lm_{cl3}$ лучше. 

```{r}
pred3 <- predict(lm_cl3) 
plot(seq(1, nrow(data_cluster3), 1), data_cluster3[, 11], pch = 16, xlab = "Номер наблюдения", ylab = "Логарифм ВВП на душу населения") 
lines(seq(1, nrow(data_cluster3), 1), pred3, col = "pink", lwd = 2)
```


В целом большая часть точек совпадает, что говорит о приемлемом качестве предсказательной способности модели.

## Регрессия для четвертого кластера
```{r}
lm_cl4 <- lm(GDP.per.capita.... ~ .,as.data.frame(data_cluster4[, -c(1,3,5, 6, 7,8,9,10,12)]))
summary(lm_cl4)
```
Скорректированный $R^2$ сравнительно мал, он составляет чуть более 54%.

```{r}
coefci(lm_cl4)
```


На этом мы можем завершить анализ данной главы и перейти к заключительному разделу всей компьютерной работы - дискриминантному анализу.

## Дискриминантный анализ

1. Построение дискриминантных функций. Выводы о качестве модели;

2. Отнесение новых объектов (3-4 наблюдения) к выделенным и описанным кластерам различ-
ными способами (с использованием ДФ, расстояния Махаланобиса, апостериорных вероят-
ностей);

3. Уточнение результатов классификации, выполненной с помощью метода к-средних, с по-
мощью аппарата дискриминантного анализа (выявление некорректно классифицированных наблюдений);

4. Анализ классификационной матрицы (classification matrix). Вывод о качестве разбиения объектов на кластеры;

5. Построение графиков и анализ визуализации результатов.

Присоединение вектора значений принадлежности к кластеру к основным данным:
```{r}
cl <- kmeans4$cluster
cluster_data <- cbind(rdata_components, cl)
```

Линейный дискриминантный анализ:
Разделение выборки на обучающую (2/3) и тестовую (1/3):
```{r}
data.train <- as.data.frame(cluster_data[seq(1,nrow(cluster_data),1.5),])
data.unknown <- as.data.frame(cluster_data[-seq(1,nrow(cluster_data),1.5),])
```

Построение дискриминантной функции:
```{r}
lda.fit <- lda(data.train[,-c(11)], grouping = data.train$cl)
lda.fit
```

the first discriminant function is a linear combination of the variables: $-0.1374Export-0.0837Consumption+...-0.6143Urban.population$

Мы можем преобразовать обучающие данные в канонические координаты, применив матрицу преобразования к масштабированным данным. Чтобы получить те же результаты, что и функция predict. lda, сначала нам нужно центрировать данные вокруг взвешенных средних:

```{r}
# центрируем данные вокруг взвешенных средних и преобразуем их
means <- colSums(lda.fit$prior * lda.fit$means)
train.mod <- scale(data.train[,-c(11)], center = means, scale = FALSE) %*% 
             lda.fit$scaling
# Используем функцию predict для преобразования:
lda.pred.train <- predict(lda.fit, data.train[,-c(11)])
all.equal(lda.pred.train$x, train.mod)
```

Мы можем использовать первые две дискриминантные переменные для визуализации данных:
```{r}
plot.df <- data.frame(train.mod, "Cluster" = data.train$cl)
library(ggplot2)
ggplot(plot.df, aes(x = LD1, y = LD2, color = Cluster)) + geom_point()
```

Очевидны только разбиения на 1 и 4 кластер, 2 и 3 непонятны, поэтому нужно рассмотреть все дискриминантные функции, построенные для мультиклассификации данных.Мы также можем построить отображение обучающих данных на все пары дискриминантных переменных с помощью функции plot:
```{r}
plot(lda.fit)
```

В соответствии с документацией R, функция LDA вычисляет принадлежность к классу по методу «all vs all» из машинного обучения. LDA модель высчитывает среднюю и вариацию для каждого класса и находит ковариацию для дискриминации каждого класса. Для предсказания модель высчитывает для каждого класса вероятность неправильной классификации, используя теорему Байеса.

Построение графиков обучающих данных для всех пар измерений показывает, что при построении они захватывают большую часть дисперсии. Используя график, мы можем получить интуицию о количестве измерений, которые мы должны выбрать для LDA пониженного ранга. Помните, что ЛД1 и ЛД2 не дали нужных результатов по разделению 2 и 3 кластера. Таким образом, нам нужны дополнительные измерения для дифференциации этих групп. Глядя на графики, кажется, что нам действительно нужны все 3 измерения.

Чтобы визуализировать центроиды групп, мы можем создать собственный график:

```{r}
plot_lda_centroids <- function(lda.fit, data.train, res) {
    centroids <- predict(lda.fit, lda.fit$means)$x
    library(RColorBrewer)
    colors <- brewer.pal(8, "Accent")
    my.cols <- colors[match(lda.pred.train$class, levels(res))]
    my.points <- predict(lda.fit, data.train)$x
    no.classes <- length(unique(res))
    par(mfrow=c(no.classes -1, no.classes -1), mar=c(1,1,1,1), oma=c(1,1,1,10))
    for (i in 1:(no.classes - 1)) {
        for (j in 1:(no.classes - 1)) {
            y <- my.points[, i]
            x <- my.points[, j]
            cen <- cbind(centroids[, j], centroids[, i])
            if (i == j) {
                plot(x, y, type="n") 
                max.y <- max(my.points[, i])
                max.x <- max(my.points[, j])
                min.y <- min(my.points[, i])
                min.x <- min(my.points[, j])
                max.both <- max(c(max.x, max.y))
                min.both <- max(c(min.x, min.y))
                center <- min.both + ((max.both - min.both) / 2)
                text(center, center, colnames(my.points)[i], cex = 3)}
            else {
                plot(x, y, col = my.cols, pch = as.character(res), xlab ="", ylab="")
                points(cen[,1], cen[,2], pch = 21, col = "black", bg = colors, cex = 3)
            }
        }
    }
    par(xpd = NA)
    legend(x=par("usr")[2] + 1, y = mean(par("usr")[3:4]) + 20, 
            legend = rownames(centroids), col = colors, pch = rep(20, length(colors)), cex = 3)
}
plot_lda_centroids(lda.fit, data.train[,-c(11)], data.train$cl)
```

Таким образом, мы можем видеть местоположение центроида каждого кластера в пространстве трех дискриминантных функций. В целом, на большинстве графиков можно увидеть довольно репрезентативную визуализацию, что свидетельствует о высоком уровне классификации. Исключение составляет четвертый график: разбиение на 1 и 4 кластер не столь очевидны ввиду пересечения этих двух центроидов.

```{r}
lda.pred <- predict(lda.fit, data.unknown[,-c(11)])
plot.df <- data.frame(lda.pred$x[,1:2], "Cluster" = data.unknown$cl, 
                    "Prediction" = lda.pred$class)
ggplot(plot.df, aes(x = LD1, y = LD2, color = Cluster, shape = Prediction)) +
        geom_point()
```

На графике ожидаемые наблюдения показаны разными цветами по кластерам, в то время как предсказания модели показаны разными символами. Модель со 100% точностью присвоила бы каждому цвету по одному символу. Таким образом, неверные предсказания обнаруживаются, когда один цвет демонстрирует различные символы. Используя график, мы быстро видим, что большинство путаниц возникает, когда наблюдения 2 и 3 кластера неправильно классифицируются.

Прогноз для наблюдений из тестовой выборки:
"class" содержит информацию о предсказанной группе для каждого наблюдения
"posterior" -- матрица вероятности принадлежности к каждому классу для каждого наблюдения из тестовой выборки
"x" - значение дискриминантной функции

```{r}
lda.pred <- predict(lda.fit, data.unknown[,-c(11)])
names(lda.pred)
```
Таблица соответствия предсказанных классов исходным:
```{r}
lda.pred$class
table(lda.pred$class, data.unknown[,c("cl")])
summary(lda.pred$class)
```

```{r}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))}

misclass(lda.pred$class, data.unknown[,c(11)])
```
По данной таблице можно сделать вывод, что во втором кластере присутствует наибольшее количество ошибок предсказания, что может свидетельствовать об ошибочном изначальном разбиении объектов на кластеры.

Гистограмма значений первой дискриминантной функции:
```{r}
par(mar = rep(2, 4))
ldahist(data = lda.pred$x[,1], g = cl) 
```

Лямбда Уилкса (Чем больше значение λ, тем более желательно присутствие переменной в процедуре дискриминации): 
```{r}
ldam <- manova(as.matrix(data.unknown[,-c(11)]) ~ lda.pred$class)
summary(ldam, test="Wilks")
```
Лямбда Уилкса - показатель качества дискриминатной функции, низкое p-value которого говорит о высокой предсказательной силе модели (известно, что с помощью Лямбды Уилкса возможно сравнивать несколько дискриминантных функций).

Для нашей функции было получено низкое p-value, что говорит о высоком качестве представленной модели. По итогам проведённого анализа мы можем сделать вывод о том, что дискриминатный анализ позволяет достаточно точно прогнозировать значения внутри классов. Таким образом, при возможном добавлении в анализ новых стран, мы можем с высокой уверенностью утверждать о том, что они относились бы к тому или иному кластеру, характеризующемуся определёнными свойствами (например,с высоким уровнем безработицы или с низкой вовлеченностью в профессиональное образование), благодаря чему можно было бы значительно усовершенствовать экономическую политику развития региона, заранее зная их проблемные места.

##  Дополнение к пункту 2 данной главы. Апостериорные вероятности

Помимо преобразования данных в дискриминантные переменные, которое обеспечивается компонентом x, функция прогнозирования также дает апостериорные вероятности, которые могут быть использованы для дальнейшей интерпретации классификатора. Например:
```{r}
posteriors <- lda.pred.train$posterior # N x K matrix

pred.class <- names(which.max(posteriors[1,])) # <=> lda.prediction.train$class[1]
print(paste0("Posterior of predicted class '", pred.class, 
    "' is: ", round(posteriors[1,pred.class], 2)))
```
```{r}
# каковы средние апостериоры для отдельных групп
classes <- colnames(posteriors)
res <- do.call(rbind, (lapply(classes, function(x) apply(posteriors[data.train$cl == x, ], 2, mean))))
rownames(res) <- classes
print(round(res, 3)) 
```

Таблица апостериоров для отдельных классов показывает, что модель наиболее неопределенна относительно 1 кластера, что согласуется с нашими ожиданиями после предыдущих визуализаций.

## Расстояние Махаланобиса
```{r}
mahalanobis(rdata_components, center = kmeans4$centers, cov = cov(rdata_components))
```

Расстояние Махаланобиса - мера расстояния между векторами случайных величин, обобщающая понятие евклидова расстояния. С помощью расстояния Махаланобиса можно определять сходство неизвестной и известной выборки. Оно отличается от расстояния Евклида тем, что учитывает корреляции между переменными и инвариантно к масштабу. В дискриминантном анализе расстояние Махаланобиса используется в определении принадлежности заданной точки одному из классов. Для этого подсчитывается расстояние Махаланобиса до центра каждого из классов и выявляется наименьшее.
К сожалению, на данном этапе наши исследовательские силы были на исходе, поэтому даже общими усилиями мы не смогли придумать, каким именно образом следует относить несколько новых наблюдений к тому или иному кластеру, используя расстояние Махаланобиса. Были бы крайне признательны возможности узнать, как должно выглядеть эталонное решение этого пункта работы.

На этом выполнение компьютерной работы №2 следует считать завершённым. Выводы по итогам всей работы будут представлены в итоговом отчёте, который будет выслан 9.12.2020.