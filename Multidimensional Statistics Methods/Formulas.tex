% Этот шаблон документа разработан в 2014 году
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% <<Документы и презентации в LaTeX>>, записанном НИУ ВШЭ
% для Coursera.org: http://coursera.org/course/latex .
% Исходная версия шаблона --- 
% https://www.writelatex.com/coursera/latex/1.2

\documentclass[a4paper,12pt]{article} % добавить leqno в [] для нумерации слева

%%% Работа с русским языком
\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в формулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы

%%% Дополнительная работа с математикой
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
\usepackage{xcolor}
\usepackage{hyperref}
% Цвета для гиперссылок
\definecolor{linkcolor}{HTML}{799B03} % цвет ссылок
\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}
%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.

%%% Работа с картинками
\usepackage{graphicx}  % Для вставки рисунков
\graphicspath{{images/}{images2/}}  % папки с картинками
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом
\usepackage{tikz} % Работа с графикой

%% Шрифты
\usepackage{euscript}	 % Шрифт Евклид
\usepackage{mathrsfs} % Красивый матшрифт

%% Убираем отступ слева 
\usepackage{indentfirst}
%% Свои команды
\DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
{\hbox{$\mathsurround=0pt #1$}}{}}

\renewcommand{\theenumi}{(\Alph{enumi}}
\renewcommand{\labelenumi}{\theenumi)}

%% Делаем каждый раздел с новой страницы
\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

%%% Заголовок
\author{Архипова Марина Юрьевна, \\
профессор департамента статистики и анализа данных \\
Золотарев Антон Олегович, БСТ182}
\title{Многомерные статистические методы 2021 \\
\\

\large Формулы по курсу. Часть I}
\date{\today}

\begin{document} % конец преамбулы, начало документа

\maketitle

\tableofcontents

\href{https://github.com/Shred27/MSM/blob/main/Table_of_distributions_Mironkina.pdf}{Математико-статистические таблицы распределений}

\section{Предварительный анализ данных}

Коэффициент вариации:

\[V_s=\frac{S}{\overline{x}}\cdot 100\%\]

Относительное линейное отклонение:

\[K_d = \frac{\frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\overline{x})}{\overline{x}}\cdot100\%\]

Квартильное отклонение:

\[d_k=\frac{Q_3-Q_1}{2}\]

Относительный показатель квартильной вариации:

\[K_Q=\frac{d_k}{Me}=\frac{Q_3-Q_1}{2\cdot Q_2}\]

Интерквартильное отклонение:
\[IQR=Q_3-Q_1\]

\subsection{Диагностика выбросов}

Правило $3\sigma$ (если $X \sim \mathcal{N} (\mu, \sigma)$, то $P\{X \notin (\mu-3\sigma; \mu+3\sigma)\} = 0.0027$.):
\[\left[\overline{x}-3\cdot S\le X \le \overline{x}+3\cdot S\right]\]

Правило Чебышёва с параметрами (правило $k\sigma$): 
\[\left[\overline{x}-k\cdot S\le X \le \overline{x}+k\cdot S\right]\]

Правило $k\cdot IQR$:
\[\left[Q_1-k\cdot IQR\le X \le Q_3+k\cdot IQR\right]\]

Если наблюдение находится вне рамок указанных интервалов, то оно считается выбросом по соответствующему правилу.

\subsection{Визуализация выбросов и распределения}

\subsubsection{Ящичковая диаграмма}
Линия на ящике - медиана;

Границы ящика - $Q_1$ и $Q_3$;

Границы усов через правило $k\cdot IQR$;

Точечки за пределами усов - выбросы

\href{https://3.bp.blogspot.com/-sqSGopnp0lo/Uvu_wl_dPQI/AAAAAAAAAgs/F2DBOSdfiU4/s1600/boxplot.PNG}{Пример графика}

\subsubsection{Stemplot/Листовая диаграмма}
Корни дерева - самый левый разряд числа, количество листьев эквивалентно количеству наблюдений в стебле/разряде. Если числа приведены, например, от 1 до 25, до у первого стебля корнем вершины будет 0, у второго - 1, у третьего - 2.

\href{https://i.stack.imgur.com/UGlpf.png}{Пример построения диаграммы}

\href{https://ru.wikipedia.org/wiki/Диаграмма_стебель-листья}{Подробнее о диаграмме}

\subsubsection{Dotplot/Точечная диаграмма}
По оси Х значения переменной, по оси У - их частота встречаемости. Если повторений нет, то, возможно, надо поделить на какое-то число, чтобы наблюдения "скучковались".

\href{https://sun9-32.userapi.com/impg/FsdsXE5cQD1Uj3tscNEvt5yO3F7SLQF9_Ae7jA/B9fgtVsEuHs.jpg?size=456x352&quality=96&proxy=1&sign=ff92dd074fa03764f981eff9c002c696&type=album}{Пример построения диаграммы}

\section{Корреляционный анализ}

\begin{figure}[htp]
    \centering
    \includegraphics{Парная корреляция.png}
\end{figure}

\subsection{Парные коэффициенты корреляции}

\[r_{xy}=\frac{\overline{XY}-\overline{X}\cdot\overline{Y}}{S_x \cdot S_y}\]

Проверка значимости парного коэффициента корреляции (через критерий Стьюдента):

\[ t_{набл}=\frac{r\cdot\sqrt{n-l-2}}{\sqrt{1-r^2}}\]

r - оценка парного коэффициента, l - число фиксируемых переменных, n - количество наблюдений. Критическое значение находится через таблицу распределения Стьюдента \href{https://github.com/Shred27/MSM/blob/main/Table_of_distributions_Mironkina.pdf}{(Ссылка на все таблицы от Миронкиной Ю.Н.)}. Если $|t_{набл}|>t_{кр}$, то парный коэффициент корреляции значимо отличен от нуля с вероятностью ошибки первого рода $\alpha$.

Проверка значимости парного коэффициента корреляции (через критерий Фишера-Йейтса) осуществляется через сравнение найденного наблюдаемого значения коэффициента корреляции и критического значения, которое можно найти через \href{https://studfile.net/preview/1720882/}{таблицу Фишера-Йейтса}



Интервальные оценки парного коэффициента корреляции:

1. Переход к статистике Фишера:
\[Z^{'}=\frac{1}{2}\cdot ln\frac{1+r}{1-r}\]

2. Находим доверительный интервал для статистики Фишера по всем знакомой и известной формуле из математической статистики:
\[P\left(Z^{'}-t_{\gamma}\sqrt{\frac{1}{n-l-3}}\le Z \le Z^{'}+t_{\gamma}\sqrt{\frac{1}{n-l-3}}\right)=\gamma\]

3. Осуществляем обратное преобразование Фишера \href{http://smc.edu.nstu.ru/preobf.htm}{по таблице} и получаем ДИ для нашего коэффициента корреляции (можно воспользоваться функцией "ФИШЕРОБР" в Excel). Обращаем ваше внимание на то, что используемая нами статистика Фишера является нечётной функцией, то есть $Z^{'}(-r)=-Z^{'}(r)$.
\subsection{Частные коэффициенты корреляции}

\[\rho_{xy/z}=\frac{r_{xy}-r_{xz}\cdot r_{yz}}{\sqrt{(1-r_{xz}^2)(1-r_{yz}^2)}}=-\frac{A_{12}}{\sqrt{A_{11}\cdot A_{22}}}\]

Построение доверительных интервалов и проверка значимости частных коэффициентов корреляции производится через те же формулы, которые были приведены для парных коэффициентов корреляции, с одним уточнением: \textbf{для парных l=0, для частных $l$ есть число фиксируемых переменных, например, в трехмерной модели $l=1$, в $k$-мерной $l=k-2$}.

\subsection{Множественные коэффициенты корреляции}

\[r_{1/2,3}=\sqrt{\frac{r_{12}^2+r_{13}^2-2\cdot r_{12}\cdot r_{13}\cdot r{_23}}{1-r_{23}^2}}=\sqrt{1-\frac{|R|}{A_{11}}}\]
|R| - определитель матрицы парных коэффициентов корреляции, $A_{ij}$ - алгебраическое дополнение элемента $r_{ij}$ корреляционный матрицы R.

Множественный коэффициент корреляции, возведённый в квадрат, называется множественным коэффициентом детерминации и играет крайне важную роль в регрессионном анализе: он отражает, какая доля дисперсии целевой переменной может быть объяснена влиянием фиксируемых переменных (2, 3 в примере выше). То есть это своего рода верхняя граница качества регресионной модели, показывающая также качество отобранных для анализа переменных.

Проверка значимости множественного коэффициента корреляции через F-критерий Фишера-Снедекора:
\[ F_{\text{набл}}=\frac{\frac{1}{k-1}r_{1/2,...,k}^2}{\frac{1}{n-k}(1-r_{1/2,...,k}^2)}\]

В рамках задач курса на ручной счёт вам предстоит работать лишь с трёхмерными моделями, где $k=3$.

\section{Регрессионный анализ}

\begin{figure}[htp]
    \centering
    \includegraphics{Регрессия.png}
\end{figure}

Рассмотрим частный случай модели двумерной регрессии (для задач на ручной счёт этого будет достаточно):
\[y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\]
где $y_i$, $x_i$ --- значение зависимой и независимой переменной для наблюдения $i$ (наблюдаемые величины), $\beta_0$, $\beta_1$ --- коэффициенты уравнения регрессии, $\varepsilon_i \sim IID (0; \sigma^2)$ --- случайная ошибка для $i$-го наблюдения (ненаблюдаемые величины). Параметры модели для оценивания: $\beta_0, \beta_1, \sigma^2$.

Оценки регрессионных коэффициентов $b_0, b_1$ могут быть получены методом наименьших квадратов (МНК):
\[L = \sum\limits_{i=1}^n(y_i-b_0-b_1 x_i)^2 \xrightarrow{b_0, b_1} min\]
и при выполнении условий Гаусса-Маркова оценки МНК являются эффективными в классе линейных несмещенных оценок.

В результате решения оптимизационной задачи, мы получаем следующие формулы для нахождения оценок коэффициентов регрессии $b_0, b_1$:

\[ b_0 = \overline{y}-b_1\cdot \overline{x} \]

\[b_1 = \frac{\overline{xy}-\overline{x}\overline{y}}{s_x^2} \]

$s_x^2$ здесь - это дисперсия объясняющей переменной. Смысловая нагрузка двух найденных коэффициентов такова: $b_0$ - точка пересечения линии регрессии с осью целевой переменной, необъясняемая признаками константа; $b_1$ - наклон линии регрессии, характеризует силу и направление влияния объясняющей перменной на целевую.

\subsection{Отклонение выходов модели от реальных значений наблюдений}

\begin{figure}[htp]
    \centering
    \includegraphics[width=15cm]{TSS Regression.jpg}
\end{figure}

Квадраты отклонений целевой перменной (отсылка к МС: дисперсия, умноженная на количество наблюдений) в регресионном анализе обозначаются как $Q_{\text{общ}}$ или TSS, разбиваясь на две величины: $Q_R$ (воздействие объясняющей переменной, также обозначается RSS) и $Q_{\text{ост}}$ (необъяснённые в модели различия целевой переменной, также обозначается ESS).  Формально:
\[TSS=RSS+ESS\]
\[TSS=Q_{\text{общ}}=\sum\limits_{i=1}^n (y_i-\overline{y})^2\]
\[RSS=Q_{\text{рег}}=\sum\limits_{i=1}^n (y_i-\hat{y_i})^2\]
\[ESS=Q_{\text{ост}}=\sum\limits_{i=1}^n (\hat{y_i}-\overline{y})^2\]

При этом следует заметить, что отношение $\frac{RSS}{TSS}$ представляет из себя коэффициент детерминации (в случае многомерной (хотя бы трёхмерной) регрессии уместно вспомнить про квадрат множественного коэффициента корреляции и проверить идентичность с коэффициентом детерминации):

\[r^2=\frac{RSS}{TSS}=\frac{TSS-ESS}{TSS}\]

\subsection{MSE и дисперсия коэффициентов}

Остаточное среднеквадратическое отклонение:
\[s=\sqrt{MSE}=\sqrt{\frac{ESS}{n-2}}\]

По аналогии с курсом математической статистики, мы можем оценить разброс найденных коэффициентов регрессии, посчитав их дисперсию:
\[s_{b_1}=\frac{s}{\sqrt{\sum\limits_{i=1}^n (x_i-\overline{x})^2}} = \frac{s}{\sqrt{(n-1)\cdot s_x^2}}\]

\[s_{b_0}^2=s^2=\frac{\sum\limits_{i=1}^n (y_i-\hat{y_i})^2}{n-2}\]

\subsection{Проверка гипотезы о значимости коэффициентов}
Найденные СКО коэффициентов очень важны для проверки значимости регресионной модели (значимо ли влияют объясняющие переменные на целевую?). Проверка гипотезы о незначимости коэффициента регрессии:
\[t_{\text{набл}}=\frac{b_1}{s_{b_1}}\]
\[t_{\text{кр}}(\alpha, \nu=n-2)\]
Если наблюдаемое значение больше критического, то гипотеза о незначимости отклоняется, то есть с вероятностью ошибки первого рода $\alpha$ существует статистически значимая линейная связь между объясняющей и целевой переменными.

Если у нас более одной объясняющей переменной, то необходимо применить F-test для проверки гипотезы:
\[F_{\text{набл}}=\frac{MSR}{MSE}=\frac{\frac{RSS}{k}}{\frac{ESS}{n-2}}=\frac{RSS \cdot (n-2)}{ESS \cdot k}\]
\[F_{\text{кр}}(\alpha, \nu_1=1, \nu_2=n-2)\]

где $MSR=\frac{RSS}{k}$ - среднеквадратичное отклонение, объясняемое в рамках регрессионной модели, k - количество объясняющих переменных.

\subsection{Доверительные интервалы}

Также значимость коэффициентов регрессии можно вычислять через построение доверительных интервалов:
\[P(b_1-t_{\alpha}s_{b_1}\le \beta_1 \le b_1+t_{\alpha}s_{b_1})=\gamma\]
\[P(b_0-t_{\alpha}s_{b_0}\le \beta_0 \le b_0+t_{\alpha}s_{b_0})=\gamma\]
$t_{\alpha}$ - из таблицы распределения Стьюдента для заданного $\alpha$ и $\nu=n-2$

При выполнении компьютерной работы крайне важным условием для проведения регресионного анализа через МНК является проверка модели на гомоскедастичность - мы должны удостовериться в постоянстве(одинаковости) дисперсии рассматриваемых признаков!

\section{Метод главных компонент}

\begin{figure}[htp]
    \centering
    \includegraphics[width=15cm]{МГК.jpg}
\end{figure}

Этапы проведения МГК:
\begin{enumerate}
    \item Стандартизация переменных:
\[X^*=\frac{X-\overline{X}}{S}\]
$\overline{X}$ - вектор средних для каждой объясняющей переменой \\
$S$ - вектор СКО для каждой объясняющей переменной
    \item Расчет корреляционной матрицы:\\
$R={{1}\over{n}}{X^*}^TX^*$, матрица R должна обладать размерностью $k\times k$, где k - количество объясняющих переменных.
    \item Поиск собственных значенний корреляционной матрицы (рассмотрен случай для 2 объясняющих переменных):
\begin{equation*}
    det \left(
\begin{array}{cc}
     1-\lambda&r_{12}  \\
     r_{12}&1-\lambda
\end{array}
\right)=(1-\lambda)^2-r_{12}^2=0
\end{equation*}
Легко вывести, что для случая двух объясняющих переменных мы имеем следующие собственные значения:
$\lambda_1=1+r_{12}$, 
$\lambda_2=1-r_{12}$\\
Поиск собственных векторов:\\
$\lambda_1=1+r_{12}$\\
\begin{equation*}
    \left(
\begin{array}{cc}
     1-1-r_{12}&r_{12}  \\
     r_{12}&1-1-r_{12}
\end{array}
\right)
\left(
\begin{array}{c}
     1  \\
    1
\end{array}
\right)=0, \;
l'_1=\left(
\begin{array}{c}
     ?  \\
    ?
\end{array}
\right)
\end{equation*}
$\lambda_2=1-r_{12}$\\
\begin{equation*}
    \left(
\begin{array}{cc}
     1-1+r_{12}&r_{12}  \\
     r_{12}&1-1+r_{12}
\end{array}
\right)
\left(
\begin{array}{c}
     1  \\
    1
\end{array}
\right)=0, \;
l'_2=\left(
\begin{array}{c}
     ?  \\
    ?
\end{array}
\right)
\end{equation*}
\item Построение ортогональной матрицы собственных векторов:\\
$L=(l_1 \; l_2)$\\
$l_i={{l'_i}\over{\sqrt{(l'_i)^Tl'_i}}}$\\


\item Построение матрицы факторных нагрузок:\\
$A=L\Lambda^{1/2}=(A_1 \; A_2)$\\

\item Построение матрицы нормированных значений главных компонент:\\
$Z=X^*(A^T)^{-1}$\\
Напомним, что обратная матрица для размерности 2*2 предполагает смену мест значений на главной диагонали и изменение знаков на побочной, при этом каждое из значений необходимо поделить на определитель исходной матрицы.
\item Поиск коэффицентов регрессии, построенной на главные компоненты:\\
$\hat{\beta}=(\tilde{Z}^T\tilde{Z})^{-1}\tilde{Z}^TY$ \\
$\tilde{Z}$ отличается от найденной пунктом ранее матрицы $Z$ лишь добавлением слева столбца из единичек, обозначающих константу.

\end{enumerate}

Удельный вклад 1-й компоненты: $\frac{\lambda_1}{\sum \lambda_i} = \frac{1+r_{12}}{2}$, удельный вклад 2-й компоненты: $\frac{\lambda_2}{\sum \lambda_i} = \frac{1-r_{12}}{2}$.

\section{Кластерный анализ}

\subsection{Методы вычисления расстояния между наблюдениями}
Евклидово расстояние:
\[d_E(x_i,x_j) = \sqrt{\sum_{l=11в}^{k}(x_{il} - x_{jl})^{2}}\]

Взвешенное Евклидово расстояние:
\[d_{WE}(x_i, x_j) = \sqrt{\sum\limits_{l=1}^k w_l(x_i^{(l)}-x_j^{(l)})^2}\]

Где $\sum\limits_{l=1}^k w_l=1$ k - количество признаков, в пространстве которых находятся наблюдения 

Расстояние по метрике городских кварталов (Манхэттеновское, Хеммингово):
\[d_T(x_i, x_j) = |x_{i1} - x_{j1}| + |x_{i2}-x_{j2}| = \sum\limits_{l=1}^k |x_i^{(l)}-x_j^{(l)}|\]

Расстояние Махаланобиса:
\[d_M=\sqrt{(x_i-x_j)^T \Sigma^{-1}(x_i-x_j)}\]

Расстояние Минковского:
\[d_T(x_i, x_j) = \left(\sum\limits_{l=1}^k |x_i^{(l)}-x_j^{(l)}|^p\right)^{1/p}\]

Большее количество методов вычисления расстояния между объектами можно посмотреть в \href{https://github.com/Shred27/MSM/blob/main/Dictionary_Of_Distances_Deza.pdf}{Энциклопедическом словаре расстояний}.

\subsection{Методы вычисления расстояния между группами наблюдений} 
    \begin{itemize}
        \item “ближнего соседа”;\\
        $d_{min}(S_l,S_m)=min_{x_i \in S_l,x_j \in S_m}d(x_i,x_j)$
        
         \item “дальнего соседа”;\\
        $d_{max}(S_l,S_m)=max_{x_i \in S_l,x_j \in S_m}d(x_i,x_j)$
        
        \item средней связи;\\
         $d_{average}(S_l,S_m)={{1} \over{n_ln_m}}\sum_{x_i \in S_l,x_j \in S_m}d(x_i,x_j)$
         
          \item центра тяжести.\\
        $d_{center}(S_l,S_m)=d(\Bar{x_i},\Bar{x_j}) $
    \end{itemize}

\subsection{Функционалы качества}

\begin{itemize}
    \item Сумма внутриклассовых дисперсий:
    
    \begin{equation*}
        Q_{1}(S) = \sum_{l=1}^{p}\sum_{O_{i} \in S_{i}}d^{2}(O_{i}, \Bar{X}(l)) \rightarrow min,
    \end{equation*}
    
    $p$ -- число классов;
    
    $S_{l}$ -- $l$-ый класс в классификации $S$;
    
    $\Bar{X}(l)$ -- центр класса $S_{l}$.
    
    \item Сумма попарных внутриклассовых расстояний между объектами:
    
    \begin{equation*}
        Q_{2}(S) = \sum_{l=1}^{p}\sum_{O_{i}O_{j} \in S_{il}}d^{2}(O_{i}, O_{j}) \rightarrow min
    \end{equation*}
        
    \item Обобщённая внутриклассовая дисперсия:
    
    \begin{equation*}
        Q_{3}(S) = \sum_{l=1}^{p}\sum_{j=1 \in k}S_{j}^{2}(l) \rightarrow min,
    \end{equation*} 
    
    $S_{j}^{2}(l)$ -- оценка дисперсии $j$-ого признака $l$-ого класса.
    
\end{itemize}

\subsection{Иерархические методы классификации (дендрограммы)}

При необходимости кластеризации относительно небольшого числа наблюдений будет уместно представить визуализацию пошагового объединения наблюдений в кластеры через дендрограмму. По оси ОУ на ней представлено расстояние между сгруппированными объектами, по оси ОХ - номера объединяемых наблюдений:

\begin{figure}[htp]
    \centering
    \includegraphics{Дендрограмма.jpg}
\end{figure}

\subsection{Метод классификации через k-средних}

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Метод к-средних.jpg}
\end{figure}

При необходимости кластеризации большого числа наблюдений следует заранее определить оптимальное число кластеров (в машинном обучении число кластеров называется гиперпараметром) и вычислять принадлежность наблюдений к тому или иному классу через итеративные алгоритмы, наиболее популярным из которых является алгоритм к-средних (k-means):

Как применяется метод?
\begin{enumerate}
    \item Выбрать случайно заданные координаты центров кластеров $(c_1, c_2, ..., c_K)$ для того количества кластеров, которое мы избрали оптимальным
    \item Отнести каждый объект к ближайшему из случайно на предыдущем шаге заданных центров:
    \[y_i=arg\; min \; d(x_i, c_j)\]
    
    \item Найти центр тяжести для каждой совокупности наблюдений, объединённых на данной итерации в один кластер, и переместить в этот центр тяжести центр каждого из кластеров (для каждого кластера разный центр тяжести!):
    
    \[c_j = \frac{\sum\limits_{i=1}^n x_i\cdot I[y_i=j]}{\sum\limits_{i=1}^n I[y_i=j]}\]
    
    \item Повторять два предыдущих шага до момента, пока на новой итерации наблюдения не перестанут переходить из одного кластера в другой
\end{enumerate}

\section{Дискриминантный анализ}

\section{Решающие деревья}

\subsection{Задача классификации}

\subsection{Задача регрессии}

\section{Секретные темы, название которых вы узнаете позже...}
\end{document} % конец документа

