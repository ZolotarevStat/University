{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6ad690",
   "metadata": {},
   "source": [
    "# МДЗ-2\n",
    "\n",
    "## Золотарев Антон Олегович\n",
    "\n",
    "# Ссылочки на участки кода\n",
    "\n",
    "Задание 1. [ссылка на результат](https://storage.yandexcloud.net/bucket-zolotarev-bst/mhw2_task1.txt)\n",
    "\n",
    "Задание 2 не сделал, ибо тупил с разворачиванием ноутбука как в семинаре 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fef50b",
   "metadata": {},
   "source": [
    "*Note: Выполнение задания 1 было осуществлено в консоли*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a20723",
   "metadata": {},
   "source": [
    "# Предварительные операции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c9d87",
   "metadata": {},
   "source": [
    "Зайдём на мастер ноду и откроем датасет из прошлой МДЗ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13dcf0",
   "metadata": {},
   "source": [
    "```bash\n",
    "eval `ssh-agent -s`\n",
    "ssh-add lsml_ssh\n",
    "ssh yc-user@62.84.125.253\n",
    "eval `ssh-agent -s`\n",
    "logout\n",
    "ssh -A -J yc-user@62.84.125.253 ubuntu@rc1a-dataproc-c-0430dmrwee1whrn9.mdb.yandexcloud.net\n",
    "cd ~/mhw1/7z-mhw1\n",
    "head -n 5 VisitsStream.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ce403d7",
   "metadata": {},
   "source": [
    "UserID  IPID    AdID    ViewDate\n",
    "59703   1259356 469877  2015-04-25 00:00:00.0\n",
    "154389  1846749 27252551        2015-04-25 00:00:00.0\n",
    "218628  2108380 31685325        2015-04-25 00:00:00.0\n",
    "231535  837110  18827716        2015-04-25 00:00:00.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34e63a",
   "metadata": {},
   "source": [
    "# Задание 1 (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7335279",
   "metadata": {},
   "source": [
    "В `VisitsStream.tsv` лежит информация про пользователей, которые открывают сайт. Используя классический Hadoop MapReduce необходимо посчитать топ 10 пользователей с самыми длинными по времени сессиями и время этой самой длинной сессии (в секундах).\n",
    "\n",
    "Сессия определяется следующим образом - это окно времени, внутри которого временное расстояние от двух соседних посещений не более **15 минут**. \n",
    "\n",
    "Иными словами - если пользователь зашел на сайт в момент X и последнее предыдущее посещение сайта в момент Y было не позднее чем 15 минут назад, то сессия \"продлевается\" до текущего момента. Если же временное расстояние от X до Y более 15 минут, то считается, что предыдущая сессия закончилась в момент Y, а новая сессия началась в момент X.\n",
    "\n",
    "Сессия может длится 0 секунд, если пользователь сделал всего 1 запрос в течение 30 минутного окна (в середине этого окна). Считается, что в начале у пользователя нет открытой сессии и что сессия автоматически заканчивается, когда записей больше не осталось.\n",
    "\n",
    "Выводить нужно только уникальных пользователей и для каждого такого пользователя находить время самой длинной его сессии. \n",
    "\n",
    "При решении можно использовать произвольное количество MapReduce задач, но чем меньше, тем лучше. За излишне неоптимальное решение можно потерять балл. \n",
    "\n",
    "Полученный файл с топ 10 нужно будет выложить в облако, обеспечить публичный доступ до него и приложить к решению.\n",
    "\n",
    "В ноутбуке должны присутствовать ячейки с \n",
    "\n",
    "1) Всеми необходимыми скриптами для работы ваших MapReduce задач\n",
    "\n",
    "2) Командами запуска самих MapReduce задач\n",
    "\n",
    "3) Ссылкой на итоговый результат работы в вашем облаке. Ссылки должны быть рабочими до того момента, как вашу домашку не проверят.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb4450",
   "metadata": {},
   "source": [
    "Посмотрим как дела в HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c880c",
   "metadata": {},
   "source": [
    "```bash\n",
    "hdfs dfs -ls /\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6be818b8",
   "metadata": {},
   "source": [
    "Found 4 items\n",
    "drwx------   - mapred hadoop          0 2022-01-25 23:07 /hadoop\n",
    "drwxrwxrwt   - hdfs   hadoop          0 2022-01-25 23:07 /tmp\n",
    "drwxrwxrwt   - hdfs   hadoop          0 2022-01-26 00:41 /user\n",
    "drwxrwxrwt   - hdfs   hadoop          0 2022-01-25 23:07 /var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf3422",
   "metadata": {},
   "source": [
    "Обрежем заголовки, и положим датасет в HDFS для возможности использования Hadoop MapReduce, после чего балансируем полученное"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fd35e",
   "metadata": {},
   "source": [
    "```bash\n",
    "tail -n +2 VisitsStream.tsv > VisitsStreamRaw.tsv\n",
    "hdfs dfs -mkdir -p /user/avito/data\n",
    "hdfs dfs -put VisitsStreamRaw.tsv /user/avito/data/\n",
    "sudo hdfs balancer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b53442",
   "metadata": {},
   "source": [
    "Чтобы решить задачу, необходимо декомпозировать поставленную задачу на следующие проблемы:\n",
    "1. На мап-стадии объединяем UserID & ViewDate\n",
    "2. Партишонер раскидывает строчки на редьюсер по ключу UserID и сортирует сначала по этому ключу, затем по дате\n",
    "3. Редьюсер считает для каждого пользователя протяжённость самой длинной сессии\n",
    "4. После этого получаем неотсортированный рейтинг пользователей с самыми длинными сессиями, переходим ко второй Map-Reduce операции\n",
    "5. Маппим по составному ключу, состоящему из UserID & протяжённости его самой длинной сессии\n",
    "6. Партишонер сортирует по самой длинной сессии\n",
    "7. Единый редьюсер выводит лишь 10 самых крутых пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d49465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T16:54:49.107043Z",
     "start_time": "2022-02-21T16:54:49.084753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./top10_longest_sessions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './top10_longest_sessions.py'\n",
    "\n",
    "import collections\n",
    "import sys\n",
    "import csv\n",
    "import datetime\n",
    "from itertools import groupby, islice\n",
    "\n",
    "def mapping():\n",
    "    for row in csv.reader(iter(sys.stdin.readline, ''), delimiter='\\t'):\n",
    "        user_id, session_start = row[0], row[3]\n",
    "        print(\"{}+{}\\t\".format(user_id.strip(), session_start.strip()))\n",
    "\n",
    "def reducing():\n",
    "    for user_id, session_starts in groupby(kv_stream('+'), lambda x: x[0]):\n",
    "        user_session_starts = (x.strip() for _, x in session_starts)\n",
    "\n",
    "        max_session_length = 0\n",
    "        session_length = 0\n",
    "        session = datetime.datetime.strptime(next(user_session_starts), '%Y-%m-%d %H:%M:%S.%f')\n",
    "        for next_session in user_session_starts:\n",
    "            next_session = datetime.datetime.strptime(next_session, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            curr_session_length = (next_session - session).total_seconds()\n",
    "            if curr_session_length <= 900.0:\n",
    "                session_length += curr_session_length\n",
    "                if session_length > max_session_length:\n",
    "                    max_session_length = session_length\n",
    "            else:\n",
    "                session_length = 0         \n",
    "            session = next_session        \n",
    "        print('{}\\t{}'.format(user_id, max_session_length))\n",
    "    \n",
    "def top_10_mapping():\n",
    "    for k, v in map(lambda x: x.split('\\t'), sys.stdin):\n",
    "        print(\"{}+{}\\t\".format(k.strip(), v.strip()))\n",
    "\n",
    "def top_10_reducing():\n",
    "    first_10_stream = islice(map(lambda x: x.split('+'), sys.stdin), 10)\n",
    "    for user, session_len in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(user, session_len.strip()))\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {'map': mapping,\n",
    "    'reduce': reducing,\n",
    "    'top_10_map': top_10_mapping,\n",
    "    'top_10_reduce': top_10_reducing,\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63174c7",
   "metadata": {},
   "source": [
    "После этого я закинул код в бакет и перенёс в отдельную папку для скриптов\n",
    "\n",
    "```bash\n",
    "hdfs dfs -get s3a://bucket-zolotarev-bst/top10_longest_sessions.py ~/scripts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff87815",
   "metadata": {},
   "source": [
    "Запускаем мап-редьюс по аналогии с семинарами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e8bac",
   "metadata": {},
   "source": [
    "```bash\n",
    "yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"length-max-session\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-files ~/scripts/top10_longest_sessions.py \\\n",
    "-mapper \"python3 top10_longest_sessions.py map\" \\\n",
    "-reducer \"python3 top10_longest_sessions.py reduce\" \\\n",
    "-input /user/avito/data/ \\\n",
    "-output /user/avito/longest-sessions/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01fec8",
   "metadata": {},
   "source": [
    "Выцепляем топ-10 пользователей с самыми длинными сессиями\n",
    "```bash\n",
    "yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top10-sessions\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files ~/scripts/top10_longest_sessions.py \\\n",
    "-mapper \"python3 top10_longest_sessions.py top_10_map\" \\\n",
    "-reducer \"python3 top10_longest_sessions.py top_10_reduce\" \\\n",
    "-input /user/avito/longest-sessions/ \\\n",
    "-output /user/avito/top10-longest-sessions/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2c4d8",
   "metadata": {},
   "source": [
    "```bash\n",
    "hdfs dfs -cat /user/avito/top10-longest-sessions/part*\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3916d81c",
   "metadata": {},
   "source": [
    "4143904 358181.0\n",
    "1113291 329463.0\n",
    "1203081 170447.0\n",
    "4263912 80605.0\n",
    "1987990 70544.0\n",
    "563902  68359.0\n",
    "2853587 53371.0\n",
    "2735474 49881.0\n",
    "1370797 49791.0\n",
    "2271348 48099.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4c824",
   "metadata": {},
   "source": [
    "Файлик с полученным результатом - https://storage.yandexcloud.net/bucket-zolotarev-bst/mhw2_task1.txt\n",
    "```bash\n",
    "hdfs dfs -cp /user/avito/top10-longest-sessions/part* s3a://bucket-zolotarev-bst/mhw2_task1.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1baf93f",
   "metadata": {},
   "source": [
    "# Задание 2.1\n",
    "\n",
    "Найти топ 10 самых популярных фильтров. Фильтры кодируются ключами в словаре SearchParams (именно ключи (числа), а не значения)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9354cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T20:13:44.074319Z",
     "start_time": "2022-02-21T20:13:44.056505Z"
    }
   },
   "source": [
    "```bash\n",
    "mkdir mhw2\n",
    "kaggle competitions download -c avito-context-ad-clicks -f SearchInfo.tsv.7z\n",
    "kaggle competitions download -c avito-context-ad-clicks -f trainSearchStream.tsv.7z\n",
    "kaggle competitions download -c avito-context-ad-clicks -f testSearchStream.tsv.7z\n",
    "kaggle competitions download -c avito-context-ad-clicks -f AdsInfo.tsv.7z\n",
    "\n",
    "mv SearchInfo.tsv.7z mhw2\n",
    "mv trainSearchStream.tsv.7z mhw2\n",
    "mv testSearchStream.tsv.7z mhw2\n",
    "mv AdsInfo.tsv.7z mhw2\n",
    "cd mhw2\n",
    "\n",
    "7z x SearchInfo.tsv.7z \n",
    "7z x trainSearchStream.tsv.7z\n",
    "7z x testSearchStream.tsv.7z\n",
    "7z x AdsInfo.tsv.7z\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
