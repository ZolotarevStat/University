{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3SxLWSyzowa"
   },
   "source": [
    "**Задание 1.** Точность почти в 10 раз меньше необходимой, ошибка на этапе построения спрямляющего пространства (3/5 за старания и доведения до конца?)\n",
    "\n",
    "**Задание 2.** Не проведена кросс-валидация для градиентного бустинга, не дождался окончания работы классического линейного SVM (0.5/3?)\n",
    "\n",
    "**Задание 3.** Нет пункта (2), написал код, но не произвёл вычисления для двух других пунктов (0.2/2?)\n",
    "\n",
    "**Штраф** -1 балл за отправку после мягкого дедлайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYp0bXOFK-hP"
   },
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 05.02.2021\n",
    "\n",
    "Мягкий дедлайн: 01:59MSK 21.02.2021\n",
    "\n",
    "Жесткий дедлайн: 01:59MSK 24.02.2021\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через систему anytask. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-08-random-features-Username.ipynb\n",
    "\n",
    "Username — ваша фамилия и имя на латинице именно в таком порядке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY8vT0W_K-hR"
   },
   "source": [
    "### О задании\n",
    "\n",
    "На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n",
    "\n",
    "Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n",
    "\n",
    "Мы будем исследовать аппроксимации методом Random Fourier Features (RFF, также в литературе встречается название Random Kitchen Sinks) для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n",
    "$$\\tilde \\varphi(x) = (\n",
    "\\cos (w_1^T x + b_1),\n",
    "\\dots,\n",
    "\\cos (w_n^T x + b_n)\n",
    "),$$\n",
    "где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n",
    "\n",
    "На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n",
    "\n",
    "Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n",
    "\n",
    "### Алгоритм\n",
    "\n",
    "Вам потребуется реализовать следующий алгоритм:\n",
    "1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n",
    "2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n",
    "3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n",
    "4. Сформировать n_features новых признаков по формулам, приведённым выше.\n",
    "5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n",
    "6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_sGunb7K-hS"
   },
   "source": [
    "Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YyG6dBfjK-hS"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train_pics.reshape(x_train_pics.shape[0], -1)\n",
    "x_test = x_test_pics.reshape(x_test_pics.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1lm24AKidEV"
   },
   "source": [
    "Импортируем библиотеки для имплементации алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vXZUqEkl7P2-"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import random\n",
    "#import itertools\n",
    "from statistics import median\n",
    "from math import pi\n",
    "from math import cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQI5w85Ki9nX"
   },
   "source": [
    "# Проверка алгоритма на верность вне класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_b8iYjVjOfE"
   },
   "source": [
    "Сначала построим алгоритм вне класса, чтобы проверить его на верность выполнения (для простоты будем использовать n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "pMwJw9ph7UUT"
   },
   "outputs": [],
   "source": [
    "x_1 = x_train\n",
    "pca_train = PCA(n_components=50).fit_transform(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "IdEEzKxNBYU4"
   },
   "outputs": [],
   "source": [
    "already_used = [[]]\n",
    "sigma = []\n",
    "for k in range((10**6)*2):\n",
    "  i = random.randint(0, len(pca_train)-1)\n",
    "  j = random.randint(0, len(pca_train)-1)\n",
    "  if i!=j and np.any(already_used != [i, j]) and np.any(already_used != [j, i]):\n",
    "    #добавляем комбинацию в массив, чтобы гарантировать отсутствие повторений\n",
    "    already_used.append([i, j])\n",
    "    sigma_i=0\n",
    "    # pca_train.shape[1] = n_components => считаем квадрат отклонения по всем признакам между двумя парами объектов\n",
    "    for d in range(pca_train.shape[1]):\n",
    "      sigma_i += (pca_train[i][d]-pca_train[j][d])**2\n",
    "    sigma.append([sigma_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "eUBTdNUfUTZY"
   },
   "outputs": [],
   "source": [
    "#Оцениваем гиперпараметр на полученной выборке\n",
    "sigma = np.array(sigma)\n",
    "sigma_median = median(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rs5LulAUs-I",
    "outputId": "313b695b-3b98-4a09-9150-8a8487ff8224"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Проверяем, что было сравнено действительно много пар объектов\n",
    "len(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "GxqUA_rzWYZv"
   },
   "outputs": [],
   "source": [
    "w = np.random.normal(0, 1/sigma_median, 1000*50)\n",
    "w = np.reshape(w, (1000, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "pEeVfX40W_0A"
   },
   "outputs": [],
   "source": [
    "b = np.random.uniform(-pi, pi, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMaSr6lijpcO",
    "outputId": "718b4518-72cf-4374-a4fc-10c12dfc3260"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.86953494e-05, -2.69624385e-04, -3.14670900e-04, ...,\n",
       "       -7.13280550e-06,  1.18547818e-05, -2.16234279e-04])"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.reshape(w[1], (len(pca_train),1)) * pca_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hr2NUmdOlnMX",
    "outputId": "8e4e0760-c1e7-4a1d-fc46-df017216749e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "8lt7LYisXwn-"
   },
   "outputs": [],
   "source": [
    "phi = []\n",
    "for i in range(1000):\n",
    "  phi.append([w[i] @ pca_train.T + b[i]])\n",
    "\n",
    "phi = np.array(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "fPTtz1Ygydpp"
   },
   "outputs": [],
   "source": [
    "phi = np.vectorize(cos)(phi.reshape(len(pca_train),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWhQv0vtWOG3",
    "outputId": "2e2303b7-a2c4-4d86-b09b-9f193c10dd14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 100)"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YgGNDEkzVL-",
    "outputId": "c2fd3a80-a495-4c28-b405-abbec70bb1eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('linearsvc',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=0,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "clf = make_pipeline(StandardScaler(), LinearSVC(random_state=0))\n",
    "clf.fit(phi, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctA2PlyOhZWd"
   },
   "source": [
    "Повторим манипуляции для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "DVyMRlrDg_JO"
   },
   "outputs": [],
   "source": [
    "pca_test = PCA(n_components=50).fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "yu6heBm2hYlJ"
   },
   "outputs": [],
   "source": [
    "already_used = [[]]\n",
    "sigma = []\n",
    "for k in range((10**6)*2):\n",
    "  i = random.randint(0, len(pca_test)-1)\n",
    "  j = random.randint(0, len(pca_test)-1)\n",
    "  if i!=j and np.any(already_used != [i, j]) and np.any(already_used != [j, i]):\n",
    "    #добавляем комбинацию в массив, чтобы гарантировать отсутствие повторений\n",
    "    already_used.append([i, j])\n",
    "    sigma_i=0\n",
    "    # pca_test.shape[1] = n_components => считаем квадрат отклонения по всем признакам между двумя парами объектов\n",
    "    for d in range(pca_test.shape[1]):\n",
    "      sigma_i += (pca_test[i][d]-pca_test[j][d])**2\n",
    "    sigma.append([sigma_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "lprthhu4hr1R"
   },
   "outputs": [],
   "source": [
    "#Оцениваем гиперпараметр на полученной выборке\n",
    "sigma = np.array(sigma)\n",
    "sigma_median = median(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "ufraBRsUhyxf"
   },
   "outputs": [],
   "source": [
    "phi_test = []\n",
    "for i in range(1000):\n",
    "  phi_test.append([w[i] @ pca_test.T + b[i]])\n",
    "\n",
    "phi_test = np.array(phi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "CZx_I3ptY6lK"
   },
   "outputs": [],
   "source": [
    "phi_test = np.vectorize(cos)(phi_test.reshape(len(pca_test),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnjnMEyCZT2c",
    "outputId": "8ca965bd-0f6c-4d58-f292-5ba18d148395"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 144,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtMEkWPV_3Xa",
    "outputId": "2d4514c8-23af-45ed-8193-7bcbfa2edd45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.711397057083308"
      ]
     },
     "execution_count": 165,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(phi_test.T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZgcXPtXnBlr",
    "outputId": "f88314d9-6272-4253-94b3-d59c53c56eec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 166,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = LinearSVC(random_state=0)\n",
    "svm.fit(phi, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIO_fdN_nUDw",
    "outputId": "84b1db57-c0cd-4234-e165-db98aefff19c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0953"
      ]
     },
     "execution_count": 167,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(phi_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "FtWaERFAZdor"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(phi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyxgWpzN39g0",
    "outputId": "485a1608-09ed-4230-c39b-61b749cfc1be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0984"
      ]
     },
     "execution_count": 146,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(phi_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "fZrtisBqPdKI"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghUMemXoQDOM",
    "outputId": "b92ccb9f-1b63-4539-cccd-45f9f32dd758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(phi, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHV2tfpcQJtk",
    "outputId": "a1878754-1e9b-4be6-d57d-6042f7029337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0983"
      ]
     },
     "execution_count": 171,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(phi_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9xVk4ThmobW",
    "outputId": "3aba382d-f289-4caf-d50c-a66c517616f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.68814859, -0.68803182, -0.68862717, ..., -0.68792254,\n",
       "        -0.68847516, -0.68830349],\n",
       "       [ 0.92044824,  0.92047345,  0.92055018, ...,  0.92039552,\n",
       "         0.92044533,  0.92043013],\n",
       "       [ 0.75954502,  0.75972819,  0.75969182, ...,  0.75977316,\n",
       "         0.75948341,  0.75936504],\n",
       "       ...,\n",
       "       [-0.94531213, -0.94521868, -0.94515596, ..., -0.94512737,\n",
       "        -0.94523074, -0.94522855],\n",
       "       [ 0.57995871,  0.58022367,  0.5800974 , ...,  0.58016402,\n",
       "         0.57993726,  0.57994272],\n",
       "       [-0.21208242, -0.21188784, -0.21097547, ..., -0.21154633,\n",
       "        -0.21176609, -0.21197081]])"
      ]
     },
     "execution_count": 176,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_test[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGPpcpnBlRa7",
    "outputId": "0a076b54-f5a5-41b3-ca97-233bf017fd58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 0, 0, 0, 0, 0, 8, 8, 0, 8], dtype=uint8)"
      ]
     },
     "execution_count": 177,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(phi_test)[::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S80sI-l8zFRp"
   },
   "source": [
    "Ощущение, что на этапе имплементации спрямляющего пространства я допустил грубую ошибку (сказалось ужасное знание линала) и поэтому модель где-то сломалась, выдавая лишь одну разделяющую прямую, превратившись в модель бинарной классификации, которая при этом в рамках двух классов выдаёт точность меньше константной (которая должна была быть 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9I3nlpktjCs5"
   },
   "source": [
    "\n",
    "# Выполнение задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJNN55F7K-hT"
   },
   "source": [
    "__Задание 1. (5 баллов)__\n",
    "\n",
    "Реализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса ниже или написать свой интерфейс.\n",
    "\n",
    "Ваша реализация должна поддерживать следующие опции:\n",
    "1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n",
    "2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n",
    "3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n",
    "\n",
    "Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jP8yepx8K-hT"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "#import itertools\n",
    "from statistics import median\n",
    "from math import pi\n",
    "from math import cos\n",
    "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
    "        \"\"\"        \n",
    "        Implements pipeline, which consists of PCA decomposition,\n",
    "        Random Fourier Features approximation and linear classification model.\n",
    "        \n",
    "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
    "\n",
    "        new_dim, int: PCA output size.\n",
    "        \n",
    "        use_PCA, bool: whether to include PCA preprocessing.\n",
    "        \n",
    "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
    "        \n",
    "        Feel free to edit this template for your preferences.    \n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.use_PCA = use_PCA\n",
    "        self.new_dim = new_dim\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        ## Этап понижения размерности\n",
    "        if self.use_PCA==True:\n",
    "          self.X = PCA(n_components=self.new_dim).fit_transform(self.X)\n",
    "        ## Этап оценки гиперпараметра сигма\n",
    "        already_used = [[]]\n",
    "        sigma = []\n",
    "        for k in range((10**6)*2):\n",
    "          i = random.randint(0, len(self.X)-1)\n",
    "          j = random.randint(0, len(self.X)-1)\n",
    "          if i!=j and np.any(already_used != [i, j]) and np.any(already_used != [j, i]):\n",
    "        #добавляем комбинацию в массив, чтобы гарантировать отсутствие повторений\n",
    "            already_used.append([i, j])\n",
    "            sigma_i=0\n",
    "            # self.X.shape[1] = n_components => считаем квадрат отклонения по всем признакам между двумя парами объектов\n",
    "            for d in range(self.X.shape[1]):\n",
    "              sigma_i += (self.X[i][d]-self.X[j][d])**2\n",
    "            sigma.append([sigma_i])\n",
    "        #Оцениваем гиперпараметр на полученной выборке\n",
    "        sigma = np.array(sigma)\n",
    "        sigma_median = median(sigma)\n",
    "        \n",
    "        ## Генерируем случайные веса w, b\n",
    "        w = np.random.normal(0, 1/sigma_median, self.n_features*self.new_dim)\n",
    "        w = np.reshape(w, (self.n_features, self.new_dim))\n",
    "        b = np.random.uniform(-pi, pi, self.n_features)\n",
    "         \n",
    "        ## Сформируем новые признаки\n",
    "        phi = []\n",
    "        for i in range(self.n_features):\n",
    "          phi.append([w[i] @ self.X.T + b[i]])\n",
    "        phi = np.array(phi)\n",
    "        phi = np.vectorize(cos)(phi.reshape(len(self.X),-1))\n",
    "\n",
    "        ## Обучаем выбранную модель\n",
    "        if self.classifier=='logreg':\n",
    "          self.clf = LogisticRegression(random_state=0)\n",
    "        else:\n",
    "          self.clf = LinearSVC(random_state=0)\n",
    "        self.clf.fit(phi, self.y)\n",
    "\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Apply pipeline to obtain scores for input data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Apply pipeline to obtain discrete predictions for input data.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "\n",
    "        ## Этап понижения размерности\n",
    "        if self.use_PCA==True:\n",
    "          self.X = PCA(n_components=self.new_dim).fit_transform(self.X)\n",
    "        ## Этап оценки гиперпараметра сигма\n",
    "        already_used = [[]]\n",
    "        sigma = []\n",
    "        for k in range((10**6)*2):\n",
    "          i = random.randint(0, len(self.X)-1)\n",
    "          j = random.randint(0, len(self.X)-1)\n",
    "          if i!=j and np.any(already_used != [i, j]) and np.any(already_used != [j, i]):\n",
    "        #добавляем комбинацию в массив, чтобы гарантировать отсутствие повторений\n",
    "            already_used.append([i, j])\n",
    "            sigma_i=0\n",
    "            # self.X.shape[1] = n_components => считаем квадрат отклонения по всем признакам между двумя парами объектов\n",
    "            for d in range(self.X.shape[1]):\n",
    "              sigma_i += (self.X[i][d]-self.X[j][d])**2\n",
    "            sigma.append([sigma_i])\n",
    "        #Оцениваем гиперпараметр на полученной выборке\n",
    "        sigma = np.array(sigma)\n",
    "        sigma_median = median(sigma)\n",
    "        \n",
    "        ## Генерируем случайные веса w, b\n",
    "        w = np.random.normal(0, 1/sigma_median, self.n_features*self.new_dim)\n",
    "        w = np.reshape(w, (self.n_features, self.new_dim))\n",
    "        b = np.random.uniform(-pi, pi, self.n_features)\n",
    "         \n",
    "        ## Сформируем новые признаки\n",
    "        phi = []\n",
    "        for i in range(self.n_features):\n",
    "          phi.append([w[i] @ self.X.T + b[i]])\n",
    "        phi = np.array(phi)\n",
    "        phi = np.vectorize(cos)(phi.reshape(len(self.X),-1))\n",
    "\n",
    "        self.y_pred = self.clf.predict(phi)\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "yE0_PwHXu8DK",
    "outputId": "4d6ad587-77f9-4262-dce8-e2682eefd67b"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-af09ec52bd21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFFPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7780e3691008>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rff = RFFPipeline()\n",
    "rff.fit(x_train, y_train)\n",
    "y_pred = rff.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYqQUEi-K-hU"
   },
   "source": [
    "__Задание 2. (3 балла)__\n",
    "\n",
    "Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n",
    "\n",
    "Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучаете градиентный бустинг. Используйте одну из реализаций CatBoost/LightGBM/XGBoost, не забудьте подобрать число деревьев и длину шага.\n",
    "\n",
    "Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4fpcxbVns5y"
   },
   "source": [
    "Попробуем обучить классический линейный классификатор SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN8LUlJgK-hV"
   },
   "outputs": [],
   "source": [
    "classic_svm = make_pipeline(StandardScaler(), LinearSVC(random_state=0))\n",
    "classic_svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0qarDvunliM"
   },
   "outputs": [],
   "source": [
    "classic_svm.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ww0RJb3nz8u"
   },
   "source": [
    "Попробуем использовать градиентный бустинг:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "3JnL9Y0onrVY"
   },
   "outputs": [],
   "source": [
    "pca_train = PCA(n_components=50).fit_transform(x_train)\n",
    "pca_test = PCA(n_components=50).fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "ahMrL2jpoEps",
    "outputId": "972671fc-74a8-4fbf-b1de-c6041594dfb1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-d266c42b27fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                         max_depth=1, random_state=0)\n\u001b[1;32m      4\u001b[0m \u001b[0mgb_sklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgb_sklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_sklearn = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                        max_depth=1, random_state=0)\n",
    "gb_sklearn.fit(pca_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAV6GoaOtTtQ",
    "outputId": "57282809-f407-48e4-9bdb-fe258b738aea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4921"
      ]
     },
     "execution_count": 183,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_sklearn.score(pca_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLI8r_ORoxKw",
    "outputId": "96d4e8db-ec79-4892-e643-f414b71045a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 1.2847812\ttotal: 566ms\tremaining: 56.1s\n",
      "1:\tlearn: 1.0355214\ttotal: 1.43s\tremaining: 1m 10s\n",
      "2:\tlearn: 0.9027130\ttotal: 2.52s\tremaining: 1m 21s\n",
      "3:\tlearn: 0.8306724\ttotal: 3.43s\tremaining: 1m 22s\n",
      "4:\tlearn: 0.7698570\ttotal: 4.46s\tremaining: 1m 24s\n",
      "5:\tlearn: 0.7132667\ttotal: 5.45s\tremaining: 1m 25s\n",
      "6:\tlearn: 0.6742825\ttotal: 6.43s\tremaining: 1m 25s\n",
      "7:\tlearn: 0.6520250\ttotal: 7.41s\tremaining: 1m 25s\n",
      "8:\tlearn: 0.6251048\ttotal: 8.36s\tremaining: 1m 24s\n",
      "9:\tlearn: 0.5949967\ttotal: 9.32s\tremaining: 1m 23s\n",
      "10:\tlearn: 0.5741014\ttotal: 10.3s\tremaining: 1m 23s\n",
      "11:\tlearn: 0.5532143\ttotal: 11.3s\tremaining: 1m 23s\n",
      "12:\tlearn: 0.5370275\ttotal: 12.4s\tremaining: 1m 22s\n",
      "13:\tlearn: 0.5233742\ttotal: 13.4s\tremaining: 1m 22s\n",
      "14:\tlearn: 0.5113189\ttotal: 14.5s\tremaining: 1m 22s\n",
      "15:\tlearn: 0.5012152\ttotal: 15.4s\tremaining: 1m 20s\n",
      "16:\tlearn: 0.4934987\ttotal: 15.9s\tremaining: 1m 17s\n",
      "17:\tlearn: 0.4814401\ttotal: 16.5s\tremaining: 1m 15s\n",
      "18:\tlearn: 0.4725334\ttotal: 17s\tremaining: 1m 12s\n",
      "19:\tlearn: 0.4644596\ttotal: 17.5s\tremaining: 1m 10s\n",
      "20:\tlearn: 0.4572515\ttotal: 18s\tremaining: 1m 7s\n",
      "21:\tlearn: 0.4512535\ttotal: 18.5s\tremaining: 1m 5s\n",
      "22:\tlearn: 0.4445060\ttotal: 19.1s\tremaining: 1m 3s\n",
      "23:\tlearn: 0.4399913\ttotal: 19.6s\tremaining: 1m 1s\n",
      "24:\tlearn: 0.4360575\ttotal: 20.1s\tremaining: 1m\n",
      "25:\tlearn: 0.4322391\ttotal: 20.6s\tremaining: 58.5s\n",
      "26:\tlearn: 0.4270330\ttotal: 21.1s\tremaining: 56.9s\n",
      "27:\tlearn: 0.4237191\ttotal: 21.5s\tremaining: 55.4s\n",
      "28:\tlearn: 0.4190553\ttotal: 22s\tremaining: 54s\n",
      "29:\tlearn: 0.4157649\ttotal: 22.6s\tremaining: 52.6s\n",
      "30:\tlearn: 0.4111244\ttotal: 23.1s\tremaining: 51.3s\n",
      "31:\tlearn: 0.4062871\ttotal: 23.6s\tremaining: 50.1s\n",
      "32:\tlearn: 0.4035134\ttotal: 24.1s\tremaining: 48.9s\n",
      "33:\tlearn: 0.4012361\ttotal: 24.6s\tremaining: 47.7s\n",
      "34:\tlearn: 0.3984276\ttotal: 25s\tremaining: 46.5s\n",
      "35:\tlearn: 0.3954611\ttotal: 25.6s\tremaining: 45.5s\n",
      "36:\tlearn: 0.3935794\ttotal: 26s\tremaining: 44.3s\n",
      "37:\tlearn: 0.3900061\ttotal: 26.6s\tremaining: 43.3s\n",
      "38:\tlearn: 0.3866613\ttotal: 27s\tremaining: 42.3s\n",
      "39:\tlearn: 0.3847618\ttotal: 27.5s\tremaining: 41.3s\n",
      "40:\tlearn: 0.3797854\ttotal: 28s\tremaining: 40.4s\n",
      "41:\tlearn: 0.3766713\ttotal: 28.6s\tremaining: 39.4s\n",
      "42:\tlearn: 0.3741137\ttotal: 29.1s\tremaining: 38.5s\n",
      "43:\tlearn: 0.3716477\ttotal: 29.6s\tremaining: 37.6s\n",
      "44:\tlearn: 0.3696633\ttotal: 30s\tremaining: 36.7s\n",
      "45:\tlearn: 0.3678724\ttotal: 30.5s\tremaining: 35.8s\n",
      "46:\tlearn: 0.3666440\ttotal: 31s\tremaining: 34.9s\n",
      "47:\tlearn: 0.3650369\ttotal: 31.5s\tremaining: 34.1s\n",
      "48:\tlearn: 0.3630961\ttotal: 32s\tremaining: 33.3s\n",
      "49:\tlearn: 0.3610072\ttotal: 32.5s\tremaining: 32.5s\n",
      "50:\tlearn: 0.3595655\ttotal: 32.9s\tremaining: 31.6s\n",
      "51:\tlearn: 0.3569419\ttotal: 33.4s\tremaining: 30.9s\n",
      "52:\tlearn: 0.3550285\ttotal: 33.9s\tremaining: 30.1s\n",
      "53:\tlearn: 0.3534419\ttotal: 34.4s\tremaining: 29.3s\n",
      "54:\tlearn: 0.3502217\ttotal: 34.9s\tremaining: 28.6s\n",
      "55:\tlearn: 0.3484755\ttotal: 35.4s\tremaining: 27.8s\n",
      "56:\tlearn: 0.3470553\ttotal: 35.9s\tremaining: 27.1s\n",
      "57:\tlearn: 0.3451575\ttotal: 36.4s\tremaining: 26.4s\n",
      "58:\tlearn: 0.3434888\ttotal: 36.9s\tremaining: 25.6s\n",
      "59:\tlearn: 0.3411925\ttotal: 37.4s\tremaining: 24.9s\n",
      "60:\tlearn: 0.3394197\ttotal: 37.9s\tremaining: 24.2s\n",
      "61:\tlearn: 0.3380158\ttotal: 38.4s\tremaining: 23.5s\n",
      "62:\tlearn: 0.3367576\ttotal: 38.9s\tremaining: 22.8s\n",
      "63:\tlearn: 0.3357062\ttotal: 39.3s\tremaining: 22.1s\n",
      "64:\tlearn: 0.3337945\ttotal: 39.8s\tremaining: 21.5s\n",
      "65:\tlearn: 0.3324613\ttotal: 40.4s\tremaining: 20.8s\n",
      "66:\tlearn: 0.3311711\ttotal: 40.9s\tremaining: 20.1s\n",
      "67:\tlearn: 0.3301902\ttotal: 41.4s\tremaining: 19.5s\n",
      "68:\tlearn: 0.3278341\ttotal: 41.9s\tremaining: 18.8s\n",
      "69:\tlearn: 0.3267832\ttotal: 42.3s\tremaining: 18.1s\n",
      "70:\tlearn: 0.3240843\ttotal: 42.8s\tremaining: 17.5s\n",
      "71:\tlearn: 0.3226250\ttotal: 43.3s\tremaining: 16.9s\n",
      "72:\tlearn: 0.3192454\ttotal: 43.9s\tremaining: 16.2s\n",
      "73:\tlearn: 0.3175697\ttotal: 44.3s\tremaining: 15.6s\n",
      "74:\tlearn: 0.3158145\ttotal: 44.8s\tremaining: 14.9s\n",
      "75:\tlearn: 0.3147509\ttotal: 45.3s\tremaining: 14.3s\n",
      "76:\tlearn: 0.3128242\ttotal: 45.8s\tremaining: 13.7s\n",
      "77:\tlearn: 0.3114300\ttotal: 46.3s\tremaining: 13.1s\n",
      "78:\tlearn: 0.3103919\ttotal: 46.9s\tremaining: 12.5s\n",
      "79:\tlearn: 0.3085521\ttotal: 47.4s\tremaining: 11.8s\n",
      "80:\tlearn: 0.3077037\ttotal: 47.8s\tremaining: 11.2s\n",
      "81:\tlearn: 0.3073001\ttotal: 48.3s\tremaining: 10.6s\n",
      "82:\tlearn: 0.3057672\ttotal: 48.8s\tremaining: 10s\n",
      "83:\tlearn: 0.3051078\ttotal: 49.3s\tremaining: 9.39s\n",
      "84:\tlearn: 0.3033858\ttotal: 49.8s\tremaining: 8.79s\n",
      "85:\tlearn: 0.3023565\ttotal: 50.3s\tremaining: 8.18s\n",
      "86:\tlearn: 0.3009049\ttotal: 50.8s\tremaining: 7.59s\n",
      "87:\tlearn: 0.3001813\ttotal: 51.2s\tremaining: 6.99s\n",
      "88:\tlearn: 0.2990511\ttotal: 51.8s\tremaining: 6.4s\n",
      "89:\tlearn: 0.2986485\ttotal: 52.2s\tremaining: 5.8s\n",
      "90:\tlearn: 0.2961382\ttotal: 52.7s\tremaining: 5.22s\n",
      "91:\tlearn: 0.2951944\ttotal: 53.2s\tremaining: 4.63s\n",
      "92:\tlearn: 0.2942314\ttotal: 53.7s\tremaining: 4.04s\n",
      "93:\tlearn: 0.2933346\ttotal: 54.2s\tremaining: 3.46s\n",
      "94:\tlearn: 0.2921312\ttotal: 54.7s\tremaining: 2.88s\n",
      "95:\tlearn: 0.2909518\ttotal: 55.2s\tremaining: 2.3s\n",
      "96:\tlearn: 0.2894560\ttotal: 55.7s\tremaining: 1.72s\n",
      "97:\tlearn: 0.2885372\ttotal: 56.2s\tremaining: 1.15s\n",
      "98:\tlearn: 0.2876322\ttotal: 56.7s\tremaining: 572ms\n",
      "99:\tlearn: 0.2868636\ttotal: 57.2s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4917"
      ]
     },
     "execution_count": 186,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "gb_catboost = CatBoostClassifier(iterations=100)\n",
    "gb_catboost.fit(pca_train, y_train)\n",
    "gb_catboost.score(pca_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HQunNCbt-_a"
   },
   "source": [
    "Учитывая то, что оптимально реализованный RFF классификатор должен выдавать качество 0.84, а градиентный бустинг без кросс-валидации выдаёт 0.49, можем сделать вывод, что RFF позволяет на порядок увеличить качество предсказания классов и добиться неплохих результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6umjhWuK-hV"
   },
   "source": [
    "__Задание 3. (2 балла)__\n",
    "\n",
    "Проведите эксперименты:\n",
    "1. Помогает ли предварительное понижение размерности с помощью PCA? \n",
    "2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n",
    "3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EC-HnOLtqVyx"
   },
   "source": [
    "1. Роль метода главных компонент в обучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2QIHIMbK-hW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rff = RFFPipeline(use_PCA=True)\n",
    "rff.fit(x_train, y_train)\n",
    "y_pred = rff.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKNpB06CsZwz"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rff = RFFPipeline(use_PCA=False)\n",
    "rff.fit(x_train, y_train)\n",
    "y_pred = rff.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRvMIKRGsuLJ"
   },
   "source": [
    "Сделаем вывод: как и предполагалось, понижение размерности ускоряет обучение, но при этом незначительно ухудшает качество модели.\n",
    "\n",
    "Так, если первоочередной задачей для исследования является получение хоть какого-то правдоподобного результата как можно скорее, то следует понизить размерность, тогда как в случае попытки пробить наибольшую точность следует отказаться от понижения размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar9dPh80qb_7"
   },
   "source": [
    "2. Зависимость между качеством и n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhmNfcxTqggN"
   },
   "outputs": [],
   "source": [
    "#тут должны быть какие-нибудь красивые графики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwI0p23Cqg4B"
   },
   "source": [
    "3. Разница между логистической регрессией и SVM\n",
    "\n",
    "Вообще говоря, очевидно, что при использовании SVM время обучения увеличивается в разы, что не очень приятно. С точки зрения качества, на моей поломанной модели оно оказывается примерно одинаковым, поэтому можно предположить, что разницы нет\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaSxs8iTqmVR"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rff = RFFPipeline(classifier='logreg')\n",
    "rff.fit(x_train, y_train)\n",
    "y_pred = rff.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O5Jt7kRsQQt"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rff = RFFPipeline(classifier='svm')\n",
    "rff.fit(x_train, y_train)\n",
    "y_pred = rff.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJqXVuasK-hW"
   },
   "source": [
    "### Бонус"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVDWHCdrK-hX"
   },
   "source": [
    "__Задание 4. (Максимум 2 балла)__\n",
    "\n",
    "Как вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет). Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSxvGI9iK-hX"
   },
   "outputs": [],
   "source": [
    "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pc7-1jmK-hY"
   },
   "source": [
    "__Задание 5. (Максимум 2 балла)__\n",
    "\n",
    "Поэкспериментируйте с функциями для вычисления новых случайных признаков. Не обязательно использовать косинус от скалярного произведения — можно брать знак от него, хэш и т.д. Придумайте побольше вариантов для генерации признаков и проверьте, не получается ли с их помощью добиваться более высокого качества. Также можете попробовать другой классификатор поверх случайных признаков, сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWj-O2vjK-hY"
   },
   "outputs": [],
   "source": [
    "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CJqXVuasK-hW"
   ],
   "name": "homework-practice-08-random-features-ZolotarevAnton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
