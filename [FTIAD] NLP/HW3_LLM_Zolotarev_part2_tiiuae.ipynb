{"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Practice: Large Language Models and Their Implications\n<!-- ![img](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4470ce74-e595-4750-92a5-5f21f040df6d_577x432.jpeg) -->\n![img](https://i.imgur.com/QGYa2J8.jpeg)\n\nIn this notebook, you're gonna play with some of the largest language models on the Internet.\n\n_Based on works of: Tim Dettmers, Ruslan Svirschevsky, Artem Chumachenko, Younes Belkada, Felix Marty, Yulian Gilyazev, Gosha Zolotov, Andrey Ishutin,  Elena Volf, Artemiy Vishnyakov, Svetlana Shirokovskih.","metadata":{"id":"aSWEcS2XKgzi"}},{"cell_type":"markdown","source":"### Part 1: prompt engineering (4 points total)\n\nIn the assignment, we'll use public APIs that host the 100B+ models for inference. Your task is to prompt-engineer the model into solving a few tasks for you.\n\n\n__Which API?__ You are free to use any publicly available API for general LM -- as long as it's __not a chat assistant__. So, gpt 3.5 is fine, but chatGPT is not. Here's a few options:\n\n- BLOOM API - [bigscience/bloom](https://huggingface.co/bigscience/bloom) (on the right; recommended)\n- OpenAI API (via VPN) - [openai.com/api](https://openai.com/api/)\n- AI21 Jurrasic API - [ai21.com](https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1)\n\nThese APIs may require you to create a (free) account on their platform. Please note that some APIs also have paid subscriptions. __You do not need to pay them__, this assignment was designed to be solved using free-tier subscriptions. If no APIs work for you, you can also solve these tasks with the 6.7B model that you will find later in this notebook - but this will make the tasks somewhat harder.\n\n__Quests:__ you will need to solve 4 problems. For each one, please attach a short __description__ of your solution and a __screenshot__ from the API you use. _[If you use python APIs, show your python code with outputs]_\n\n__Example:__ Tony is talking to Darth Vader ([BLOOM API](https://huggingface.co/bigscience/bloom)). Black text is written manually, blue text is generated.\n<hr>\n\n![img](https://i.imgur.com/a1QhKF7.png)\n<hr>\n\n__It is fine to roll back a few times,__ e.g. in the example above, the model first generated Vader lines twice in a row, and we rolled that back. However, if you need more than 1-2 rollbacks per session, you should probably try a different prompt.","metadata":{"id":"1jYrxHF8Kgzl"}},{"cell_type":"markdown","source":"__Task 1 (1 pt):__ arange a conversation between any two of the following:\n\n- a celebrity or politician of your choice\n- any fictional character (except Darth Vader)\n- yourself\n\nCompare two setups: a) you prompt with character names only b) you supply additional information (see example).","metadata":{"id":"CHIvIFjsKgzm"}},{"cell_type":"markdown","source":"Здесь и далее для решения задачи используется AI21\n\nВ этом задании будем симулировать диалог между Леброном Джеймсом (один из величайших баскетболистов в истории НБА) и Wraith King (героем из игры Dota 2).\n1. В первом случае задаётся промпт \"Lebron James talking to Wraith King\"\n\n\n\n> LeBron James and Wraith King are both prominent figures in the basketball and gaming communities, respectively. While it is not clear what exactly LeBron and Wraith King have discussed, it is likely that they share a passion for their respective fields and may have discussed their experiences and strategies for success. LeBron is a well-known professional basketball player and entrepreneur, while Wraith King is a popular Dota 2 player and YouTube personality.\n\n[Скриншот по ссылке](https://drive.google.com/file/d/1gjGVqvCbQrj4X_eHZ1dQAtZrs6dzjOj3/view?usp=drive_link)\n\nПолучился какой-то поломанный промпт, чуть модифицируем до \"Simulate dialogue between Lebron James and Wraith King\"\n\n\n\n> Lebron James: Hey Wraith King, what are you doing here at the Lakers practice facility?\n\n> Wraith King: Hey Lebron, I'm just here to watch you guys practice.\n\n> Lebron: That's cool, but we're in the middle of a scrimmage.\n\n> Wraith King: That's okay, I'll just watch from the sidelines.\n\n> Lebron: All right, but don't get in the way.\n\n> Wraith King: I won't, I promise.\n\n> Lebron: Good. We'll finish up in a bit, then you can ask me any questions you have.\n\n> Wraith King: Thanks Lebron, I'll see you then.\n\n[Скриншот по ссылке](https://drive.google.com/file/d/1CEOYsZ5EJgnc72knIft-iZE5cf9O2i6D/view?usp=drive_link)\n\n\n2. Во втором случае задаётся промпт \"Lebron James (basketball player, one of the best in history) talking to Wraith King (hero from Dota 2). They are discussing the problem of the immortality\"\n\n\n> Wraith King: \"LeBron, tell me, have you considered the implications of immortality?\"\n\n> Lebron James: \"Immortal? No, I haven't. But it's definitely something I've been thinking about a lot lately.\"\n\n> Wraith King: \"Well, let me tell you, it's not all it's cracked up to be. I've been around for thousands of years, and let me tell you, it gets boring after a while. You can only do so many things before you've seen it all.\"\n\n> Lebron: \"Wow, that's surprising. I would have thought that immortality would be the ultimate gift.\"\n\n> Wraith King: \"It's not. Eventually, you run out of things to do and people to talk to. It's like being trapped in a never-ending cycle of boredom.\"\n\n> Lebron: \"That's really sad. I guess immortality isn't everything it's cracked up to be.\"\n\n> Wraith King: \"Exactly. That's why it's important to make the most of what you have, while you still have it.\"\n\n[Скриншот по ссылке](https://drive.google.com/file/d/1b_Zx9TqOa7Kub5vdR-uRXV-azrBUcd_1/view?usp=drive_link)","metadata":{"id":"0OJ_xHQBIjD_"}},{"cell_type":"markdown","source":"Видим, что модель хорошо генерирует диалоги даже при минимуме информации, поскольку, видимо, обучена в том числе на достаточной информации о том, кем являются данные персонажи","metadata":{"id":"rYsk1hqNLelS"}},{"cell_type":"markdown","source":"__Please choose task 2a or 2b (1pt)__ depending on your model (you can do both, but you will be awarded points for one of these two tasks).\n\n__Task 2a: (for BLOOM or other multilingual model)__ zero-shot translation. Take the first verse of [Edgar Allan Poe's \"Raven\"](https://www.poetryfoundation.org/poems/48860/the-raven) and __translate it into French.__ (You are free to use any other text of at least the same size)\n\nOriginal text: ```\nOnce upon a midnight dreary, while I pondered, weak and weary,\nOver many a quaint and curious volume of forgotten lore—\n    While I nodded, nearly napping, suddenly there came a tapping,\nAs of some one gently rapping, rapping at my chamber door.\n“’Tis some visitor,” I muttered, “tapping at my chamber door—\n            Only this and nothing more.”\n```\n\nVerify your translation by converting french back into english using a public machine translation service.\n\n__Task 2b: (non-BLOOM):__ toxicity classification for [SetFit/toxic_conversations](https://huggingface.co/datasets/SetFit/toxic_conversations). Make the model solve binary classification (toxic vs not toxic) in the few shot mode. For few-shot examples, use 2-3 toxic and 2-3 non-toxic non-toxic examples. Measure accuracy on at least 25 samples. You may need to try several different prompts before you find the one that works.","metadata":{"id":"Z6Bc13ueKgzn"}},{"cell_type":"markdown","source":"[Using this mode](https://studio.ai21.com/playground/complete/single-input-1-output?presetId=classify_topics), I've performed few-shot mode to solve binary classification task, using next prompt input:\n\nClassify the following news article into one of the following topics:\n1. toxic\n2. no toxic\nText:\nThat was perhaps the most hackneyed vomit, to say NOTHING of politically ignorant pap that I have read today. The ONLY way that the \"billionaires and CEO's\" can create monopolies and manipulate the market is through the use of governmental power. In a TRUE free market that operates without the corrupting influence of government, companies like Google and Microsoft would have been challenged by rivals that would possibly been able to wrench market share from these two behemoths before they became the \"monopolies\" that you decry. However, both companies paid for, and used, governmental power to crush their enemies in court, resulting in what you have today. Wake up, you desperately need to educate yourself and stop listening to retread communists and socialists. Oh, and stop smoking crack.\nThe topic of this article is:\ntoxic\n\n===\n\nClassify the following news article into one of the following topics:\n1. toxic\n2. no toxic\nText:\nCan she not be dismissed for incompetence and doing damage - through stupid recommendations on the economic front which has harmed the very public she is supposed to protect? She is just trying to get money for government coffers so that the looting can continue.\nThe topic of this article is:\ntoxic\n\n===\n\nClassify the following news article into one of the following topics:\n1. toxic\n2. no toxic\nText:\nRobben Island won't be big enough, to hold all these f.....g. scoundrels, who have not only betrayed their own faithful people, but also, our entire country. The mess they have created will take many, many years, to sort. They shouldn't be voted out. They should be shoved, out.\nThe topic of this article is:\ntoxic\n\n===\n\nClassify the following news article into one of the following topics:\n1. toxic\n2. no toxic\nText:\ntheres not enough going on around here for air service none want to waste there time on this town\nThe topic of this article is:\nno toxic\n\n===\n\nClassify the following news article into one of the following topics:\n1. toxic\n2. no toxic\nText:\nHer name is still floating? Might I suggest flushing twice?\nThe topic of this article is:\nno toxic\n\n===\n\nClassify the following news article into one of the following topics:\n1. toxic\n2. no toxic\nText:\nYES CANADA IS BACK!! The national embarrassment is Trudope, spend, spend spend . \" the budget will balances it self\" Just in one year little potato has taken more vacations than HARPER did in 10 years !!! We are talking about a man who has a home on Harrington lake, each trip is cost the tax payers a minimum of $49 thousand dollars, RCMP, FLIGHT CREW, HOTELS etc. YES 10 IN THE LAST YEAR!!! According to the CBC Trudope was reimbursed for 9 vacations prior to the Bahamas fiasco. This man is simple not invested into his job. I got seven to memory: Saint Kits Whistler Fogo island Fino BC Carnival Quebec Japan Anniversary- extra day New York- extra day Bahamas Junior is putting Obama to shame, yes the middle class - what joke Harper we miss you!!!\nThe topic of this article is:\nno toxic","metadata":{"id":"HgMKjJjY2A0H"}},{"cell_type":"code","source":"# <your code OR writeup with screenshots>","metadata":{"id":"GygPXoyKKgzo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"У меня хватило сил сделать только 3 скриншота, остальное, надеюсь, вы поверите, что я честно прогнал через модель\n\n0. screen / true / prediction\n1. [input 1](https://drive.google.com/file/d/1LlW_xgQ7MCmUP1Yo-Py6871Xx-0IAOID/view?usp=drive_link) - no toxic / no toxic\n2. [input 2](https://drive.google.com/file/d/1QIufidnbAdBf04EbxjQrBmSbnBfYngDy/view?usp=drive_link) - no toxic / toxic\n3. [input 3](https://drive.google.com/file/d/1llcRQkpYuHmHHq7eKuRpmoK0d1kKwpTO/view?usp=drive_link) - no toxic / no toxic\n4. Asif, this is way, way, way, way worse than the Broederbond. This is grand theft for the sole benefit of a criminal cartel. - no toxic / toxic\n5. Oh yeah, thanks for the reminder. There's just so many it's hard to keep track of. Just imagining her being President gives me the shakes. - no toxic / toxic\n6. Is this where, \"two wrongs don't make a right,\" comes in?\t- no toxic / no toxic\n7. It would have serious repercussions alright, for both the US and China. Why China won't cooperate on reigning in Kim is baffling. - no toxic / toxic\n8. Gardener is ONLY for Gardener. - no toxic / no toxic\n9. `Russian bot or Trump apologist, same traitorous thing. - no toxic / toxic\n10. Haaaa ha haaaa ha oh my - What a classic post!!!!\t - no toxic / no toxic\n11. It is based on science and it is a fact. There is no god.\t - no toxic / toxic\n12. Good idea. I'll try to find out. - no toxic / no toxic\n13. I look forward to the day that the Prime Minister represents ALL Canadians, not just his activist causes. The next election cannot come too soon!\t - no toxic / no toxic\n14. Preposterous nonsense unsupported by any facts. More hate dreaming by a Hater. You do realize of course that Harper was our PM in 2015 and Trump had already started his hate campaign.. - no toxic / no toxic\n15. #MAGA is quickly losing ground. #FAKE NEWS and #HILLARYLOST is their new cry. Lol. #BIGGESTWITCHHUNT, #LETSBERUSSIAFRIEND\t\t - no toxic / toxic\n16. I rest my case\t - no toxic / no toxic\n17. Yup, but most voters don't. Elections are annoying, when they go against you, aren't they.\t - no toxic / no toxic\n18. Congratulations on a well-deserved win.\t - no toxic / no toxic\n19. SB 21 results in our dividends being redirected to the Oil and Gas industry.\t- no toxic / toxic\n20. But he is, Blanche! He is a racist!\t - toxic / toxic\n21. Just grow some balls and protect what's your white trash Valley residents. Take off your belt and discipline your children when they don't listen. Take away their lifted Chevy pickups you bought them on their 16th birthdays. I swear you millennial parents are the saddest things I've ever seen.\t - toxic / toxic\n22. The stupidity and arrogance of U.S. foreign policy is indeed exceptional.\t - toxic / toxic\n23. Ellen Weintraub can go to hell, and the sooner she is packed off to that particular place, the better. - toxic / toxic\n24. John A ,the dirty bugger even hid the whiskey from the Irish rail workers too\t - toxic / toxic\n25. I don't think they blocked it because he's black. I do think they blocked it because he's a Dem.\t - toxic / no toxic","metadata":{"id":"IidGgvl83DA_"}},{"cell_type":"markdown","source":"accuracy для 25 примеров составляет $16 / 25 = 64\\%$, при этом в некоторых местах кажется, что LLM разметила предложение лучше, чем это сделано в исходном датасете (например, в строчках 10, 15)","metadata":{"id":"lRMCf1p99YNY"}},{"cell_type":"code","source":"","metadata":{"id":"1zm2R_8F9WQg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n__Task 3 (1pt):__ create a prompt and few-shot examples tha make the model __change the gender pronouns__ of the main actor in a given sentence in any direction of your choice. E.g. the doctor took off _his_ mask <-> the doctor took of _her_ mask.\n","metadata":{"id":"iaYweoPsKgzo"}},{"cell_type":"markdown","source":"* Здесь всё хорошо\n\n**Prompt:** Change the gender pronouns of the main actor in a given sentence.\n\"AI-developer is thick and tired of his tasks\"\n\n**Response:** \"AI-developer is thick and tired of her tasks\"\n\n* А вот здесь модель чуть-чуть ошиблась, должна была выдать \"her\" вместо \"their\" и зачем-то скопировала исходный запрос.\n\n**Prompt:** Change the gender pronouns of the main actor in a given sentence.\nSir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who,\nfor his own amusement, never took up any book but the Baronetage\n\n**Response:** Can you change the gender pronouns for this sentence\nMadam Walter Elliot, of Kellynch Hall, in Somersetshire, was a woman who,\nfor their own amusement, never took up any book but the Baronetage\n\n\n* Здесь модель сумела даже заменить имя на вполне релевантное для последующего повествования.\n\n**Prompt:** Change the gender pronouns of the main actor in a given sentence.\nIt was a Sunday evening in October, and in common with many other young ladies of her class, Katharine Hilbery was pouring out tea.\n\n**Response:** It was a Sunday evening in October, and in common with many other young men of his class, Anthony Strafford was pouring out tea.","metadata":{"id":"SqA-Irq9M2t5"}},{"cell_type":"markdown","source":"Кажется, что few-shot example предполагает обучение модели на паре примеров и решение для последнего предложения, поэтому чуть переформатируем формат запроса в модель\n\n1. [Пример 1](https://drive.google.com/file/d/1wUZNINQAu0G7ERwE7H-S-xDFKS0znZSA/view?usp=drive_link) - на примере из реальной литературы выдало какой-то ужас\n2. [Пример 2](https://drive.google.com/file/d/1109V4SAyk5N-SNSCWBj-GOqjDxja5nlL/view?usp=drive_link) - на синтетическом примере только с местоимениями всё норм\n3. [Пример 3](https://drive.google.com/file/d/1zF0PA-dN_LJUzNt3eTx-pTb1g3-h9Q6Y/view?usp=drive_link) - на доработанном синтетическом примере с именем тоже работает","metadata":{"id":"3mORK8MppHlb"}},{"cell_type":"markdown","source":"Что писал в промпте\n\nСhange the gender pronouns of the main actor in a given sentence:\n\n* Sentence: \"AI-developer is thick and tired of his tasks.\"\n* Response: \"AI-developer is thick and tired of her tasks.\"\n\n==\n\nСhange the gender pronouns of the main actor in a given sentence:\n\n* Sentence: \"Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who, for his own amusement, never took up any book but the Baronetage.\"\n* Response: \"Madam Alice Elliot, of Kellynch Hall, in Somersetshire, was a woman who, for her own amusement, never took up any book but the Baronetage.\"\n\n==\n\nСhange the gender pronouns of the main actor in a given sentence:\n\n* Sentence: \"He was proud of his son.\"\n* Response: \"She was proud of her son.\"\n\n==\n\nСhange the gender pronouns of the main actor in a given sentence:\n\n* Sentence: \"It was a Sunday evening in October, and in common with many other young ladies of her class, Katharine Hilbery was pouring out tea.\"\n* Response:","metadata":{"id":"dr8VZnwzwrQ9"}},{"cell_type":"markdown","source":"__Task 4 (1pt):__ write a prompt and supply examples such that the model would __convert imperial units to metric units__ (miles -> kilometers; mph -> kph). More specifically, the model should rewrite a given sentence and replace all imperial units with their metric equivalents. After it works with basic distances and speed, try to find complicated examples where it does *not* work.\n\nPlease note that 1 mile is not equal to 1 km :)","metadata":{"id":"bbNrRmgMKgzq"}},{"cell_type":"markdown","source":"Промпт, который загонял в модель:\n\nConvert imperial units to metric units:\n\n- Sentence: \"He walked 10 feet and fell\"\n- Response: \"He walked 3.048 meters and fell\"\n\n==\n\nConvert imperial units to metric units:\n\n- Sentence: \"Sir Walter Elliot weighed 200 pounds\"\n- Response: \"Sir Walter Elliot weighed 90.72 kilograms\"\n\n==\n\nСhange the gender pronouns of the main actor in a given sentence:\n\n- Sentence: \"Michael Phelps drank 24 galons of water\"\n- Response: \"Michael Phelps drank 90.84 liters of water\"\n\n==\n\nСhange the gender pronouns of the main actor in a given sentence:\n\n- Sentence:","metadata":{"id":"YW_Wf3_X3t-W"}},{"cell_type":"markdown","source":"1. [Пример  на градусы по Фаренгейту, где ломается](https://drive.google.com/file/d/1p7jpZFKOzFLudlH7ZCKAyXhzm4nA9SYB/view?usp=drive_link)\n2. [Пример на мили, где j2-mid и j2-ultra дают одинаковый ответ](https://drive.google.com/file/d/1N1ebUhhQwERDzE0pbKlhI3SO8Mvh4oBP/view?usp=drive_link)\n3. [Пример на футы, где j2-mid и j2-ultra дают одинаковый ответ](https://drive.google.com/file/d/10-YVm9DyWe6xScaS2j4mHG4meH4GFtgI/view?usp=drive_link)\n4. [Пример на фунты, где j2-mid и j2-ultra дают одинаковый ответ](https://drive.google.com/file/d/1nqdI64UPgjqJTCe8qUbRFEP8NhHAXOTJ/view?usp=drive_link)","metadata":{"id":"Gku-zbCH4eB_"}},{"cell_type":"code","source":"# <your code OR writeup with screenshots>","metadata":{"id":"UBxMVGHNKgzr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 2: local inference\n\nNow, let's try and load the strongest model that can fit a typical Colab GPU (T4 with 16 GB as of spring 2023).\n\nOur best candidates are the smaller versions of the best performing open source models:\n- 7 Bn parameters version of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) - best for spring 2023, released by Facebook\n- 7 Bn parameters version of [Falcon](https://falconllm.tii.ae) - close competitor to Llama, released in May 2023 by [Technology Innovation Institute of UAE](https://www.tii.ae).\n- 6.7 Bn parameters version of [OPT](https://arxiv.org/abs/2205.01068) - top choice in this nomination in 2022, released by Facebook.\n\nBeware: while these models are smaller than the ones in API, they're still over 60x larger than the BERT we played with last time. The code below will *just barely* fit into memory, so make sure you don't have anything else loaded. Sometimes you may need to restart runtime for the code to work.\n\nIt's a good time to restart your kernel and switch to GPU! (Runtime -> Change runtime type)\n<center><img src=\"https://i.imgur.com/OOfDYzJ.png\" width=240px></center>","metadata":{"id":"ZKw-mjuRKgzs"}},{"cell_type":"code","source":"%pip install --quiet bitsandbytes==0.41.1 transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 auto-gptq==0.4.2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nimport bitsandbytes as bnb\nfrom tqdm.auto import tqdm, trange\nassert torch.cuda.is_available(), \"you need cuda for this part\"\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"7xeRF_hSKgzs","execution":{"iopub.status.busy":"2023-12-04T02:12:43.360099Z","iopub.execute_input":"2023-12-04T02:12:43.360510Z","iopub.status.idle":"2023-12-04T02:12:55.760945Z","shell.execute_reply.started":"2023-12-04T02:12:43.360477Z","shell.execute_reply":"2023-12-04T02:12:55.759610Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 'TheBloke/Llama-2-13B-GPTQ'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    offload_state_dict=True\n)","metadata":{"id":"VMzFwx29Kgzu","outputId":"85c4c9b3-683a-4a85-fb60-e0253999168c","execution":{"iopub.status.busy":"2023-12-04T00:51:33.541728Z","iopub.execute_input":"2023-12-04T00:51:33.542152Z","iopub.status.idle":"2023-12-04T00:52:36.266738Z","shell.execute_reply.started":"2023-12-04T00:51:33.542122Z","shell.execute_reply":"2023-12-04T00:52:36.265624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text generation\n\n**Comparison of strategies for language model text generation:**\n\n| Strategy | Description | Pros & Cons |\n| --- | --- | --- |\n| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n\nDocumentation references:\n- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)","metadata":{"id":"5k2zCgAhG7l5"}},{"cell_type":"markdown","source":"### Generation with HuggingFace","metadata":{"id":"GWm6KDSzMiAf"}},{"cell_type":"code","source":"prompt = 'The first discovered martian lifeform looks like'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\nprint(\"Input batch (encoded):\", batch)\n\noutput_tokens = model.generate(**batch, max_new_tokens=64, do_sample=True, temperature=0.8)\n# greedy inference:                                        do_sample=False)\n# beam search for highest probability:                     num_beams=4)\n\nprint(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))","metadata":{"id":"gGfyeM-vdq5o","tags":[],"outputId":"2909e0b0-34ec-4dca-b796-7a9bd38fe89c","execution":{"iopub.status.busy":"2023-12-04T03:36:13.038473Z","iopub.execute_input":"2023-12-04T03:36:13.039490Z","iopub.status.idle":"2023-12-04T03:36:19.103680Z","shell.execute_reply.started":"2023-12-04T03:36:13.039447Z","shell.execute_reply":"2023-12-04T03:36:19.102663Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Input batch (encoded): {'input_ids': tensor([[  487,   772,  6524, 13346,   715,  1063,   590,  2896,   606]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n\nOutput: The first discovered martian lifeform looks like a cross between a piece of spaghetti and a slinky.\nThe creature was discovered by NASA’s Curiosity rover in a jar of Martian dust that’s been stored, in a refrigerator, since 2012.\nThe Curiosity rover landed on Mars in 2012, and has been collecting\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Low-level code for text generation","metadata":{"id":"P17ehC1sKgzx"}},{"cell_type":"code","source":"prompt = \"Moscow is the capital of\"\n# prompt = \"Skippy, a young android, likes to dream about electric\"\n\nprint(prompt, '\\n')\n\nvoc = tokenizer.get_vocab()\nvoc_rev = {v:k for k, v in voc.items()}  # reverse vocab for decode\n\nfor i in range(10):\n    inputs = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n    logits = model.forward(**inputs).logits[0, -1, :]\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    next_token_id = torch.multinomial(probs.flatten(), num_samples=1)\n\n    next_token = tokenizer.decode(next_token_id)\n    prompt += next_token\n\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n    top_tokens = sorted_indices[:5]\n    print(f\"Step #{i} candidates:\")\n    for t, p in zip (top_tokens, sorted_probs):\n        t = voc_rev[t.item()]\n        print(f\"{t:<10}: {p:.4f} \")\n\n    print(f'\\nChosen token: {next_token}', end='\\n\\n', flush=True)","metadata":{"id":"LZJvOMbmG7l8","tags":[],"outputId":"447474de-8bfc-4bb0-e836-0de4681528f2","execution":{"iopub.status.busy":"2023-12-04T03:36:23.919353Z","iopub.execute_input":"2023-12-04T03:36:23.919698Z","iopub.status.idle":"2023-12-04T03:36:26.738808Z","shell.execute_reply.started":"2023-12-04T03:36:23.919671Z","shell.execute_reply":"2023-12-04T03:36:26.737958Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Moscow is the capital of \n\nStep #0 candidates:\nĠRussia   : 0.7266 \nĠthe      : 0.1871 \nĠRussian  : 0.0200 \nĠa        : 0.0054 \nĠboth     : 0.0052 \n\nChosen token:  Russia\n\nStep #1 candidates:\nĠand      : 0.3962 \n.         : 0.2534 \n,         : 0.2505 \nĠas       : 0.0162 \nĠ         : 0.0130 \n\nChosen token: ,\n\nStep #2 candidates:\nĠthe      : 0.2350 \nĠand      : 0.1807 \nĠa        : 0.1074 \nĠlocated  : 0.0617 \nĠas       : 0.0530 \n\nChosen token:  and\n\nStep #3 candidates:\nĠthe      : 0.2448 \nĠis       : 0.1628 \nĠit       : 0.1103 \nĠone      : 0.0707 \nĠalso     : 0.0634 \n\nChosen token:  its\n\nStep #4 candidates:\nĠlargest  : 0.1929 \nĠmost     : 0.0652 \nĠpopulation: 0.0645 \nĠmain     : 0.0360 \nĠcultural : 0.0307 \n\nChosen token:  metropolitan\n\nStep #5 candidates:\nĠarea     : 0.8296 \nĠregion   : 0.0740 \nĠpopulation: 0.0085 \nĠis       : 0.0083 \nĠterritory: 0.0073 \n\nChosen token:  area\n\nStep #6 candidates:\nĠis       : 0.6211 \nĠhas      : 0.1119 \n,         : 0.0543 \nĠ         : 0.0227 \nĠincludes : 0.0184 \n\nChosen token:  the\n\nStep #7 candidates:\nĠlargest  : 0.3708 \nĠmost     : 0.2795 \nĠsecond   : 0.0864 \nĠworld    : 0.0486 \nĠ         : 0.0341 \n\nChosen token:  largest\n\nStep #8 candidates:\nĠin       : 0.5508 \nĠcity     : 0.1693 \nĠurban    : 0.0789 \nĠmetropolis: 0.0246 \nĠmetropolitan: 0.0200 \n\nChosen token:  in\n\nStep #9 candidates:\nĠEurope   : 0.6147 \nĠthe      : 0.2847 \nĠRussia   : 0.0637 \nĠall      : 0.0053 \nĠboth     : 0.0047 \n\nChosen token:  Europe\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Task 5: write code for nucleus sampling generation (2 points)**:\n\nUse the `nucleus_sampling()` template below. Look at the detailed generation code above for inspiration. __Please do not use model.generate__.\n\n**Bonus task: write code for beam search (3 bonus points)**","metadata":{"id":"a686Z2SQKgz0"}},{"cell_type":"code","source":"from typing import Tuple, List\n\ndef nucleus_sampling(model, tokenizer, prompt: str, prob: float = 0.5) -> Tuple[str, List[str]]:\n    \"\"\"generates the next token from the nucleus of tokens with cumulative probability up to param:prob\"\"\"\n    inputs = tokenizer(prompt,\n                       return_tensors='pt',\n                       return_token_type_ids=False).to(device)\n\n    probas, indicies = torch.sort(F.softmax(model.forward(**inputs).logits[0, -1, :].float(),\n                                 dim=-1),\n                       descending=True)\n\n    max_input = len((torch.cumsum(probas, dim=-1)<prob).nonzero())\n    samples = indicies[:max_input]\n    conditional_probas = probas[:max_input] / probas[:max_input].sum()\n\n    sampled_token = tokenizer.decode(samples[torch.multinomial(conditional_probas.flatten(), num_samples=1)])\n    possible_tokens = [tokenizer.decode(s) for s in samples]\n\n    # sampled_token should be a string token that was generated\n    # possible_tokens should be a list of all tokens that have non-zero probability\n    return sampled_token, possible_tokens","metadata":{"id":"3WIqDgfBKgz0","execution":{"iopub.status.busy":"2023-12-04T03:36:32.031292Z","iopub.execute_input":"2023-12-04T03:36:32.031667Z","iopub.status.idle":"2023-12-04T03:36:32.040766Z","shell.execute_reply.started":"2023-12-04T03:36:32.031634Z","shell.execute_reply":"2023-12-04T03:36:32.039786Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Tests for nucleus sampling\ntest_prompt = \"Elbrus is the highest\"\nnext_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.9)\nprint(test_prompt, next_token, possible_tokens)\nassert next_token in possible_tokens\nassert 3 <= len(possible_tokens) <= 3\nassert sorted(possible_tokens) == ['mountain', 'peak', 'point']\n\ntest_prompt = \"Large language models can learn to\"\nnext_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.4)\nprint(test_prompt, next_token, possible_tokens)\nassert next_token in possible_tokens\nassert sorted(possible_tokens) == ['be', 'communicate', 'do', 'generate', 'perform', 'predict', 'speak', 'write']\nassert len(possible_tokens) == 8","metadata":{"id":"LzPLQl-HKgz1","outputId":"802294e6-b3d9-4f02-e29c-ea9876b1d9d3","execution":{"iopub.status.busy":"2023-12-04T03:36:33.678017Z","iopub.execute_input":"2023-12-04T03:36:33.678373Z","iopub.status.idle":"2023-12-04T03:36:33.840964Z","shell.execute_reply.started":"2023-12-04T03:36:33.678345Z","shell.execute_reply":"2023-12-04T03:36:33.839764Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Elbrus is the highest  mountain [' mountain', ' peak']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_prompt, next_token, possible_tokens)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m next_token \u001b[38;5;129;01min\u001b[39;00m possible_tokens\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(possible_tokens) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(possible_tokens) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmountain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoint\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m test_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLarge language models can learn to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mAssertionError\u001b[0m: "],"ename":"AssertionError","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Part 3: Chain-of-thought prompting (4 points total)\n\n![img](https://github.com/kojima-takeshi188/zero_shot_cot/raw/main/img/image_stepbystep.png)\n\n---\n\n","metadata":{"id":"5ZaQZhPXOPSG"}},{"cell_type":"code","source":"import json\nimport random\nimport locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n!wget https://raw.githubusercontent.com/kojima-takeshi188/zero_shot_cot/2824685e25809779dbd36900a69825068e9f51ef/dataset/AQuA/test.json -O aqua.json\ndata = list(map(json.loads, open(\"aqua.json\")))","metadata":{"id":"N2AmfelTn5en","outputId":"d744495d-0eeb-415a-95a5-3132391b0428","execution":{"iopub.status.busy":"2023-12-04T02:02:24.882035Z","iopub.execute_input":"2023-12-04T02:02:24.882479Z","iopub.status.idle":"2023-12-04T02:02:26.134479Z","shell.execute_reply.started":"2023-12-04T02:02:24.882446Z","shell.execute_reply":"2023-12-04T02:02:26.133307Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-12-04 02:02:25--  https://raw.githubusercontent.com/kojima-takeshi188/zero_shot_cot/2824685e25809779dbd36900a69825068e9f51ef/dataset/AQuA/test.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 130192 (127K) [text/plain]\nSaving to: ‘aqua.json’\n\naqua.json           100%[===================>] 127.14K  --.-KB/s    in 0.02s   \n\n2023-12-04 02:02:26 (6.29 MB/s) - ‘aqua.json’ saved [130192/130192]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Example:\")\ndata[150]","metadata":{"id":"IATXmPfYw8s6","outputId":"471be3cc-05f6-47af-9420-da6862526c90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive solution\n\nHere, we prompt the model to choose an answer to the example above (`data[150]`) out of the options given above. We're using a format that mimics grade school solution textbook.\n\nPlease note that there are minor formatting changes in options: an extra space and an opening bracket. Those may or may not be important :)","metadata":{"id":"6UcOYQPW8sVq"}},{"cell_type":"code","source":"EXAMPLE_0SHOT = \"\"\"\nQuestion: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\nAnswer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\nCorrect Answer:\n\"\"\".strip()","metadata":{"id":"KtkkdiJl3-UI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# solving an equation directly\nbatch = tokenizer(EXAMPLE_0SHOT, return_tensors='pt', return_token_type_ids=False).to(device)\ntorch.manual_seed(1337)\noutput_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\nprint(\"[Prompt:]\\n\" + EXAMPLE_0SHOT)\nprint(\"=\" * 80)\nprint(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))","metadata":{"id":"hyQ_8tJc6nyv","outputId":"13958400-e98c-4ee1-c09c-dc067f37f4c3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here's how you can solve this with few-shot chain-of-thought prompting.\n\nYou need to chang 3 things\n- use a new field called **Rationale**, that contains a step-by-step solution to the problem\n- add several few-shot examples of previously solved problems **with rationales**\n- change the final prompt so that the model has to generate rationale before answering","metadata":{"id":"suSkiDk28I6C"}},{"cell_type":"code","source":"EXAMPLE_3SHOT_CHAIN_OF_THOUGHT = \"\"\"\nQuestion: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\nAnswer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\nRationale: wholesale cost = 100;\\noriginal price = 100*1.6 = 160;\\nactual price = 160*0.8 = 128.\\nAnswer: B.\nCorrect Answer: B\n\n\nQuestion: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\nAnswer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\nRationale: Profit on one bag: 100*1.25= 125\\nNumber of bags sold = 3000/125 = 24\\nAnswer is C.\nCorrect Answer: C\n\n\nQuestion: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\nAnswer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\nRationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\\nAnswer: D.\nCorrect Answer: D\n\n\nQuestion: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\nAnswer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\nRationale:\n\"\"\".strip()","metadata":{"id":"K0F1jYdRvoJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = tokenizer(EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, return_tensors='pt', return_token_type_ids=False).to(device)\ntorch.manual_seed(1337)\noutput_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\nprint(\"[Prompt:]\\n\" + EXAMPLE_3SHOT_CHAIN_OF_THOUGHT)\nprint(\"=\" * 80)\nprint(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))\n#### NOTE: scroll down for the final answer (below the ======= line)","metadata":{"id":"Tn8QoAYcRkHC","outputId":"6b00352e-3db4-4cb9-ece1-2deb4392e3f1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Task 6 (1 pt)__ write a function that automatically creates chain-of-thought prompts. Follow the instructions from the function docstring.","metadata":{"id":"s4px3jv-99-m"}},{"cell_type":"code","source":"QUESTION_PREFIX = \"Question: \"\nOPTIONS_PREFIX = \"Answer Choices: \"\nCHAIN_OF_THOUGHT_PREFIX = \"Rationale: \"\nANSWER_PREFIX = \"Correct Answer: \"\nFEWSHOT_SEPARATOR = \"\\n\\n\\n\"\n\ndef make_prompt(*, main_question, fewshot_examples):\n  \"\"\"\n  Your goal is to produce the same prompt as the EXAMPLE_3SHOT_CHAIN_OF_THOUGHT automatically\n\n  For each few-shot question, make sure to follow the following rules:\n  1. Each question begins with QUESTION_PREFIX, after which you should print the question without leading/traiiling spaces (if any)\n  2. After the question, provide space-separated options. Each option should be put in double brackets, followed by option text, e.g. \"(A) 146%\"\n  3. Then, provide the answer as a single letter (A-E)\n  4. Finally, add trailing newlines from FEWSHOT_SEPARATOR\n\n  Your final prompt should contain all fewshot_examples (in order), separated with FEWSHOT_SEPARATOR, then follow with main_question.\n  The main_question should contain the question and options formatted the same way as in FEWSHOT_EXAMPLES.\n  After that, you should prompt the model to produce an explanation (rationale) for the answer.\n\n  Please make sure your prompt contains no leading/trailing newlines or spaces, same as in EXAMPLE_3SHOT_CHAIN_OF_THOUGHT\n  \"\"\"\n\n  prompt = ''\n  for f_s in fewshot_examples:\n    prompt += f\"{QUESTION_PREFIX}{f_s['question']}\\n\"\n    prompt += f\"{OPTIONS_PREFIX}({' ('.join(f_s['options']).replace(')', ') ')}\\n\"\n    prompt += f\"{CHAIN_OF_THOUGHT_PREFIX}{f_s['rationale']}\\n\"\n    prompt += f\"{ANSWER_PREFIX}{f_s['correct']}{FEWSHOT_SEPARATOR}\"\n\n  prompt += f\"{QUESTION_PREFIX}{main_question['question']}\\n\"\n  prompt += f\"{OPTIONS_PREFIX}({' ('.join(main_question['options']).replace(')', ') ')}\\n\"\n  prompt += f\"{CHAIN_OF_THOUGHT_PREFIX}\"\n\n  return prompt.strip()\n\n\n\ngenerated_fewshot_prompt = make_prompt(main_question=data[150], fewshot_examples=(data[30], data[20], data[5]))\nassert generated_fewshot_prompt == EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, \"prompts don't match\"\nassert generated_fewshot_prompt != make_prompt(main_question=data[150], fewshot_examples=())\nassert generated_fewshot_prompt.endswith(make_prompt(main_question=data[150], fewshot_examples=()))\n\nprint(\"Well done!\")\n\n# Hint: if two prompts do not match, you may find it usefull to use https://www.diffchecker.com or similar to find the difference","metadata":{"id":"_ntyFPMt9fyt","outputId":"7bba6973-a91f-4d1c-b4a2-9ec3df4d1778","execution":{"iopub.status.busy":"2023-12-04T02:06:25.436845Z","iopub.execute_input":"2023-12-04T02:06:25.437561Z","iopub.status.idle":"2023-12-04T02:06:25.490002Z","shell.execute_reply.started":"2023-12-04T02:06:25.437532Z","shell.execute_reply":"2023-12-04T02:06:25.488872Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     39\u001b[0m generated_fewshot_prompt \u001b[38;5;241m=\u001b[39m make_prompt(main_question\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m150\u001b[39m], fewshot_examples\u001b[38;5;241m=\u001b[39m(data[\u001b[38;5;241m30\u001b[39m], data[\u001b[38;5;241m20\u001b[39m], data[\u001b[38;5;241m5\u001b[39m]))\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m generated_fewshot_prompt \u001b[38;5;241m==\u001b[39m \u001b[43mEXAMPLE_3SHOT_CHAIN_OF_THOUGHT\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompts don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m generated_fewshot_prompt \u001b[38;5;241m!=\u001b[39m make_prompt(main_question\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m150\u001b[39m], fewshot_examples\u001b[38;5;241m=\u001b[39m())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m generated_fewshot_prompt\u001b[38;5;241m.\u001b[39mendswith(make_prompt(main_question\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m150\u001b[39m], fewshot_examples\u001b[38;5;241m=\u001b[39m()))\n","\u001b[0;31mNameError\u001b[0m: name 'EXAMPLE_3SHOT_CHAIN_OF_THOUGHT' is not defined"],"ename":"NameError","evalue":"name 'EXAMPLE_3SHOT_CHAIN_OF_THOUGHT' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"__Task 7 (1 points):__ Evaluate your prompt.\n\nPlease run the model on the entire dataset and measure it's accuracy.\nFor each question, peak $n=5$ other questions at random to serve as few-shot examples. Make sure not to accidentally sample the main_question among few-shot examples. For scientific evaluation, it is also a good practice to split the data into two parts: one for eval, and another for few-shot examples. However, doing so is optional in this homework.\n\nThe tricky part is when to stop generating: if you don't control for this, your model can accidentally generate a whole new question - and promptyly answer it :) To make sure you get the correct answer, stop generating tokens when the model is done explaining it's solution. To circumvent this, you need to __stop generating as soon as the model generates Final Answer: [A-E]__\nTo do so, you can either generate manually (see low-level generation above) or use [transformers stopping criteria](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/2), whichever you prefer.\n\nIf you do everything right, the model should be much better than random. However, please __do not expect miracles__: this is far from the best models, and it will perform much worse than an average human.","metadata":{"id":"P7DzQ8hfOcFR"}},{"cell_type":"code","source":"NUM_SAMPLES = 0    # use this to count how many samples you evaluated\nNUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\nNUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer","metadata":{"id":"oMvj9eCtQwnz","execution":{"iopub.status.busy":"2023-12-04T02:06:13.459856Z","iopub.execute_input":"2023-12-04T02:06:13.460222Z","iopub.status.idle":"2023-12-04T02:06:13.465254Z","shell.execute_reply.started":"2023-12-04T02:06:13.460189Z","shell.execute_reply":"2023-12-04T02:06:13.464038Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"< A whole lot of your code here >\n\n# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)","metadata":{"id":"Zh6gejr0JNuh","outputId":"8f8412fe-2185-4629-dcc3-c5588499664b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import StoppingCriteriaList, StoppingCriteria\nfrom tqdm import tqdm\n\ntrain_few_shot, test_main_question = train_test_split(data, test_size=0.165, random_state=2023)\n\n# Возникали ошибки с cuda, поэтому возьмём вот это решение из предложенной ссылочки\n# https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/7\nclass StoppingCriteriaSub(StoppingCriteria):\n\n    def __init__(self, stops = [], encounters=1):\n        super().__init__()\n        self.stops = [stop.to(\"cuda\") for stop in stops]\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n        for stop in self.stops:\n            if torch.all((stop == input_ids[0][-len(stop):])).item():\n                return True\n\n        return False\n\n\nstop_words = [\": A\", \": B\", \": C\", \": D\", \": E\"]\nstop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\nstopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])","metadata":{"id":"bFoUUIVrAnrK","execution":{"iopub.status.busy":"2023-12-04T02:15:26.887303Z","iopub.execute_input":"2023-12-04T02:15:26.887686Z","iopub.status.idle":"2023-12-04T02:15:26.900429Z","shell.execute_reply.started":"2023-12-04T02:15:26.887654Z","shell.execute_reply":"2023-12-04T02:15:26.899413Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"NUM_SAMPLES = 0    # use this to count how many samples you evaluated\nNUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\nNUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer\n\ncorrect_answers, generated_answers, generated = [], [], []\nwith tqdm(total=len(test_main_question)) as pbar:\n  for i in range(len(test_main_question)):\n      correct_answer = test_main_question[i]['correct']\n      correct_answers.append(correct_answer)\n\n      few_shots = np.random.choice(train_few_shot, 5)\n      prompt = make_prompt(main_question=test_main_question[i],\n                          fewshot_examples=few_shots)\n\n      batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n      torch.manual_seed(2023)\n      output_tokens = model.generate(**batch,\n                                    max_new_tokens=1024, do_sample=True,\n                                    stopping_criteria=stopping_criteria)\n      generated_answer = tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu())\n      generated.append(generated_answer)\n      generated_answers.append(generated_answer[-1])\n\n      if generated_answer[-1] == test_main_question[i]['correct']:\n          NUM_CORRECT += 1\n      if generated_answer[-17: -1] == \"Correct Answer: \":\n          NUM_RESPONDED += 1\n      NUM_SAMPLES += 1\n      pbar.update(1)\n","metadata":{"id":"UzaWjik8BLRc","outputId":"7a2739e6-a340-4c05-a90c-f4a484aff7e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Responded : %.4f\" % NUM_RESPONDED / NUM_SAMPLES)\nprint(\"Accuracy (when responded): %.4f\" % NUM_CORRECT / NUM_RESPONDED)\nprint(\"Accuracy (overall): %.4f\" % NUM_CORRECT / NUM_SAMPLES)\n\nif NUM_RESPONDED / NUM_SAMPLES < 0.9:\n  print(\"Something is wrong with the evaluation technique (for 5-shot CoT): the model refuses to answer too many questions.\")\n  print(\"Make sure you generate enough tokens that the model can produce a correct answer.\")\n  print(\"When in doubt, take a look at the full model output. You can often spot errors there.\")","metadata":{"id":"gxrCJvxIJSjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Решение проблемы с плохим обучением\n\nУвидел, что у меня не работает модель. Проверил на конкретном примере и понял, что напрасно ждал час, пока обучится, потому что критерий остановки генерации ответа не сработал. \n\nПочитал вот [эту проблемку на гитхабе](https://github.com/huggingface/transformers/issues/22436?ysclid=lpq92y6gyr436024385) и понял, что надо поменять модель.","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:15:50.698669Z","iopub.execute_input":"2023-12-04T02:15:50.699069Z","iopub.status.idle":"2023-12-04T02:15:50.721716Z","shell.execute_reply.started":"2023-12-04T02:15:50.699038Z","shell.execute_reply":"2023-12-04T02:15:50.720560Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model_name = 'tiiuae/falcon-7b'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    offload_state_dict=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:02:26.136149Z","iopub.execute_input":"2023-12-04T02:02:26.136591Z","iopub.status.idle":"2023-12-04T02:04:28.519477Z","shell.execute_reply.started":"2023-12-04T02:02:26.136548Z","shell.execute_reply":"2023-12-04T02:04:28.518705Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c35df82190f14ccf8a1016a37f78ac7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a9957cb8994bfca2d638170a6ee408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757fe02f85df45a2aa62b7bebf63e0b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"486fe256ba1a487bbfbd1feeef39eabf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b03a7424f4141cba8e31f241acc07d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5736546f8c954e4dbed62fc67afb8bb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5669755b7454b169207fd0c709a8fa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a5e9b725d8b48f2a23628ed5203f867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e8df3ed7a14480bca4752be92d0d69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039c85fe37704a1682d76905e7ed04bb"}},"metadata":{}}]},{"cell_type":"code","source":"few_shots = np.random.choice(train_few_shot, 5)\nprompt = make_prompt(main_question=test_main_question[0],\n                  fewshot_examples=few_shots)\n\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\ntorch.manual_seed(2023)\noutput_tokens = model.generate(**batch, \n                            max_new_tokens=1024, do_sample=True, \n                            stopping_criteria=stopping_criteria,\n                            pad_token_id=tokenizer.eos_token_id)\ngenerated_answer = tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu())\nprint(generated_answer)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:14:00.787316Z","iopub.execute_input":"2023-12-04T02:14:00.787700Z","iopub.status.idle":"2023-12-04T02:14:06.501704Z","shell.execute_reply.started":"2023-12-04T02:14:00.787667Z","shell.execute_reply":"2023-12-04T02:14:06.500766Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Generating stopped\n 600(0.8) + 80 = 480 golf balls\n640 = 480\nR = 1.6\n640(0.6) - 80 = 480\nAnswer E.\nCorrect Answer: E\n","output_type":"stream"}]},{"cell_type":"code","source":"NUM_SAMPLES = 0    # use this to count how many samples you evaluated\nNUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\nNUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer\n\ncorrect_answers, generated_answers, generated = [], [], []\nwith tqdm(total=len(test_main_question)) as pbar:\n  for i in range(len(test_main_question)):\n      correct_answer = test_main_question[i]['correct']\n      correct_answers.append(correct_answer)\n\n      few_shots = np.random.choice(train_few_shot, 5)\n      prompt = make_prompt(main_question=test_main_question[i],\n                          fewshot_examples=few_shots)\n\n      batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n      torch.manual_seed(2023)\n      output_tokens = model.generate(**batch,\n                                    max_new_tokens=1024, do_sample=True,\n                                    stopping_criteria=stopping_criteria,\n                                    pad_token_id=tokenizer.eos_token_id)\n\n      generated_answer = tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu())\n      generated.append(generated_answer)\n      generated_answers.append(generated_answer[-1])\n\n      if generated_answer[-1] == test_main_question[i]['correct']:\n          NUM_CORRECT += 1\n      if generated_answer[-17: -1] == \"Correct Answer: \":\n          NUM_RESPONDED += 1\n      NUM_SAMPLES += 1\n      pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:15:53.967694Z","iopub.execute_input":"2023-12-04T02:15:53.968639Z","iopub.status.idle":"2023-12-04T02:27:34.045674Z","shell.execute_reply.started":"2023-12-04T02:15:53.968599Z","shell.execute_reply":"2023-12-04T02:27:34.044804Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 42/42 [11:40<00:00, 16.67s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Responded : %.4f\" % (NUM_RESPONDED / NUM_SAMPLES))\nprint(\"Accuracy (when responded): %.4f\" % (NUM_CORRECT / NUM_RESPONDED))\nprint(\"Accuracy (overall): %.4f\" % (NUM_CORRECT / NUM_SAMPLES))\n\nif NUM_RESPONDED / NUM_SAMPLES < 0.9:\n  print(\"Something is wrong with the evaluation technique (for 5-shot CoT): the model refuses to answer too many questions.\")\n  print(\"Make sure you generate enough tokens that the model can produce a correct answer.\")\n  print(\"When in doubt, take a look at the full model output. You can often spot errors there.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:31:27.190081Z","iopub.execute_input":"2023-12-04T02:31:27.190477Z","iopub.status.idle":"2023-12-04T02:31:27.196687Z","shell.execute_reply.started":"2023-12-04T02:31:27.190445Z","shell.execute_reply":"2023-12-04T02:31:27.195805Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Responded : 0.6429\nAccuracy (when responded): 0.1481\nAccuracy (overall): 0.0952\nSomething is wrong with the evaluation technique (for 5-shot CoT): the model refuses to answer too many questions.\nMake sure you generate enough tokens that the model can produce a correct answer.\nWhen in doubt, take a look at the full model output. You can often spot errors there.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"В целом, модель приемлемо обучилась, выдавая правильный ответ в 15% случаев. Восстание машин ещё не близко! По крайней мере, если они обучены мной.","metadata":{}},{"cell_type":"code","source":"# хотел полечить проблему NUM_RESPONDED / NUM_SAMPLES < 0.9, но оч долго работает\nNUM_SAMPLES = 0    # use this to count how many samples you evaluated\nNUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\nNUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer\n\ncorrect_answers, generated_answers, generated = [], [], []\nwith tqdm(total=len(test_main_question)) as pbar:\n  for i in range(len(test_main_question)):\n      correct_answer = test_main_question[i]['correct']\n      correct_answers.append(correct_answer)\n\n      few_shots = np.random.choice(train_few_shot, 5)\n      prompt = make_prompt(main_question=test_main_question[i],\n                          fewshot_examples=few_shots)\n\n      batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n      torch.manual_seed(2023)\n      output_tokens = model.generate(**batch,\n                                    max_new_tokens=1024, do_sample=True,\n                                    num_beams=4,\n                                    stopping_criteria=stopping_criteria,\n                                    pad_token_id=tokenizer.eos_token_id)\n\n      generated_answer = tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu())\n      generated.append(generated_answer)\n      generated_answers.append(generated_answer[-1])\n\n      if generated_answer[-1] == test_main_question[i]['correct']:\n          NUM_CORRECT += 1\n      if generated_answer[-17: -1] == \"Correct Answer: \":\n          NUM_RESPONDED += 1\n      NUM_SAMPLES += 1\n      pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:33:12.521994Z","iopub.execute_input":"2023-12-04T02:33:12.522982Z","iopub.status.idle":"2023-12-04T02:35:01.668805Z","shell.execute_reply.started":"2023-12-04T02:33:12.522945Z","shell.execute_reply":"2023-12-04T02:35:01.667033Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"  0%|          | 0/42 [01:48<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m batch \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, return_token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2023\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m generated_answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_tokens[\u001b[38;5;241m0\u001b[39m][batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:]\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     24\u001b[0m generated\u001b[38;5;241m.\u001b[39mappend(generated_answer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1722\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1715\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1716\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1717\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1718\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1719\u001b[0m     )\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1739\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1740\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1746\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3350\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3346\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3350\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3353\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3358\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:1279\u001b[0m, in \u001b[0;36mFalconForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1279\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1293\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:1163\u001b[0m, in \u001b[0;36mFalconModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1154\u001b[0m         create_custom_forward(block),\n\u001b[1;32m   1155\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         padding_mask,\n\u001b[1;32m   1161\u001b[0m     )\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:794\u001b[0m, in \u001b[0;36mFalconDecoderLayer.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, position_ids, layer_past, head_mask, use_cache, output_attentions, padding_mask)\u001b[0m\n\u001b[1;32m    791\u001b[0m     attention_layernorm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_layernorm_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnew_decoder_architecture:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:439\u001b[0m, in \u001b[0;36mFalconAttention.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, position_ids, layer_past, head_mask, use_cache, output_attentions, padding_mask)\u001b[0m\n\u001b[1;32m    437\u001b[0m num_kv_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_decoder_architecture \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kv_heads\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# 3 x [batch_size, seq_length, num_heads, head_dim]\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m (query_layer, key_layer, value_layer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_qkv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m batch_size, query_length, _, _ \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    443\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, query_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"__Task 8 (2 points)__ Experiment time!\n<img width=200px src=https://www.evolvefish.com/cdn-cgi/image/quality%3D85/assets/images/Apparel/TShirtsWomenCont/Main/EF-APP-CWT-00068(Main).jpg>\n\nYour final quest is to use the testbench you've just written to answer one of the following questions:\n\n### Option 1: How many shots do you need?\n\nHow does model accuracy change with the number of fewshot examples?\n\na. check if the model accuracy changes as you increase/decrease the number of \"shots\"\n\nb. try to prompt-engineer a model into giving the best rationale __without__ any few-shot examples, i.e. zero-shot\n\nFor zero-shot mode, feel free to use wild prompt-engineering or modify the inference procedure.\n\n### Option 2: Is this prompting tecnique reliable?\n\n_Inspired by ongoing research by Anton Voronov, Lena Volf and Max Ryabinin._\n\nFor this option, you need to check if the model behavior (and hence, accuracy) is robust to perturbations in the input prompt.\n\na. Does the accuracy degrade if you provide wrong answers to few-shot examples? (make sure to modify rationale if it contains answer in the end)\n\nb. Does it degrade if you replace question/answer prompts with \"Q\" and \"A\"? What if you write both on the same line? Change few-shot separators?\n\n\n\n### Option 3: Inference Matters\n\nThere are many ways to inference the model, not all of them equal.\n\na. check whether greedy inference or beam search affects model generation quality\n\nb. implement and evaluate sampling with voting (see explanation below).\n\n\nThe voting technique(b) should work as follows: first, you generate k (e.g. 50) \"attempts\" at an answer using nucleus sampling (or a similar technique).\nThen, you count how many of those attempts chose a particular option (A, B, etc) as the final answer. The option that was chosen most frequently has the most \"votes\", and therefore \"wins\".\n\nTo speed up voting, you may want to generate these attempts in parallel as a batch. That should be very easy to implement: just run `model.generate` on a list with multiple copies of the same prompt.\n\n\n\n\n================================================\n\n__Common rules:__ You will need to test both hypothes (A and B) in the chosen option. You may choose to replace one of them with your own idea - but please ask course staff in advance (via telegram) if you want full points.\n\nFeel free to organize your code and report as you see fit - but please make sure it's readable and the code runs top-to-bottom :)\nWrite a short informal report about what you tried and, in doing so, what did you found. Minimum of 2 paragraphs; more is ok; creative visualizations are welcome.\n\nYou are allowed (but not required) to prompt the model into generating a report for you --- or helping you write one. However, if you do so, make sure that it is still human-readable :)\n\n","metadata":{"id":"UZLK2rLiKxbM"}},{"cell_type":"code","source":"# feel free to organize your solution as you see fit","metadata":{"id":"_r6UVDl4NEua"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Option 1","metadata":{"id":"NMe5H53rHh5K"}},{"cell_type":"markdown","source":"Намого более простой в реализации с точки зрения нового кода кажется опция 1, её и выберу","metadata":{"id":"boGe2KpvH8iy"}},{"cell_type":"markdown","source":"## A","metadata":{"id":"WmDrrNZ9HkZR"}},{"cell_type":"code","source":"sizes = [1, 10]\nresults = {1:{'NUM_CORRECT': 0,\n              'NUM_RESPONDED': 0,\n              'NUM_SAMPLES': 0},\n           10:{'NUM_CORRECT': 0,\n              'NUM_RESPONDED': 0,\n              'NUM_SAMPLES': 0}}\n\n# провожу эксперимент на усечённом датасете, поскольку хочу спать\n# отмена, понижение числа токенов ускоряет работу\n\nwith tqdm(total=len(test_main_question)) as pbar:\n  for size in sizes:\n    for i in range(len(test_main_question)):\n        correct_answer = test_main_question[i]['correct']\n\n        few_shots = np.random.choice(train_few_shot, size)\n        prompt = make_prompt(main_question=test_main_question[i],\n                            fewshot_examples=few_shots)\n\n        batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n        torch.manual_seed(2023)\n        output_tokens = model.generate(**batch,\n                                    max_new_tokens=1024, do_sample=True,\n                                    stopping_criteria=stopping_criteria,\n                                    pad_token_id=tokenizer.eos_token_id)\n        generated_answer = tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu())\n\n        if generated_answer[-1] == correct_answer:\n            results[size]['NUM_CORRECT'] += 1\n        if generated_answer[-17: -1] == \"Correct Answer: \":\n            results[size]['NUM_RESPONDED'] += 1\n        results[size]['NUM_SAMPLES'] += 1\n        pbar.update(1)","metadata":{"id":"CYz37Y1KHnWp","execution":{"iopub.status.busy":"2023-12-04T02:41:58.810567Z","iopub.execute_input":"2023-12-04T02:41:58.811091Z","iopub.status.idle":"2023-12-04T02:56:30.217586Z","shell.execute_reply.started":"2023-12-04T02:41:58.811047Z","shell.execute_reply":"2023-12-04T02:56:30.216048Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"43it [11:00, 25.22s/it]                        This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n48it [13:12, 27.26s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2135 > 2048). Running this sequence through the model will result in indexing errors\n54it [14:30, 16.13s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m batch \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, return_token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2023\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m generated_answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_tokens[\u001b[38;5;241m0\u001b[39m][batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:]\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_answer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m correct_answer:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:1279\u001b[0m, in \u001b[0;36mFalconForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1279\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1293\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:1163\u001b[0m, in \u001b[0;36mFalconModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1154\u001b[0m         create_custom_forward(block),\n\u001b[1;32m   1155\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         padding_mask,\n\u001b[1;32m   1161\u001b[0m     )\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:794\u001b[0m, in \u001b[0;36mFalconDecoderLayer.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, position_ids, layer_past, head_mask, use_cache, output_attentions, padding_mask)\u001b[0m\n\u001b[1;32m    791\u001b[0m     attention_layernorm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_layernorm_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnew_decoder_architecture:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:159\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 159\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:290\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    286\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    287\u001b[0m             module, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name], fp16_statistics\u001b[38;5;241m=\u001b[39mfp16_statistics\n\u001b[1;32m    288\u001b[0m         )\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_keys\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 160\u001b[0m         {\n\u001b[1;32m    161\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    160\u001b[0m         {\n\u001b[0;32m--> 161\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"results[7] = {'NUM_CORRECT': 0,\n              'NUM_RESPONDED': 0,\n              'NUM_SAMPLES': 0}\n\nfor i in range(len(test_main_question)):\n    correct_answer = test_main_question[i]['correct']\n\n    few_shots = np.random.choice(train_few_shot, 7)\n    prompt = make_prompt(main_question=test_main_question[i],\n                        fewshot_examples=few_shots)\n\n    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n    torch.manual_seed(2023)\n    output_tokens = model.generate(**batch,\n                                max_new_tokens=512, do_sample=True,\n                                stopping_criteria=stopping_criteria,\n                                pad_token_id=tokenizer.eos_token_id)\n    generated_answer = tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu())\n\n    if generated_answer[-1] == correct_answer:\n        results[7]['NUM_CORRECT'] += 1\n    if generated_answer[-17: -1] == \"Correct Answer: \":\n        results[7]['NUM_RESPONDED'] += 1\n    results[7]['NUM_SAMPLES'] += 1\n    pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T02:58:28.999566Z","iopub.execute_input":"2023-12-04T02:58:29.000327Z","iopub.status.idle":"2023-12-04T03:08:51.522375Z","shell.execute_reply.started":"2023-12-04T02:58:29.000294Z","shell.execute_reply":"2023-12-04T03:08:51.521354Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:09:47.933384Z","iopub.execute_input":"2023-12-04T03:09:47.933896Z","iopub.status.idle":"2023-12-04T03:09:47.940923Z","shell.execute_reply.started":"2023-12-04T03:09:47.933862Z","shell.execute_reply":"2023-12-04T03:09:47.940020Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{1: {'NUM_CORRECT': 10, 'NUM_RESPONDED': 21, 'NUM_SAMPLES': 42},\n 10: {'NUM_CORRECT': 3, 'NUM_RESPONDED': 7, 'NUM_SAMPLES': 12},\n 7: {'NUM_CORRECT': 6, 'NUM_RESPONDED': 26, 'NUM_SAMPLES': 42}}"},"metadata":{}}]},{"cell_type":"markdown","source":"## B","metadata":{"id":"o-S7e3KxHldW"}},{"cell_type":"code","source":"QUESTION_PREFIX = \"Question: \"\nOPTIONS_PREFIX = \"Answer Choices: \"\nCHAIN_OF_THOUGHT_PREFIX = \"Rationale: \"\nANSWER_PREFIX = \"Correct Answer: \"\nFEWSHOT_SEPARATOR = \"\\n\\n\\n\"\n\ndef make_zero_shot_prompt(*, main_question):\n\n  prompt = \"Answer the question using Math. You are to choose the correct answer in format 'Correct Answer: *letter choice*'\\n\"\n  prompt += f\"{QUESTION_PREFIX}{main_question['question']}\\n\"\n  prompt += f\"{OPTIONS_PREFIX}({' ('.join(main_question['options']).replace(')', ') ')}\\n\"\n  prompt += f\"{CHAIN_OF_THOUGHT_PREFIX}\"\n\n  return prompt.strip()","metadata":{"id":"5wOw69hkLIIj","execution":{"iopub.status.busy":"2023-12-04T03:11:03.907410Z","iopub.execute_input":"2023-12-04T03:11:03.907846Z","iopub.status.idle":"2023-12-04T03:11:03.914000Z","shell.execute_reply.started":"2023-12-04T03:11:03.907811Z","shell.execute_reply":"2023-12-04T03:11:03.912961Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(make_zero_shot_prompt(main_question=data[150]))","metadata":{"id":"oFmRoDLXMRHd","execution":{"iopub.status.busy":"2023-12-04T03:11:05.624967Z","iopub.execute_input":"2023-12-04T03:11:05.625343Z","iopub.status.idle":"2023-12-04T03:11:05.630855Z","shell.execute_reply.started":"2023-12-04T03:11:05.625312Z","shell.execute_reply":"2023-12-04T03:11:05.629874Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Answer the question using Math. You are to choose the correct answer in format 'Correct Answer: *letter choice*'\nQuestion: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\nAnswer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\nRationale:\n","output_type":"stream"}]},{"cell_type":"code","source":"results[0] = {'NUM_CORRECT': 0,\n              'NUM_RESPONDED': 0,\n              'NUM_SAMPLES': 0}\n\n# провожу эксперимент на усечённом датасете, поскольку хочу спать\n# отмена, понижение числа токенов ускоряет работу\n\nwith tqdm(total=len(test_main_question)) as pbar:\n  for i in range(len(test_main_question)):\n      correct_answer = test_main_question[i]['correct']\n\n      prompt = make_zero_shot_prompt(main_question=test_main_question[i])\n      batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n      torch.manual_seed(2023)\n      output_tokens = model.generate(**batch,\n                                    max_new_tokens=1024, do_sample=True,\n                                    stopping_criteria=stopping_criteria,\n                                    pad_token_id=tokenizer.eos_token_id)\n      generated_answer = tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu())\n\n\n      if generated_answer[-1] == correct_answer:\n          results[size]['NUM_CORRECT'] += 1\n      if generated_answer[-17: -1] == \"Correct Answer: \":\n          results[size]['NUM_RESPONDED'] += 1\n      results[size]['NUM_SAMPLES'] += 1\n      pbar.update(1)","metadata":{"id":"KEc4y6D4M15C","execution":{"iopub.status.busy":"2023-12-04T03:11:13.109103Z","iopub.execute_input":"2023-12-04T03:11:13.109489Z","iopub.status.idle":"2023-12-04T03:25:22.937960Z","shell.execute_reply.started":"2023-12-04T03:11:13.109456Z","shell.execute_reply":"2023-12-04T03:25:22.936946Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"100%|██████████| 42/42 [14:09<00:00, 20.23s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_answer","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:27:18.851684Z","iopub.execute_input":"2023-12-04T03:27:18.852099Z","iopub.status.idle":"2023-12-04T03:27:18.858554Z","shell.execute_reply.started":"2023-12-04T03:27:18.852067Z","shell.execute_reply":"2023-12-04T03:27:18.857536Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'\\n1) The cost of an _______ _______ is $18. Let _______ _______ be the cost of the _______ _______. The cost of a _______ _______ is 3 times the cost of an _______ _______. Therefore using the logic above, the cost of a _______ _______ is 3 times the cost of an _______ _______.\\n2) The cost of an _______ _______ is $18. Let _______ _______ be the cost of an _______ _______. The cost of a _______ _______ is 3 times the cost of an _______ _______. Therefore the cost of an _______ _______ is 3 times the cost of a _______ _______.\\n3) The cost of an _______ _______ is $18. Let _______ _______ be the cost of a _______ _______. (A) The cost of an _______ _______ is 6 times the cost of an _______ _______. (B) The cost of an _______ _______ is 3 times the cost of an _______ _______. (C) The cost of an _______ is $10. (D) The cost of an _______ _______ is $15. (E) The cost of a _______ _______ is 9 times that of an _______ _______.. Therefore using the logic above, the cost of a _______ _______ is 9 times the cost of a _______ _______.\\n4) The cost of an _______ _______ is $18. Let _______ _______ be the cost of an _______ _______. (A) The cost of an _______ _______ is 3 times the cost of an _______ _______. (B) The cost of an _______ _______ is 2 times the cost of an _______ _______.\\n(A) Cost of a ______ mango = (1/18) * 18. = $10\\n(B) Cost of a ______ mango = (1/15) * 15. = $15\\n(C) Cost of a ______ mango = (1/18) * 18. = $10\\n(D) Cost of a ______ mango = (1/18) * 18. = $10\\n(E) Cost of a ______ mango = (1/27) * 27. = $27\\n(E) Correct Answer to the question: *letter choice*\\nThe correct answer to the question was \"Cost of a _______ mango = (1/27) * 27\"<|endoftext|>'"},"metadata":{}}]},{"cell_type":"markdown","source":"Что-то явно пошло не так.","metadata":{}},{"cell_type":"markdown","source":"## Выводы","metadata":{"id":"neJS-hwqHmY7"}},{"cell_type":"code","source":"results[5] = {'NUM_CORRECT': NUM_CORRECT,\n              'NUM_RESPONDED': NUM_RESPONDED,\n              'NUM_SAMPLES': NUM_SAMPLES}","metadata":{"id":"riQdTtCkNW-G","execution":{"iopub.status.busy":"2023-12-04T03:25:48.322520Z","iopub.execute_input":"2023-12-04T03:25:48.323288Z","iopub.status.idle":"2023-12-04T03:25:48.327679Z","shell.execute_reply.started":"2023-12-04T03:25:48.323228Z","shell.execute_reply":"2023-12-04T03:25:48.326718Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:25:52.918408Z","iopub.execute_input":"2023-12-04T03:25:52.918802Z","iopub.status.idle":"2023-12-04T03:25:52.926113Z","shell.execute_reply.started":"2023-12-04T03:25:52.918758Z","shell.execute_reply":"2023-12-04T03:25:52.925118Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{1: {'NUM_CORRECT': 10, 'NUM_RESPONDED': 21, 'NUM_SAMPLES': 42},\n 10: {'NUM_CORRECT': 3, 'NUM_RESPONDED': 7, 'NUM_SAMPLES': 54},\n 7: {'NUM_CORRECT': 6, 'NUM_RESPONDED': 26, 'NUM_SAMPLES': 42},\n 0: {'NUM_CORRECT': 0, 'NUM_RESPONDED': 0, 'NUM_SAMPLES': 0},\n 5: {'NUM_CORRECT': 0, 'NUM_RESPONDED': 0, 'NUM_SAMPLES': 0}}"},"metadata":{}}]},{"cell_type":"markdown","source":"Вижу, что нашаманил не то в словаре. Очень лень в 6-30 поправлять это кодом и заново прогонять корректно, поэтому быстро вручную скорректируем словарь и сравним полученные результаты:\n\n* 5\nResponded : 0.6429\nAccuracy (when responded): 0.1481\nAccuracy (overall): 0.0952\n\n* 0 получим, вычтя 10 отсюда из 10 в новой версии словаря\n{1: {'NUM_CORRECT': 10, 'NUM_RESPONDED': 21, 'NUM_SAMPLES': 42},\n 10: {'NUM_CORRECT': 3, 'NUM_RESPONDED': 7, 'NUM_SAMPLES': 12},\n 7: {'NUM_CORRECT': 6, 'NUM_RESPONDED': 26, 'NUM_SAMPLES': 42}}","metadata":{}},{"cell_type":"code","source":"results[0]['NUM_CORRECT'] = 0\nresults[0]['NUM_RESPONDED'] = 0\nresults[0]['NUM_SAMPLES'] = 42\n\nresults[5]['NUM_CORRECT'] = round(0.0952*42, 0)\nresults[5]['NUM_RESPONDED'] = round(0.6429*42, 0)\nresults[5]['NUM_SAMPLES'] = 42\n\nresults","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:31:45.995421Z","iopub.execute_input":"2023-12-04T03:31:45.995833Z","iopub.status.idle":"2023-12-04T03:31:46.006630Z","shell.execute_reply.started":"2023-12-04T03:31:45.995798Z","shell.execute_reply":"2023-12-04T03:31:46.005580Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{1: {'NUM_CORRECT': 10, 'NUM_RESPONDED': 21, 'NUM_SAMPLES': 42},\n 10: {'NUM_CORRECT': 3, 'NUM_RESPONDED': 7, 'NUM_SAMPLES': 54},\n 7: {'NUM_CORRECT': 6, 'NUM_RESPONDED': 26, 'NUM_SAMPLES': 42},\n 0: {'NUM_CORRECT': 0, 'NUM_RESPONDED': 0, 'NUM_SAMPLES': 42},\n 5: {'NUM_CORRECT': 4.0, 'NUM_RESPONDED': 27.0, 'NUM_SAMPLES': 42}}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\nresults_df = pd.DataFrame(results).T.drop(index=10)\nresults_df['perc_correct'] = results_df.NUM_CORRECT / results_df.NUM_SAMPLES * 100\nresults_df['perc_correct_among_responded'] = results_df.NUM_CORRECT / results_df.NUM_RESPONDED * 100\nresults_df['perc_responded'] = results_df.NUM_RESPONDED / results_df.NUM_SAMPLES * 100\nresults_df.sort_values('perc_correct', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T03:33:46.462701Z","iopub.execute_input":"2023-12-04T03:33:46.463088Z","iopub.status.idle":"2023-12-04T03:33:46.485058Z","shell.execute_reply.started":"2023-12-04T03:33:46.463056Z","shell.execute_reply":"2023-12-04T03:33:46.484132Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"   NUM_CORRECT  NUM_RESPONDED  NUM_SAMPLES  perc_correct  \\\n1         10.0           21.0         42.0     23.809524   \n7          6.0           26.0         42.0     14.285714   \n5          4.0           27.0         42.0      9.523810   \n0          0.0            0.0         42.0      0.000000   \n\n   perc_correct_among_responded  perc_responded  \n1                     47.619048       50.000000  \n7                     23.076923       61.904762  \n5                     14.814815       64.285714  \n0                           NaN        0.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NUM_CORRECT</th>\n      <th>NUM_RESPONDED</th>\n      <th>NUM_SAMPLES</th>\n      <th>perc_correct</th>\n      <th>perc_correct_among_responded</th>\n      <th>perc_responded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>10.0</td>\n      <td>21.0</td>\n      <td>42.0</td>\n      <td>23.809524</td>\n      <td>47.619048</td>\n      <td>50.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6.0</td>\n      <td>26.0</td>\n      <td>42.0</td>\n      <td>14.285714</td>\n      <td>23.076923</td>\n      <td>61.904762</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4.0</td>\n      <td>27.0</td>\n      <td>42.0</td>\n      <td>9.523810</td>\n      <td>14.814815</td>\n      <td>64.285714</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>42.0</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Как ни странно, модел лучше всего обучилась на 1-shot режиме. Пример с 7-shot мог бы выступить сильнее, но я сам чуть-чуть подпортил эксперимент, снизив в 2 раза количество `max_new_tokens`.","metadata":{}}]}