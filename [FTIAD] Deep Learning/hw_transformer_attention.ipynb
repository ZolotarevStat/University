{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Xf6aZOPIQimD","executionInfo":{"status":"ok","timestamp":1687113951392,"user_tz":-180,"elapsed":28678,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c77d147b-7a54-47ac-9fe2-faa0dfb639d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.6/720.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q datasets pytorch-lightning transformers evaluate sacrebleu accelerate"]},{"cell_type":"markdown","metadata":{"id":"ukIa2JQ3TGEa"},"source":["# Загрузка датасета и предобработка"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"F0V0Fi5EvuSc","executionInfo":{"status":"ok","timestamp":1687113962249,"user_tz":-180,"elapsed":10861,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","from typing import List, Optional\n","\n","from datasets import load_dataset\n","from tokenizers.processors import TemplateProcessing\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336,"referenced_widgets":["82f1fb41462746dab14e4bc5e887f761","7bc719fc632e4fdc9cd3564d71463616","79303a55444b4e0bb96c9e5eec6b562c","b5f9e45460ac4c57a77bdf4b5564b132","59c2f9b71383448fa25f3b0e5d8011e2","d8c6fd543bd54a6da52cd0a8124d7938","5c1451d54a784c9bb3798451b3646872","0b1acf44cd7b4472a1e6e32ed0487d90","8c1f7e2266e74736b255c4764dfb9421","c3c025c9049f4b679f0408a5c5f596c2","1cad02be0d3b46e6a2a1b4b01b0f0d24","e683c5e78f304bd1a07dfd2f4c64f33f","f62b06387589485f91f106e5cb37863a","cb30f27c01ab44f4aebe600d7d32fe39","a2d81f153be24b8782be8789e988018f","914c690890f24fe69083519cad775f38","122f7888caee4127a6807a8e095c7bd0","17e1b392043e4d738c3d325fff14d7e0","1960f57ed1a741f78ade0c5b6954de62","92bf5b87b181451fa2fbe8e444af7a6c","a1f606b1fcf9461b9b5e085bbcb01d5b","351b2ebd47cb45209e43ca00aa1375f2","7957326602a9477db66ee476739e602a","ccbe8dbfe25a4f72b87e67f9952c4bef","4b95bb311acf4c588d15cb0194c2bf77","ec9896fb5ddc4684a0d955216b28084a","ddb4ea7be77f430e9680ce80285540b4","097266d1f3db4d95abe561c165af31c3","1c19b4c13d7f422c9c73d8df486e0376","6a7deb94808647f08fc5a7bd3c98db53","7f722a1ae269458aa354d68de05e663c","929a77a6a29b4540a79530b79662a9ef","7b1e1c4883294929aa95645ac27f643a","744d593c69254e7088ad584d04ad6748","82f7bfcebeb34e51839ae57ba82fa1a9","75c6d3b58e5a48849121815dcd5e1e0f","fef9220abed9435cb2ddafd50bb7a8ff","ffb6238a21f74f73953523a55ccfc27d","ea98eeef887048c99c44a19cec10de00","ec1293748c044b27960135a0ea10c57c","2f312c03d6a747148ca7079dce3ba61c","a054b880942e48ada1d3f60e94d0843c","62118d9e2b844961817aba3e7cb9cf8b","4a6b70dc4e4344d387fdead4ee730dd9","1bf4dab4c07c4ffca9d3aad8d32e3d5b","f48a345a43fb46d48491500f231ba773","f74381fdedcc421a8d2be5b016b48c23","a67826190d2a4583b4ad995a9baa2265","65f15d092cf34e5daeae42d2248ebec6","6cf1298f0b684511a3ced954e8d6916f","c719b481e3e941fd8a875e5425328952","2c336dcee59b4a07a759e62e0d3a439d","98d41aa9138844df97d505574c5f89aa","b8a992837389418895aeb711eebec592","c8836409292548529d22226cbc1eb6aa","7b5ace29dda844ee8190f2b3e2440580","74937881247a4a45b5d9a8b4d10338cf","dfdacf91b85749a0bb3ad3ddce0c9998","4ba63da9f76c466da87d1bd05877fdb8","bab3c4a23257419686423ea9effb226c","6aed1ab411f54fbc907bfe0bd51d3e8b","55af3a104b4d451e830bd3c7ad88470d","28e0b9beca6347ef8b926b4d05b17a32","1eb51ca0efb34a4aa2fe3a9a4ae4553f","2dc23be6b20d4941a87cebd981b2d5c3","9fa6df630b0b4d9c8049ffa34fcb076a","1ff4a106c99c4fc4a09abced5ace0f16","a7d660362f9448fbbd365642db06d0b8","0ed3d1d6f8ac473e9524d8726c5fa903","964a59a5bacc41708a84c7a4191d24cf","0c58d020654a448b90441c61d5029dcd","ca2d1f165a0c4d81afe5022326a56e4d","d07416e43d854810850a8182164f8eb7","dd720b70ce5748bf9b11d1f264188023","3c63c835867a4ca59fc0135c396c7b06","dbe8fcf466f048a699b5b46aff966b78","8cedb51387f2464fa1caae67975aa4ad"]},"id":"97KtI637Qd9u","outputId":"99f613f2-4682-4c8a-8820-588f2f7bee2c","executionInfo":{"status":"ok","timestamp":1687113984602,"user_tz":-180,"elapsed":22355,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f1fb41462746dab14e4bc5e887f761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading metadata:   0%|          | 0.00/161k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e683c5e78f304bd1a07dfd2f4c64f33f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/20.5k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7957326602a9477db66ee476739e602a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset opus_books/en-fr to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744d593c69254e7088ad584d04ad6748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bf4dab4c07c4ffca9d3aad8d32e3d5b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset opus_books downloaded and prepared to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5ace29dda844ee8190f2b3e2440580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff4a106c99c4fc4a09abced5ace0f16"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'translation'],\n","        num_rows: 8919\n","    })\n","})\n"]}],"source":["books = load_dataset(\"opus_books\", \"en-fr\")\n","books[\"train\"] = books[\"train\"].select(range(10000))\n","\n","books = books.filter(lambda x: len(x['translation']['en']) < 250)\n","print(books)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FH2CqnVLQd6J","outputId":"2d51cebd-8233-41d7-81ca-89036558001c","executionInfo":{"status":"ok","timestamp":1687113984603,"user_tz":-180,"elapsed":4,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0', 'translation': {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}}"]},"metadata":{},"execution_count":4}],"source":["books['train'][0]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Hml0LUMaSWZ8","executionInfo":{"status":"ok","timestamp":1687113987091,"user_tz":-180,"elapsed":2490,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["ALL_SETENCES_FILE = 'all_book_sentences.txt'\n","\n","with open(ALL_SETENCES_FILE, 'w') as f:\n","    for item in books['train']:\n","        f.write(item['translation']['en'] + \"\\n\")\n","        f.write(item['translation']['fr'] + \"\\n\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RnU1XcLSxPa","outputId":"eb6907e5-2415-4c12-fffa-452484ef848b","executionInfo":{"status":"ok","timestamp":1687113987092,"user_tz":-180,"elapsed":5,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The Wanderer\n","Le grand Meaulnes\n","Alain-Fournier\n","Alain-Fournier\n","First Part\n","PREMIÈRE PARTIE\n","I\n","CHAPITRE PREMIER\n","THE BOARDER\n","LE PENSIONNAIRE\n"]}],"source":["!head all_book_sentences.txt"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ahGAyXaSQduz","executionInfo":{"status":"ok","timestamp":1687113988996,"user_tz":-180,"elapsed":1906,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n","\n","VOCAB_SIZE = 20000\n","\n","bpe_trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[BOS]\", \"[EOS]\", \"[PAD]\"], show_progress=True, vocab_size=VOCAB_SIZE)\n","\n","tokenizer.pre_tokenizer = Whitespace()\n","\n","files = [ ALL_SETENCES_FILE ]\n","\n","tokenizer.train(files, bpe_trainer)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-Wt646LwWU7u","executionInfo":{"status":"ok","timestamp":1687113988998,"user_tz":-180,"elapsed":8,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["tokenizer.post_processor = TemplateProcessing(\n","    single=\"[BOS] $A [EOS]\",\n","    special_tokens=[\n","        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n","        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n","    ],\n",")\n","\n","tokenizer.save(\"tokenizer.json\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvwZquVuhtL6","outputId":"f596e2d3-166a-4dc8-d979-ff7ea306dec7","executionInfo":{"status":"ok","timestamp":1687113988998,"user_tz":-180,"elapsed":7,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=3, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{},"execution_count":9}],"source":["tokenizer.encode('[BOS]')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"A32llDIyhmEb","executionInfo":{"status":"ok","timestamp":1687113988999,"user_tz":-180,"elapsed":7,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n","fast_tokenizer.bos_token = \"[BOS]\"\n","fast_tokenizer.eos_token = \"[EOS]\"\n","fast_tokenizer.pad_token = \"[PAD]\"\n","fast_tokenizer.unk_token = \"[UNK]\""]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SViSDvl7RdR6","executionInfo":{"status":"ok","timestamp":1687113988999,"user_tz":-180,"elapsed":6,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["source_lang = \"en\"\n","target_lang = \"fr\"\n","\n","def preprocess_function(examples):\n","    inputs = [example[source_lang] for example in examples[\"translation\"]]\n","    targets = [example[target_lang] for example in examples[\"translation\"]]\n","    model_inputs = fast_tokenizer(inputs, text_target=targets, max_length=64, truncation=True, add_special_tokens=True)\n","    return model_inputs\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["e17d1f4c6b414a96b8e859ff5f8eddfa","41260bfc802f4dabbb230b02e269fb57","acaf1648b2c4440bbb3925f074e4939f","1f6c07ffde62497896a05aea957c341a","657b043b376745e886f2e75341fda762","d1a4af05f5234e5889ee81046e1d4458","e311a37df7a04888b6a1f7a99d1b7c4a","e8ceec54144947f0b4e4bbf2f1238c17","a66229ee84ad49d2ab491fe7a003dc8b","14c1c8914eba440493861b595f57ebaf","daacfe20f4594c75a6320209b0ab0ab8"]},"id":"La3U7_2pTxYI","outputId":"9b6e589c-20b8-40a9-84ae-a11ab8940ec5","executionInfo":{"status":"ok","timestamp":1687113992030,"user_tz":-180,"elapsed":3037,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8919 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17d1f4c6b414a96b8e859ff5f8eddfa"}},"metadata":{}}],"source":["books_preprocessed = books.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asRELfhSZcQR","outputId":"9c2f1e1f-3812-47e7-b7cb-6806941b681a","executionInfo":{"status":"ok","timestamp":1687113992031,"user_tz":-180,"elapsed":11,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'translation': {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'},\n"," 'input_ids': [1, 281, 19049, 1203, 2],\n"," 'token_type_ids': [0, 0, 0, 0, 0],\n"," 'attention_mask': [1, 1, 1, 1, 1],\n"," 'labels': [1, 521, 558, 346, 2]}"]},"metadata":{},"execution_count":13}],"source":["books_preprocessed['train'][0]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"WtDbxDN0Xy47","executionInfo":{"status":"ok","timestamp":1687114000812,"user_tz":-180,"elapsed":8790,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=fast_tokenizer, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pb6V-x0pS4Tk","outputId":"97aeac95-b353-46e0-d98d-0ca3d613b92d","executionInfo":{"status":"ok","timestamp":1687114000813,"user_tz":-180,"elapsed":12,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [1, 4207, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1], 'labels': [1, 4207, 2]}"]},"metadata":{},"execution_count":15}],"source":["fast_tokenizer(\"test\", text_target=\"test\")"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxWPqNSjSTnF","outputId":"f2f62fc3-2440-4cd5-f939-2992716a86ca","executionInfo":{"status":"ok","timestamp":1687114001165,"user_tz":-180,"elapsed":362,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":16}],"source":["fast_tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Co7Md42JpWN8","outputId":"f8c35872-7e87-4cb4-925a-b82f8928037a","executionInfo":{"status":"ok","timestamp":1687114001165,"user_tz":-180,"elapsed":4,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[100, 200, 300,   3,   3],\n","        [100, 200, 300, 400, 500]]), 'labels': tensor([[ 100,  200,  300,  400,  500, -100],\n","        [ 100,  200,  300,  400,  500,  600]]), 'attention_mask': tensor([[1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1]])}"]},"metadata":{},"execution_count":17}],"source":["\n","data_collator( [ {\"input_ids\": [ 100, 200, 300 ], \"labels\": [100,200,300, 400, 500]}, { \"input_ids\": [ 100, 200, 300,400,500], \"labels\": [100,200,300,400,500,600]} ] )\n"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"OzvXqbJRQ1Bt","executionInfo":{"status":"ok","timestamp":1687116203868,"user_tz":-180,"elapsed":306,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["# ВАЖНО! Эту ячейку в пайплайнах будет копировать в отдельный файл.\n","# Нельзя изменять названия и конструкторы классов.\n","# Не стоит изменять сигнатуры методов\n","# Не стоит добавлять сюда новые ненужные импорты\n","# Не стоит удалять импорты, которые тут были\n","# Не стоит добавлять в эту ячейку новые классы\n","\n","from transformers.modeling_outputs import Seq2SeqModelOutput, Seq2SeqLMOutput\n","\n","import torch\n","import torch.nn as nn\n","\n","from transformers.modeling_outputs import Seq2SeqModelOutput, Seq2SeqLMOutput\n","\n","from transformers import GenerationConfig, PretrainedConfig, PreTrainedModel\n","import random\n","import numpy as np\n","\n","# Для прохождения тестов не будет требоваться, чтобы модель полностью обучилась\n","# это может занять много времени. В этой домашке будет достаточно переобучить модель\n","# на небольшой части датасета.\n","#\n","# Цель домашки -- это реализовать свой трансформер\n","# Нужно дополнить процесс обучения ниже, заполнить пропуски\n","#\n","# Чтобы домашка была тестируемой, пришлось прибегнуть к таким требованиям\n","# Кроме того, нет большого практического смысла в обучении такой архитектуры,\n","# потому что она устарела, лучше обучите трансформер во втором ноутбуке)\n","#\n","# HINT!\n","# Прописывайте размерности разных тензоров в комментах -- так будет проще разбираться в коде и дебажить\n","#\n","# Черпать вдохновение можно отсюда http://nlp.seas.harvard.edu/2018/04/03/attention.html\n","# И Attention Is All You Need https://arxiv.org/abs/1706.03762\n","\n","# PretrainedConfig см тут https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/configuration#transformers.PretrainedConfig\n","class TransformerAttentionConfig(PretrainedConfig):\n","    model_type = \"rnn\"\n","\n","    r\"\"\"\n","    В классе конфига должны быть описаны все гипер-параметры модели\n","\n","    Args:\n","        vocab_size (`int`):\n","            Размер словаря\n","        embedding_dim (`int`):\n","            Размерность эмбэддингов\n","        hidden_dim (`int`):\n","            размерность скрытых слоев\n","        num_layers (`int`):\n","            количество слоев трансформера (и для енкодера, и для декодера)\n","        max_length (`int`):\n","            Максимальная длинна сгенерированной последовательности\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        vocab_size=20000,\n","        embedding_dim=128,\n","        hidden_dim=128,\n","        num_layers=3,\n","        max_length=64,\n","        # Эти токены должны быть предопределены уже в PretrainedConfig\n","        # https://github.com/huggingface/transformers/blob/cf11493dce0a1d22446efe0d6c4ade02fd928e50/src/transformers/configuration_utils.py#L214\n","        # pad_token_id=None,\n","        # bos_token_id=None,\n","        # eos_token_id=None,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs, max_length=max_length)\n","\n","        assert embedding_dim == hidden_dim, 'this transformer implementation requires embedding_dim to be equals to hidden_dim'\n","\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","\n","# вот тут про PreTrainedModel  https://github.com/huggingface/transformers/blob/cf11493dce0a1d22446efe0d6c4ade02fd928e50/src/transformers/modeling_utils.py#LL1009C7-L1009C22\n","class Seq2SeqTransformerAttention(PreTrainedModel):\n","\n","    config_class = TransformerAttentionConfig\n","    base_model_prefix = \"transformer\"\n","    supports_gradient_checkpointing = False\n","\n","    def __init__(self, config):\n","\n","        super().__init__( config )\n","\n","        self.generation_config = GenerationConfig()\n","\n","        # начинаем с того, что правильно опишем используемые модули\n","        # используем self.config.*\n","\n","        self.embeddings = nn.Embedding(num_embeddings=self.config.vocab_size,\n","                                       embedding_dim=self.config.embedding_dim)\n","        self.embeddings_dropout = nn.Dropout(p=0.1)\n","\n","        self.encoder_norm = nn.LayerNorm(self.config.embedding_dim)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=self.config.embedding_dim,\n","                                                   nhead=8, batch_first=True)\n","        encoder_norm = nn.LayerNorm(self.config.embedding_dim)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,\n","                                                         num_layers=self.config.num_layers)\n","\n","        decoder_layer = nn.TransformerDecoderLayer(d_model=self.config.embedding_dim,\n","                                                   nhead=8, batch_first=True)\n","        decoder_norm = nn.LayerNorm(self.config.embedding_dim)\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layer,\n","                                                         num_layers=self.config.num_layers)\n","\n","        self.decoder_labels_linear = nn.Linear(self.config.embedding_dim, self.config.vocab_size)\n","\n","        self.criterion = nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)\n","\n","        return\n","\n","\n","    # copy paste from\n","    # http://nlp.seas.harvard.edu/2018/04/03/attention.html\n","    def subsequent_mask(self, size, device='cpu'):\n","        \"Mask out subsequent positions.\"\n","        attn_shape = (size, size)\n","        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","        return (torch.from_numpy(subsequent_mask) != 0).to(device)\n","\n","    def encode(self, input_embeddings=None, key_padding_mask=None):\n","\n","        # получаем неконтекстные эмбэддинги\n","        # получаем контекстные эмбэддинги с помощью self.encoder\n","\n","        encoder_embeddings = self.transformer_encoder(input_embeddings, key_padding_mask)\n","\n","        return encoder_embeddings\n","\n","    def decode(self, encoder_outputs=None, lebels_embeddings=None, key_padding_mask=None, encoder_key_padding_mask=None):\n","        # не забудьте, что нужно обработать сдвиг токенов\n","        # декодер должен вернуть для каждого текущего токена последующий токен\n","        # у последнего токена нет последующего тк он последний\n","        # поэтому последний токен не надо передавать\n","\n","        # прелесть трансформеров заключается в том, что у них параллелится обучение декодера\n","        # не надо на каждый токен запускать generate_encoded как это было в RNN\n","        # это возможно благодаря тому, что мы можем замаскировать будущие токены в механизме внимания\n","        # декодера с помощью subsequent_mask\n","        tgt_mask = self.subsequent_mask( lebels_embeddings.shape[1] - 1, device=encoder_outputs.device)\n","\n","        # не забываем про сдвиг\n","        if key_padding_mask is not None:\n","          tgt_key_padding_mask = key_padding_mask[:-1]\n","        else:\n","          tgt_key_padding_mask = key_padding_mask\n","        if encoder_key_padding_mask is not None:\n","          memory_key_padding_mask = encoder_key_padding_mask[:-1]\n","        else:\n","          memory_key_padding_mask = encoder_key_padding_mask\n","\n","        # см доку https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html\n","        # осталось правильно передать аргументы\n","        # не забываем про сдвиг!\n","        decoder_output = self.transformer_decoder(tgt=lebels_embeddings[1:],\n","                                                  memory=encoder_outputs[:-1],\n","                                                  tgt_mask=tgt_mask[:-1],\n","                                                  tgt_key_padding_mask=tgt_key_padding_mask,\n","                                                  memory_key_padding_mask=memory_key_padding_mask)\n","\n","        return decoder_output\n","\n","    # https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput\n","    def forward(self, input_ids=None, labels=None, attention_mask=None, token_type_ids=None, ) -> Seq2SeqModelOutput:\n","\n","        # input_ids: [ batch_size, src_seq_len ]\n","        # labels: [ batch_size, tgt_seq_len ]\n","\n","        encoder_key_padding_mask = (attention_mask == 0)\n","\n","        decoder_key_padding_mask = (labels == -100)\n","\n","        input_embeddings = self.embeddings(input_ids)\n","        input_embeddings = self.embeddings_dropout(input_embeddings)\n","\n","        labels[labels == -100] = self.config.pad_token_id\n","        lebels_embeddings = self.embeddings(labels)\n","        lebels_embeddings = self.embeddings_dropout(lebels_embeddings) # используется для обучения декодера\n","\n","        # encoder_outputs может еще называться memory\n","        # TODO размерность [ ? ]\n","        encoder_outputs = self.encode(input_embeddings=input_embeddings,\n","                                      key_padding_mask=encoder_key_padding_mask)\n","\n","\n","        decoder_outputs = self.decode(encoder_outputs=encoder_outputs,\n","                                      lebels_embeddings=lebels_embeddings,\n","                                      key_padding_mask=decoder_key_padding_mask,\n","                                      encoder_key_padding_mask=encoder_key_padding_mask)\n","\n","        # TODO размерность labels_logits: [ ? ]\n","        labels_logits = self.decoder_labels_linear(decoder_outputs)\n","\n","        # не забываем про смещение токенов для labels\n","        loss = self.criterion(labels_logits.reshape(-1, self.config.vocab_size),\n","                              labels[:, 1:].reshape(-1).long())\n","\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            decoder_hidden_states=decoder_outputs,\n","            encoder_hidden_states=encoder_outputs,\n","        )\n","\n","    def generate(self, input_ids=None, attention_mask=None, token_type_ids=None, max_length=None, num_beams=None, **kwargs):\n","        \"\"\"\n","        Метод используется в trainer.evaluate\n","        \"\"\"\n","\n","        batch_size = input_ids.shape[0]\n","\n","        if attention_mask is not None:\n","            attention_mask = (attention_mask == 0)\n","\n","        # надо\n","        # 1. Получить эмбэдддинги\n","        # 2. Получить Контекстные эмбэддинги (memory) через енкодер\n","        # 3. Запустить генерацию generate_encoded\n","\n","        embeddings = self.embeddings(input_ids)\n","        rnn_outputs = self.encode(embeddings, attention_mask)\n","        predicted_tokens_sequences, _ = self.generate_encoded(batch_size=batch_size,\n","                                                              encoder_outputs=rnn_outputs,\n","                                                              encoder_key_padding_mask=attention_mask,\n","                                                              max_length=max_length,\n","                                                              num_beams=num_beams)\n","\n","        return predicted_tokens_sequences\n","\n","    def generate_encoded(self, batch_size=None, encoder_outputs=None, encoder_key_padding_mask=None, max_length=None, num_beams=None, **kwargs):\n","\n","        generated_tokens = torch.tensor([self.config.bos_token_id] * batch_size, device=encoder_outputs.device).unsqueeze(1) # [ bs, 1 ]\n","\n","        predicted_tokens_sequences = [ generated_tokens ] # [ bs, 1 ]\n","\n","\n","        # todo размерность decoder_outputs: [ ? ]\n","        decoder_outputs = []\n","        softmax_layer = nn.Softmax(dim=-1)\n","\n","        for token_i in range(max_length):\n","            # генерим по одному токену\n","            # greedy decode\n","\n","            # тут должны быть неконтекстные эмбэддинги для уже сгенерированных токенов\n","            generated_tokens_embeddings = self.embeddings(predicted_tokens_sequences[token_i])\n","            print('generated_tokens_embeddings', generated_tokens_embeddings.shape)\n","            print('encoder_outputs', encoder_outputs.shape)\n","            #print('key_padding_mask', key_padding_mask)\n","            #print('encoder_key_padding_mask', encoder_key_padding_mask.shape)\n","            print(self.subsequent_mask( generated_tokens_embeddings.shape[1] - 1,\n","                                       device=encoder_outputs.device))\n","            decoder_output = self.decode(encoder_outputs=encoder_outputs,\n","                                         lebels_embeddings=generated_tokens_embeddings,\n","                                          key_padding_mask=None,\n","                                          encoder_key_padding_mask=encoder_key_padding_mask)\n","            print('decoder_output', decoder_output.shape)\n","            last_token_outputs = decoder_output[:, -1:, :]\n","            decoder_outputs.append(last_token_outputs)\n","\n","            next_token_logits = self.decoder_labels_linear(decoder_output)\n","\n","            # из логитов надо получить вероятности токенов, как это делали в RNN\n","            next_generated_tokens = torch.max(next_token_logits, dim=2)[1]\n","            print('next_generated_tokens', next_generated_tokens)\n","            predicted_tokens_sequences.append(next_generated_tokens)\n","            # predicted_tokens_sequences расширили, поэтому надо обновить и generated_tokens\n","        generated_tokens = torch.cat(predicted_tokens_sequences, dim=-1)\n","\n","        # это мы уже делали для RNN, делаем паддинг после EOS токена\n","        eos_token_indexes = torch.nonzero(generated_tokens == self.config.eos_token_id, as_tuple=False)\n","        for eos_idx in eos_token_indexes:\n","            generated_tokens[eos_idx[0], eos_idx[1]:] = self.config.pad_token_id\n","\n","        return generated_tokens, decoder_outputs\n","\n","\n"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"jKaj9KOJQ0_D","executionInfo":{"status":"ok","timestamp":1687116207099,"user_tz":-180,"elapsed":348,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[],"source":["transformer_attention_config = TransformerAttentionConfig(\n","    pad_token_id=fast_tokenizer.pad_token_id,\n","    bos_token_id=fast_tokenizer.bos_token_id,\n","    eos_token_id=fast_tokenizer.eos_token_id,\n",")\n","transformer_attention_model = Seq2SeqTransformerAttention(transformer_attention_config)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XglsXbiUrvwV","outputId":"78ad9a85-8a8a-4e08-880e-0896427fbed2","executionInfo":{"status":"ok","timestamp":1687114463903,"user_tz":-180,"elapsed":3,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["8897312"]},"metadata":{},"execution_count":30}],"source":["sum( p.numel() for p in transformer_attention_model.parameters() if p.requires_grad )"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hh1R5oucQ08v","outputId":"8f104339-f92c-46b7-9fd3-82b393520a06","executionInfo":{"status":"error","timestamp":1687116211940,"user_tz":-180,"elapsed":3364,"user":{"displayName":"Антон Золотарев","userId":"00132696149152071356"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["generated_tokens_embeddings torch.Size([3, 1, 128])\n","encoder_outputs torch.Size([3, 5, 128])\n","tensor([], size=(0, 0), dtype=torch.bool)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mgenerate\u001b[0m:\u001b[94m230\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mgenerate_encoded\u001b[0m:\u001b[94m261\u001b[0m                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mdecode\u001b[0m:\u001b[94m164\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mtransformer.py\u001b[0m:\u001b[94m369\u001b[0m in \u001b[92mforward\u001b[0m           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m366 \u001b[0m\u001b[2m│   │   \u001b[0moutput = tgt                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m367 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m mod \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.layers:                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m369 \u001b[2m│   │   │   \u001b[0moutput = mod(output, memory, tgt_mask=tgt_mask,                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m370 \u001b[0m\u001b[2m│   │   │   │   │   │    \u001b[0mmemory_mask=memory_mask,                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m371 \u001b[0m\u001b[2m│   │   │   │   │   │    \u001b[0mtgt_key_padding_mask=tgt_key_padding_mask,                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m372 \u001b[0m\u001b[2m│   │   │   │   │   │    \u001b[0mmemory_key_padding_mask=memory_key_padding_mask)                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mtransformer.py\u001b[0m:\u001b[94m716\u001b[0m in \u001b[92mforward\u001b[0m           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m713 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = x + \u001b[96mself\u001b[0m._mha_block(\u001b[96mself\u001b[0m.norm2(x), memory, memory_mask, memory_key_paddi   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m714 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = x + \u001b[96mself\u001b[0m._ff_block(\u001b[96mself\u001b[0m.norm3(x))                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m715 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m716 \u001b[2m│   │   │   \u001b[0mx = \u001b[96mself\u001b[0m.norm1(x + \u001b[96mself\u001b[0m._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m717 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = \u001b[96mself\u001b[0m.norm2(x + \u001b[96mself\u001b[0m._mha_block(x, memory, memory_mask, memory_key_paddin   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m718 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = \u001b[96mself\u001b[0m.norm3(x + \u001b[96mself\u001b[0m._ff_block(x))                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m719 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mtransformer.py\u001b[0m:\u001b[94m725\u001b[0m in \u001b[92m_sa_block\u001b[0m         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m722 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# self-attention block\u001b[0m                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m723 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_sa_block\u001b[0m(\u001b[96mself\u001b[0m, x: Tensor,                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m724 \u001b[0m\u001b[2m│   │   │   │     \u001b[0mattn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_ca   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m725 \u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.self_attn(x, x, x,                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m726 \u001b[0m\u001b[2m│   │   │   │   │   │      \u001b[0mattn_mask=attn_mask,                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m727 \u001b[0m\u001b[2m│   │   │   │   │   │      \u001b[0mkey_padding_mask=key_padding_mask,                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m728 \u001b[0m\u001b[2m│   │   │   │   │   │      \u001b[0mis_causal=is_causal,                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mactivation.py\u001b[0m:\u001b[94m1205\u001b[0m in \u001b[92mforward\u001b[0m           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1202 \u001b[0m\u001b[2m│   │   │   │   \u001b[0maverage_attn_weights=average_attn_weights,                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1203 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mis_causal=is_causal)                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1204 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1205 \u001b[2m│   │   │   \u001b[0mattn_output, attn_output_weights = F.multi_head_attention_forward(            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1206 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mquery, key, value, \u001b[96mself\u001b[0m.embed_dim, \u001b[96mself\u001b[0m.num_heads,                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1207 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.in_proj_weight, \u001b[96mself\u001b[0m.in_proj_bias,                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1208 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.bias_k, \u001b[96mself\u001b[0m.bias_v, \u001b[96mself\u001b[0m.add_zero_attn,                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/\u001b[0m\u001b[1;33mfunctional.py\u001b[0m:\u001b[94m5251\u001b[0m in                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92mmulti_head_attention_forward\u001b[0m                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5248 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m attn_mask.dim() == \u001b[94m2\u001b[0m:                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5249 \u001b[0m\u001b[2m│   │   │   \u001b[0mcorrect_2d_size = (tgt_len, src_len)                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5250 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m attn_mask.shape != correct_2d_size:                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m5251 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThe shape of the 2D attn_mask is \u001b[0m\u001b[33m{\u001b[0mattn_mask.shape\u001b[33m}\u001b[0m\u001b[33m,\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5252 \u001b[0m\u001b[2m│   │   │   \u001b[0mattn_mask = attn_mask.unsqueeze(\u001b[94m0\u001b[0m)                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5253 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m attn_mask.dim() == \u001b[94m3\u001b[0m:                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5254 \u001b[0m\u001b[2m│   │   │   \u001b[0mcorrect_3d_size = (bsz * num_heads, tgt_len, src_len)                         \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mRuntimeError: \u001b[0mThe shape of the 2D attn_mask is \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, but should be \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">230</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_encoded</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">261</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decode</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">164</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">transformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">369</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">366 │   │   </span>output = tgt                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">367 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">368 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> mod <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layers:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>369 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = mod(output, memory, tgt_mask=tgt_mask,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">370 │   │   │   │   │   │    </span>memory_mask=memory_mask,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">371 │   │   │   │   │   │    </span>tgt_key_padding_mask=tgt_key_padding_mask,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">372 │   │   │   │   │   │    </span>memory_key_padding_mask=memory_key_padding_mask)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">transformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">716</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">713 │   │   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._mha_block(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm2(x), memory, memory_mask, memory_key_paddi   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">714 │   │   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._ff_block(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm3(x))                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">715 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>716 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm1(x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">717 │   │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm2(x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._mha_block(x, memory, memory_mask, memory_key_paddin   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">718 │   │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm3(x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._ff_block(x))                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">719 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">transformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">725</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_sa_block</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">722 │   # self-attention block</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">723 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_sa_block</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x: Tensor,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724 │   │   │   │     </span>attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_ca   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>725 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.self_attn(x, x, x,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">726 │   │   │   │   │   │      </span>attn_mask=attn_mask,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">727 │   │   │   │   │   │      </span>key_padding_mask=key_padding_mask,                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">728 │   │   │   │   │   │      </span>is_causal=is_causal,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">activation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1205</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1202 │   │   │   │   </span>average_attn_weights=average_attn_weights,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1203 │   │   │   │   </span>is_causal=is_causal)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1204 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1205 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_output, attn_output_weights = F.multi_head_attention_forward(            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1206 │   │   │   │   </span>query, key, value, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed_dim, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1207 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.in_proj_weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.in_proj_bias,                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1208 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias_k, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias_v, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.add_zero_attn,                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">functional.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5251</span> in                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">multi_head_attention_forward</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5248 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attn_mask.dim() == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5249 │   │   │   </span>correct_2d_size = (tgt_len, src_len)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5250 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attn_mask.shape != correct_2d_size:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>5251 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"The shape of the 2D attn_mask is {</span>attn_mask.shape<span style=\"color: #808000; text-decoration-color: #808000\">},</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5252 │   │   │   </span>attn_mask = attn_mask.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5253 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> attn_mask.dim() == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5254 │   │   │   </span>correct_3d_size = (bsz * num_heads, tgt_len, src_len)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>The shape of the 2D attn_mask is <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">])</span>, but should be <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>.\n","</pre>\n"]},"metadata":{}}],"source":["transformer_attention_model.generate( input_ids = torch.arange(15).reshape(3, 5), max_length=64 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7J2UFKJms67w"},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nH0KEAoaRN2N"},"outputs":[],"source":["import numpy as np\n","import evaluate\n","\n","metric = evaluate.load(\"sacrebleu\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels\n","\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = fast_tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    labels = np.where(labels != -100, labels, fast_tokenizer.pad_token_id)\n","    decoded_labels = fast_tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    print('decoded_preds[0]', fast_tokenizer.batch_decode(preds, skip_special_tokens=False)[0])\n","    print('decoded_labels[0]', decoded_labels[0])\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","\n","    prediction_lens = [np.count_nonzero(pred != fast_tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7Qe1bN4s65H"},"outputs":[],"source":["transformer_attention_config = TransformerAttentionConfig(\n","    pad_token_id=fast_tokenizer.pad_token_id,\n","    bos_token_id=fast_tokenizer.bos_token_id,\n","    eos_token_id=fast_tokenizer.eos_token_id,\n",")\n","transformer_attention_model = Seq2SeqTransformerAttention(transformer_attention_config)\n","\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"my_awesome_opus_books_model\",\n","    evaluation_strategy=\"steps\",\n","    eval_steps=1000,\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=24,\n","    per_device_eval_batch_size=24,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=1000,\n","    predict_with_generate=True,\n","    logging_steps=25,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=transformer_attention_model,\n","    args=training_args,\n","    train_dataset=books_preprocessed[\"train\"].select(range(256)),\n","    eval_dataset=books_preprocessed[\"train\"].select(torch.tensor(range(256))),\n","    tokenizer=fast_tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhCJWut5zeT9"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"7qtG5n4whtCN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir my_awesome_opus_books_model/runs"],"metadata":{"id":"ZCecbm7jhs_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Протестируем модель"],"metadata":{"id":"xBZaXakZgbJ9"}},{"cell_type":"code","source":["# Сохраняем модель и токенайзер\n","fast_tokenizer.save_pretrained(\"./transformer_attention_tokenizer\")\n","transformer_attention_model.save_pretrained(\"./transformer_attention_model\")"],"metadata":{"id":"NaodOc13fkcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n","\n","# DataCollator отвечает за объединение данных в батчи -- добивает предложения до одной длинны (делает паддинг)\n","# преобразует numpy.array или питоновские списки в torch.Tensor\n","\n","import numpy as np\n","import evaluate\n","\n","metric = evaluate.load(\"sacrebleu\")\n","\n","loaded_tokenizer = AutoTokenizer.from_pretrained(\"./transformer_attention_tokenizer/\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = loaded_tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    labels = np.where(labels != -100, labels, loaded_tokenizer.pad_token_id)\n","    decoded_labels = loaded_tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","\n","    prediction_lens = [np.count_nonzero(pred != loaded_tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result\n","\n","\n","rnn_config = TransformerAttentionConfig(\n","    pad_token_id=loaded_tokenizer.pad_token_id,\n","    bos_token_id=loaded_tokenizer.bos_token_id,\n","    eos_token_id=loaded_tokenizer.eos_token_id,\n",")\n","\n","rnn_attention_model_loaded = Seq2SeqTransformerAttention(rnn_config).from_pretrained(\"./transformer_attention_model/\", local_files_only=True)\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"my_awesome_opus_books_model\",\n","    evaluation_strategy=\"steps\",\n","    eval_steps=1000,\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=24,\n","    per_device_eval_batch_size=24,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=1000,\n","    predict_with_generate=True,\n","    logging_steps=25,\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=loaded_tokenizer, return_tensors=\"pt\")\n","\n","\n","trainer = Seq2SeqTrainer(\n","    model=transformer_attention_model,\n","    args=training_args,\n","    eval_dataset=books_preprocessed[\"train\"].select(range(256)),  # валидировать будем тоже на обучающих данных (дисклаймер: это можно делать только для тестирования)\n","    tokenizer=fast_tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","\n","evaluate_result = trainer.evaluate(test_dataset=books_preprocessed[\"train\"].select(range(256)))\n","\n","print(\"evaluate_result\", evaluate_result)\n","\n","assert evaluate_result['eval_bleu'] >= 50"],"metadata":{"id":"DGsbkYK1gznj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Если ассерт выше прошел, можете загрузить ноутбук и обученные модельки+конфиги на гитхаб. На гитхабе из ноутбука будет выгружена ячейка с описанием модели и будет запускаться тест аналогичный ячейке выше.\n","\n","Веса и конфиги надо заархивировать и на гитхаб загрузить в виде архива с сохранением названий: `rnn_tokenizer.zip`, `rnn_attention_model.zip`\n","\n","Ноутбук надо загрузить с таким же названием, какое было в репозитории.\n","\n","\n","Важно! Архив с весами и конфигом модели не должен весить больше 100МБ иначе этот файлик не будет скачан в пайплайнах даже если вы его загрузите на гитхаб через git-lfs"],"metadata":{"id":"qGrK_AGWg2mS"}},{"cell_type":"code","source":["!zip -r transformer_attention_model.zip transformer_attention_model\n","!zip -r transformer_attention_tokenizer.zip transformer_attention_tokenizer\n","!ls -ltrh | tail -n2"],"metadata":{"id":"TRZ0TnFSg91i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Мы зааихивировали конфиги и веса, скачайте архивы из колаба и загрузите в github репозиторий. **Не забудьте обновить ноутбук в github репозитории тоже!**\n","\n","\n","Архив с моделькой может занимтаь больше 25 мегабайт. Гитхаб не разрешает грузить большие файлы через веб-интерфейс. Можете закоммитить эти архивы через консоль или погуглить, как это сделать по-другому\n","https://bytesbin.com/upload-files-larger-than-25mb-to-github/\n","https://www.google.com/search?q=Fix+GitHub+%E2%80%98Yowza+That%E2%80%99s+a+Big+File%E2%80%99"],"metadata":{"id":"V60VSB5ng3NK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-GD6w1vZFlj"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Протестируйте модель в pipeline, будет ли она давать предсказания для предложений, которых не было в обучающем датасете?"],"metadata":{"id":"TZ0Kvs_xfnwY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iIGsnSDv-2d"},"outputs":[],"source":["from transformers import pipeline\n","\n","# todo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLW0EGAWvZ1I"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Вопросы!"],"metadata":{"id":"KDGMfhBNf0Qo"}},{"cell_type":"markdown","source":["## Почему статья называется \"Attention Is All You Need\"?"],"metadata":{"id":"BKIfvzu86Kq4"}},{"cell_type":"markdown","source":["## Почему обучение декодера RNN нельзя распараллелить, как это делается Transformer Decoder с помощью subsequent mask?"],"metadata":{"id":"7dwDBO6y6Row"}}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/fintech-dl-hse/hw-transformer-attention-ZolotarevStat/blob/main/hw_transformer_attention.ipynb","timestamp":1686929168303}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"82f1fb41462746dab14e4bc5e887f761":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bc719fc632e4fdc9cd3564d71463616","IPY_MODEL_79303a55444b4e0bb96c9e5eec6b562c","IPY_MODEL_b5f9e45460ac4c57a77bdf4b5564b132"],"layout":"IPY_MODEL_59c2f9b71383448fa25f3b0e5d8011e2"}},"7bc719fc632e4fdc9cd3564d71463616":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8c6fd543bd54a6da52cd0a8124d7938","placeholder":"​","style":"IPY_MODEL_5c1451d54a784c9bb3798451b3646872","value":"Downloading builder script: 100%"}},"79303a55444b4e0bb96c9e5eec6b562c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b1acf44cd7b4472a1e6e32ed0487d90","max":6081,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c1f7e2266e74736b255c4764dfb9421","value":6081}},"b5f9e45460ac4c57a77bdf4b5564b132":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3c025c9049f4b679f0408a5c5f596c2","placeholder":"​","style":"IPY_MODEL_1cad02be0d3b46e6a2a1b4b01b0f0d24","value":" 6.08k/6.08k [00:00&lt;00:00, 78.3kB/s]"}},"59c2f9b71383448fa25f3b0e5d8011e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c6fd543bd54a6da52cd0a8124d7938":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c1451d54a784c9bb3798451b3646872":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b1acf44cd7b4472a1e6e32ed0487d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c1f7e2266e74736b255c4764dfb9421":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3c025c9049f4b679f0408a5c5f596c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cad02be0d3b46e6a2a1b4b01b0f0d24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e683c5e78f304bd1a07dfd2f4c64f33f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f62b06387589485f91f106e5cb37863a","IPY_MODEL_cb30f27c01ab44f4aebe600d7d32fe39","IPY_MODEL_a2d81f153be24b8782be8789e988018f"],"layout":"IPY_MODEL_914c690890f24fe69083519cad775f38"}},"f62b06387589485f91f106e5cb37863a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_122f7888caee4127a6807a8e095c7bd0","placeholder":"​","style":"IPY_MODEL_17e1b392043e4d738c3d325fff14d7e0","value":"Downloading metadata: 100%"}},"cb30f27c01ab44f4aebe600d7d32fe39":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1960f57ed1a741f78ade0c5b6954de62","max":161154,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92bf5b87b181451fa2fbe8e444af7a6c","value":161154}},"a2d81f153be24b8782be8789e988018f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1f606b1fcf9461b9b5e085bbcb01d5b","placeholder":"​","style":"IPY_MODEL_351b2ebd47cb45209e43ca00aa1375f2","value":" 161k/161k [00:00&lt;00:00, 2.43MB/s]"}},"914c690890f24fe69083519cad775f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"122f7888caee4127a6807a8e095c7bd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17e1b392043e4d738c3d325fff14d7e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1960f57ed1a741f78ade0c5b6954de62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92bf5b87b181451fa2fbe8e444af7a6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1f606b1fcf9461b9b5e085bbcb01d5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"351b2ebd47cb45209e43ca00aa1375f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7957326602a9477db66ee476739e602a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccbe8dbfe25a4f72b87e67f9952c4bef","IPY_MODEL_4b95bb311acf4c588d15cb0194c2bf77","IPY_MODEL_ec9896fb5ddc4684a0d955216b28084a"],"layout":"IPY_MODEL_ddb4ea7be77f430e9680ce80285540b4"}},"ccbe8dbfe25a4f72b87e67f9952c4bef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_097266d1f3db4d95abe561c165af31c3","placeholder":"​","style":"IPY_MODEL_1c19b4c13d7f422c9c73d8df486e0376","value":"Downloading readme: 100%"}},"4b95bb311acf4c588d15cb0194c2bf77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a7deb94808647f08fc5a7bd3c98db53","max":20464,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f722a1ae269458aa354d68de05e663c","value":20464}},"ec9896fb5ddc4684a0d955216b28084a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_929a77a6a29b4540a79530b79662a9ef","placeholder":"​","style":"IPY_MODEL_7b1e1c4883294929aa95645ac27f643a","value":" 20.5k/20.5k [00:00&lt;00:00, 785kB/s]"}},"ddb4ea7be77f430e9680ce80285540b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"097266d1f3db4d95abe561c165af31c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c19b4c13d7f422c9c73d8df486e0376":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a7deb94808647f08fc5a7bd3c98db53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f722a1ae269458aa354d68de05e663c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"929a77a6a29b4540a79530b79662a9ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b1e1c4883294929aa95645ac27f643a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"744d593c69254e7088ad584d04ad6748":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82f7bfcebeb34e51839ae57ba82fa1a9","IPY_MODEL_75c6d3b58e5a48849121815dcd5e1e0f","IPY_MODEL_fef9220abed9435cb2ddafd50bb7a8ff"],"layout":"IPY_MODEL_ffb6238a21f74f73953523a55ccfc27d"}},"82f7bfcebeb34e51839ae57ba82fa1a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea98eeef887048c99c44a19cec10de00","placeholder":"​","style":"IPY_MODEL_ec1293748c044b27960135a0ea10c57c","value":"Downloading data: 100%"}},"75c6d3b58e5a48849121815dcd5e1e0f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f312c03d6a747148ca7079dce3ba61c","max":12009501,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a054b880942e48ada1d3f60e94d0843c","value":12009501}},"fef9220abed9435cb2ddafd50bb7a8ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62118d9e2b844961817aba3e7cb9cf8b","placeholder":"​","style":"IPY_MODEL_4a6b70dc4e4344d387fdead4ee730dd9","value":" 12.0M/12.0M [00:01&lt;00:00, 13.7MB/s]"}},"ffb6238a21f74f73953523a55ccfc27d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea98eeef887048c99c44a19cec10de00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec1293748c044b27960135a0ea10c57c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f312c03d6a747148ca7079dce3ba61c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a054b880942e48ada1d3f60e94d0843c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62118d9e2b844961817aba3e7cb9cf8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a6b70dc4e4344d387fdead4ee730dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bf4dab4c07c4ffca9d3aad8d32e3d5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f48a345a43fb46d48491500f231ba773","IPY_MODEL_f74381fdedcc421a8d2be5b016b48c23","IPY_MODEL_a67826190d2a4583b4ad995a9baa2265"],"layout":"IPY_MODEL_65f15d092cf34e5daeae42d2248ebec6"}},"f48a345a43fb46d48491500f231ba773":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cf1298f0b684511a3ced954e8d6916f","placeholder":"​","style":"IPY_MODEL_c719b481e3e941fd8a875e5425328952","value":"Generating train split: 100%"}},"f74381fdedcc421a8d2be5b016b48c23":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c336dcee59b4a07a759e62e0d3a439d","max":127085,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98d41aa9138844df97d505574c5f89aa","value":127085}},"a67826190d2a4583b4ad995a9baa2265":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8a992837389418895aeb711eebec592","placeholder":"​","style":"IPY_MODEL_c8836409292548529d22226cbc1eb6aa","value":" 126976/127085 [00:15&lt;00:00, 5050.73 examples/s]"}},"65f15d092cf34e5daeae42d2248ebec6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"6cf1298f0b684511a3ced954e8d6916f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c719b481e3e941fd8a875e5425328952":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c336dcee59b4a07a759e62e0d3a439d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98d41aa9138844df97d505574c5f89aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8a992837389418895aeb711eebec592":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8836409292548529d22226cbc1eb6aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b5ace29dda844ee8190f2b3e2440580":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74937881247a4a45b5d9a8b4d10338cf","IPY_MODEL_dfdacf91b85749a0bb3ad3ddce0c9998","IPY_MODEL_4ba63da9f76c466da87d1bd05877fdb8"],"layout":"IPY_MODEL_bab3c4a23257419686423ea9effb226c"}},"74937881247a4a45b5d9a8b4d10338cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6aed1ab411f54fbc907bfe0bd51d3e8b","placeholder":"​","style":"IPY_MODEL_55af3a104b4d451e830bd3c7ad88470d","value":"100%"}},"dfdacf91b85749a0bb3ad3ddce0c9998":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_28e0b9beca6347ef8b926b4d05b17a32","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1eb51ca0efb34a4aa2fe3a9a4ae4553f","value":1}},"4ba63da9f76c466da87d1bd05877fdb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dc23be6b20d4941a87cebd981b2d5c3","placeholder":"​","style":"IPY_MODEL_9fa6df630b0b4d9c8049ffa34fcb076a","value":" 1/1 [00:00&lt;00:00,  7.53it/s]"}},"bab3c4a23257419686423ea9effb226c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aed1ab411f54fbc907bfe0bd51d3e8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55af3a104b4d451e830bd3c7ad88470d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28e0b9beca6347ef8b926b4d05b17a32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eb51ca0efb34a4aa2fe3a9a4ae4553f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2dc23be6b20d4941a87cebd981b2d5c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fa6df630b0b4d9c8049ffa34fcb076a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ff4a106c99c4fc4a09abced5ace0f16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7d660362f9448fbbd365642db06d0b8","IPY_MODEL_0ed3d1d6f8ac473e9524d8726c5fa903","IPY_MODEL_964a59a5bacc41708a84c7a4191d24cf"],"layout":"IPY_MODEL_0c58d020654a448b90441c61d5029dcd"}},"a7d660362f9448fbbd365642db06d0b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca2d1f165a0c4d81afe5022326a56e4d","placeholder":"​","style":"IPY_MODEL_d07416e43d854810850a8182164f8eb7","value":"Filter: 100%"}},"0ed3d1d6f8ac473e9524d8726c5fa903":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd720b70ce5748bf9b11d1f264188023","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c63c835867a4ca59fc0135c396c7b06","value":10000}},"964a59a5bacc41708a84c7a4191d24cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbe8fcf466f048a699b5b46aff966b78","placeholder":"​","style":"IPY_MODEL_8cedb51387f2464fa1caae67975aa4ad","value":" 10000/10000 [00:00&lt;00:00, 18270.07 examples/s]"}},"0c58d020654a448b90441c61d5029dcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ca2d1f165a0c4d81afe5022326a56e4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d07416e43d854810850a8182164f8eb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd720b70ce5748bf9b11d1f264188023":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c63c835867a4ca59fc0135c396c7b06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbe8fcf466f048a699b5b46aff966b78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cedb51387f2464fa1caae67975aa4ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e17d1f4c6b414a96b8e859ff5f8eddfa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41260bfc802f4dabbb230b02e269fb57","IPY_MODEL_acaf1648b2c4440bbb3925f074e4939f","IPY_MODEL_1f6c07ffde62497896a05aea957c341a"],"layout":"IPY_MODEL_657b043b376745e886f2e75341fda762"}},"41260bfc802f4dabbb230b02e269fb57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1a4af05f5234e5889ee81046e1d4458","placeholder":"​","style":"IPY_MODEL_e311a37df7a04888b6a1f7a99d1b7c4a","value":"Map: 100%"}},"acaf1648b2c4440bbb3925f074e4939f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8ceec54144947f0b4e4bbf2f1238c17","max":8919,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a66229ee84ad49d2ab491fe7a003dc8b","value":8919}},"1f6c07ffde62497896a05aea957c341a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14c1c8914eba440493861b595f57ebaf","placeholder":"​","style":"IPY_MODEL_daacfe20f4594c75a6320209b0ab0ab8","value":" 8919/8919 [00:02&lt;00:00, 2762.84 examples/s]"}},"657b043b376745e886f2e75341fda762":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d1a4af05f5234e5889ee81046e1d4458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e311a37df7a04888b6a1f7a99d1b7c4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8ceec54144947f0b4e4bbf2f1238c17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a66229ee84ad49d2ab491fe7a003dc8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14c1c8914eba440493861b595f57ebaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daacfe20f4594c75a6320209b0ab0ab8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}